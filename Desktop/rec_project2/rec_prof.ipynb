{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from surprise import Dataset, SVD\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "import string\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import random\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Преобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m df_info \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCourse_info.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m df_coment \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComments.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:626\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[1;32m--> 626\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\u001b[38;5;241m.\u001b[39mread(nrows)\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1923\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1916\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1918\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[0;32m   1919\u001b[0m     (\n\u001b[0;32m   1920\u001b[0m         index,\n\u001b[0;32m   1921\u001b[0m         columns,\n\u001b[0;32m   1922\u001b[0m         col_dict,\n\u001b[1;32m-> 1923\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mread(  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m         nrows\n\u001b[0;32m   1925\u001b[0m     )\n\u001b[0;32m   1926\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1927\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[1;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39mread_low_memory(nrows)\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[0;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[1;32mparsers.pyx:838\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:905\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:874\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._check_tokenize_status\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mparsers.pyx:2053\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen codecs>:331\u001b[0m, in \u001b[0;36mgetstate\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_info = pd.read_csv('Course_info.csv')\n",
    "df_coment = pd.read_csv('Comments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#формат даты был неудобный и таблица была огромной с учетом того что данные с 2011 до 2022 года, сократила их до 2017-2022\n",
    "df_coment['date'] = df_coment['date'].apply(lambda x: datetime.fromisoformat(x).date())\n",
    "df_info['published_time'] = df_info['published_time'].apply(lambda x: datetime.fromisoformat(x).date())\n",
    "\n",
    "df_coment['date'] = pd.to_datetime(df_coment['date'], errors='coerce')\n",
    "df_info['published_time'] = pd.to_datetime(df_info['published_time'], errors='coerce')\n",
    "\n",
    "# filtered_df_coment = df_coment[(df_coment['date'].dt.year >= 2021) & (df_coment['date'].dt.year <= 2022)]\n",
    "# filtered_df_info = df_info[(df_info['published_time'].dt.year >= 2021) & (df_info['published_time'].dt.year <= 2022)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df_coment = df_coment[df_coment['date'].dt.year == 2022]\n",
    "filtered_df_info = df_info[df_info['published_time'].dt.year == 2022]\n",
    "\n",
    "filtered_df_info.reset_index().to_csv('info2017.csv', index=False)\n",
    "filtered_df_coment.reset_index().to_csv('comment2017.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('info2017.csv')\n",
    "df_comment = pd.read_csv('comment2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commen = df_comment.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comments data - колаработирвня фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "390724"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_comment.display_name.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#кто сколько комментов оставил\n",
    "a = df_comment.groupby('display_name').agg({\n",
    "    'id': 'count'\n",
    "}).sort_values(by='id').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>390724.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.894120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>39.212408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9748.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id\n",
       "count  390724.000000\n",
       "mean        3.894120\n",
       "std        39.212408\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         1.000000\n",
       "75%         2.000000\n",
       "max      9748.000000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# список пользователей с количеством комментариев > 2000\n",
    "users_to_remove = a[a['id'] > 2200]['display_name']\n",
    "df_commen = df_commen[~df_commen['display_name'].isin(users_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных курсов: 2500\n",
      "Уникальных пользователей: 3633\n",
      "Размер финального датафрейма: (3633, 7)\n"
     ]
    }
   ],
   "source": [
    "sample_users = df_commen['id'].drop_duplicates().sample(5000, random_state=42)\n",
    "df_commen = df_commen[df_commen['id'].isin(sample_users)]\n",
    "\n",
    "top_courses = df_commen['course_id'].value_counts().head(2500).index\n",
    "df_commen = df_commen[df_commen['course_id'].isin(top_courses)]\n",
    "\n",
    "# Проверим результат\n",
    "print(f\"Уникальных курсов: {df_commen['course_id'].nunique()}\")\n",
    "print(f\"Уникальных пользователей: {df_commen['id'].nunique()}\")\n",
    "print(f\"Размер финального датафрейма: {df_commen.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_commen['userId'] = df_commen['display_name'].astype('category').cat.codes +2\n",
    "\n",
    "# Шаг 2: Создать финальную таблицу\n",
    "df_result = df_commen[['userId', 'course_id', 'rate', 'date', 'comment']].copy()\n",
    "\n",
    "# Опционально: сортировка\n",
    "df_ratings = df_result.sort_values(by=['userId']).reset_index(drop=True)\n",
    "df_ratings.drop_duplicates(inplace=True, \n",
    "                           subset=['userId', 'course_id']\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "rate\n",
       "5    2344\n",
       "4     987\n",
       "3     117\n",
       "2      83\n",
       "1      62\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings['rate'] = df_ratings['rate'].round().astype(int)\n",
    "df_ratings.rate.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(X, ratio=0.2, user_col='userId', item_col='course_id',\n",
    "                     rating_col='rate', time_col='date'):\n",
    "    # сортируем оценки по времени\n",
    "    X.sort_values(by=[time_col], inplace=True)\n",
    "    # список всех юзеров\n",
    "    userIds = X[user_col].unique()\n",
    "    X_train_data = []\n",
    "    X_test_data = []\n",
    "    y_train = []\n",
    "    y_test = []\n",
    "    for userId in tqdm_notebook(userIds):\n",
    "        curUser = X[X[user_col] == userId]\n",
    "        # определяем позицию, по которой делим выборку и размещаем данные по массивам\n",
    "        idx = int(curUser.shape[0] * (1 - ratio))\n",
    "        X_train_data.append(curUser[[user_col, item_col]].iloc[:idx, :].values)\n",
    "        X_test_data.append(curUser[[user_col, item_col]].iloc[idx:, :].values)\n",
    "        y_train.append(curUser[rating_col].values[:idx])\n",
    "        y_test.append(curUser[rating_col].values[idx:])\n",
    "    # cтекуем данные по каждому пользователю в общие массивы\n",
    "    X_train = pd.DataFrame(np.vstack(X_train_data), columns=[user_col, item_col])\n",
    "    X_test = pd.DataFrame(np.vstack(X_test_data), columns=[user_col, item_col])\n",
    "    y_train = np.hstack(y_train)\n",
    "    y_test = np.hstack(y_test)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6184\\1727997363.py:11: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for userId in tqdm_notebook(userIds):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c5d291c408949a59edb531049472600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2905 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((648, 2), 648, (2945, 2), 2945)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, len(y_train), X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## User Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserBased(BaseEstimator):\n",
    "    def fit(self, X, y, user_col='userId', item_col='course_id'):\n",
    "        X = X.copy()\n",
    "        # сохраним текущих пользователей и имеющиеся предметы\n",
    "        self.users = X[user_col].unique()\n",
    "        self.items = X[item_col].unique()\n",
    "        \n",
    "        X['y'] = y\n",
    "        # рассчитаем среднее значение рейтинга для пользователя и предмета\n",
    "        self.mean_y_user = X.groupby(user_col)['y'].mean()\n",
    "        self.mean_y_item = X.groupby(item_col)['y'].mean()\n",
    "        \n",
    "        # вычитаем среднюю оценку пользователя\n",
    "        X['y'] -= X[user_col].apply(lambda x: self.mean_y_user[x])\n",
    "        \n",
    "        # создаём векторы для каждого пользователя из просмотренных фильмов\n",
    "        # для неизвестных фильмов ставим оценку 0\n",
    "        self.user_ratings = pd.pivot_table(X, values='y', index=user_col,\n",
    "                                           columns=item_col, fill_value=0)\n",
    "        \n",
    "        # считаем попарную схожесть между юзерами\n",
    "        self.user_sim = cosine_similarity(self.user_ratings)\n",
    "        # также сделаем словарь - {значение user_col: index в user_ratings}\n",
    "        self.user_pos = dict()\n",
    "        for user in self.users:\n",
    "            self.user_pos[user] = np.argwhere(self.user_ratings.index.values == user)[0][0]\n",
    "        return self\n",
    "    \n",
    "    def predict_rating(self, pr_user, pr_item):\n",
    "        # если в обучающей выборке нет такого предмета\n",
    "        # или пользователя, то вернём 0\n",
    "        if not pr_item in self.items or not pr_user in self.users:\n",
    "            return 3\n",
    "    \n",
    "        sim_vector = self.user_sim[self.user_pos[pr_user]]\n",
    "        item_ratings = self.user_ratings.loc[:, pr_item]\n",
    "        \n",
    "        numerator = sim_vector.dot(item_ratings)\n",
    "        denominator = sim_vector.sum() - 1\n",
    "        \n",
    "        if denominator == 0:\n",
    "            pred = self.mean_y_user[pr_user]\n",
    "        else:\n",
    "            pred = self.mean_y_user[pr_user] + numerator / denominator\n",
    "        \n",
    "        # Ограничим предсказание значениями от 1 до 5\n",
    "        return np.clip(pred, 1.0, 5.0)\n",
    "\n",
    "    \n",
    "    def predict(self, X, user_col='userId', item_col='course_id'):\n",
    "        y = X[[user_col, item_col]].apply(lambda row: self.predict_rating(row[0], row[1]), axis=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "start predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6184\\994343726.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y = X[[user_col, item_col]].apply(lambda row: self.predict_rating(row[0], row[1]), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 891 ms\n",
      "Wall time: 397 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('start fitting...')\n",
    "ub = UserBased().fit(X_train, y_train)\n",
    "print('start predicting...')\n",
    "y_pred = ub.predict(X_test)\n",
    "# print('rmse = {}'.format(rmse(y_test,y_pred )))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = y_pred.fillna(3)\n",
    "y_pred.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_regression(y_true, y_pred):\n",
    "    print(\"RMSE:\", np.sqrt(mean_squared_error(y_true, y_pred)))\n",
    "    print(\"MAE:\", mean_absolute_error(y_true, y_pred))\n",
    "    print(\"R2:\", r2_score(y_true, y_pred))\n",
    "    print(\"MAPE:\", mean_absolute_percentage_error(y_true, y_pred))\n",
    "\n",
    "# evaluate_regression(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6184\\994343726.py:51: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y = X[[user_col, item_col]].apply(lambda row: self.predict_rating(row[0], row[1]), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 1.592807909647939\n",
      "MAE: 0.8024691358024691\n",
      "R2: -3.0693691078277077\n",
      "MAPE: 0.20612139917695474\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = ub.predict(X_train)\n",
    "y_pred2 = y_pred2.fillna(3)\n",
    "y_pred2 = y_pred2.round()\n",
    "evaluate_regression(y_train, y_pred2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Item Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ItemBased(BaseEstimator):\n",
    "    def fit(self, X, y, user_col='userId', item_col='course_id'):\n",
    "        X = X.copy()\n",
    "        # сохраним текущих пользователей и имеющиеся предметы\n",
    "        self.users = X[user_col].unique()\n",
    "        self.items = X[item_col].unique()\n",
    "        \n",
    "        X['y'] = y\n",
    "        # рассчитаем среднее значение рейтинга для пользователя и предмета\n",
    "        self.mean_y_user = X.groupby(user_col)['y'].mean()\n",
    "        self.mean_y_item = X.groupby(item_col)['y'].mean()\n",
    "        \n",
    "        # вычитаем среднюю оценку предмета\n",
    "        X['y'] -= X[item_col].apply(lambda x: self.mean_y_item[x])\n",
    "        \n",
    "        # создаём векторы для каждого фильма с оценками пользователя\n",
    "        # если пользователь не поставил оценку, то ставим 0\n",
    "        self.item_ratings = pd.pivot_table(X, values='y', index=item_col,\n",
    "                                           columns=user_col, fill_value=0)\n",
    "        \n",
    "        # считаем попарную схожесть между фильмами\n",
    "        self.item_sim = cosine_similarity(self.item_ratings)\n",
    "        \n",
    "        # также сделаем словарь {значение item_col: index в item_ratings}\n",
    "        self.item_pos = dict()\n",
    "        for item in self.items:\n",
    "            self.item_pos[item] = np.argwhere(self.item_ratings.index.values == item)[0][0]\n",
    "        return self\n",
    "    \n",
    "    def predict_rating(self, pr_user, pr_item):\n",
    "        # если в обучающей выборке нет такого предмета\n",
    "        # или пользователя, то вернём 0\n",
    "        if not pr_item in self.items or not pr_user in self.users:\n",
    "            return 0\n",
    "        \n",
    "        # считаем числитель и знаменатель дроби из формулы предсказания\n",
    "        numerator = self.item_sim[self.item_pos[pr_item]].dot(\n",
    "                        self.item_ratings.loc[:, pr_user])   \n",
    "        # вычитаем 1, так как схожесть предмета с самим собой равна 1,\n",
    "        # но модель не должна это учитывать\n",
    "        denominator = self.item_sim[self.item_pos[pr_item]].sum() - 1\n",
    "        \n",
    "        return self.mean_y_item[pr_item] + numerator / denominator\n",
    "\n",
    "    def predict(self, X, user_col='userId', item_col='course_id'):\n",
    "        y = X[[user_col, item_col]].apply(lambda row: self.predict_rating(row[0], row[1]), axis=1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "start predicting...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_6184\\2454844154.py:46: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  y = X[[user_col, item_col]].apply(lambda row: self.predict_rating(row[0], row[1]), axis=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 938 ms\n",
      "Wall time: 402 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('start fitting...')\n",
    "ib = ItemBased().fit(X_train, y_train)\n",
    "print('start predicting...')\n",
    "y_pred = ib.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.5436546865574305\n",
      "MAE: 4.426521950036381\n",
      "R2: -29.89298694617109\n",
      "MAPE: 0.9790395343196701\n"
     ]
    }
   ],
   "source": [
    "evaluate_regression(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Course data - контентная фильтрация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inf = df_info.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'id', 'title', 'is_paid', 'price', 'headline',\n",
       "       'num_subscribers', 'avg_rating', 'num_reviews', 'num_comments',\n",
       "       'num_lectures', 'content_length_min', 'published_time',\n",
       "       'last_update_date', 'category', 'subcategory', 'topic', 'language',\n",
       "       'course_url', 'instructor_name', 'instructor_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_combine = ['language', 'topic', 'instructor_name', 'category', 'subcategory', 'headline', 'title' ] \n",
    "# Обработка: приводим к нижнему регистру и заполняем пропуски\n",
    "df_inf[cols_to_combine] = df_inf[cols_to_combine].fillna('').apply(lambda col: col.str.lower())\n",
    "df_inf['description'] = df_inf[cols_to_combine].apply(lambda row: ' '.join(filter(None, row)), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>course_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>date</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>209</td>\n",
       "      <td>3426446</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Es un curso bastante entretenido y muy bien ex...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>1759</td>\n",
       "      <td>2359992</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Cause it's good</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>1005</td>\n",
       "      <td>1503540</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Ótimo curso! O Prof. Willian é bem didático. V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>1997</td>\n",
       "      <td>821986</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>useful concepts are covered</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2132</td>\n",
       "      <td>1341268</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-01-01</td>\n",
       "      <td>Boa apresentação.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>858</td>\n",
       "      <td>1178124</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>[Español] El curso esta muy bien muy recomenda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>2207</td>\n",
       "      <td>1978132</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>Good elaborations of topics &amp; easy to understa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1021</td>\n",
       "      <td>2013892</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>I am basically from banking domain with core b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>2836</td>\n",
       "      <td>2405042</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>細かい部分まで丁寧に教えてくださっています。</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>2817</td>\n",
       "      <td>4336504</td>\n",
       "      <td>5</td>\n",
       "      <td>2022-10-22</td>\n",
       "      <td>Отличный курс, как и все курсы у Дмитрия. Все ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3593 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  course_id  rate        date  \\\n",
       "437      209    3426446     5  2022-01-01   \n",
       "2301    1759    2359992     5  2022-01-01   \n",
       "1371    1005    1503540     5  2022-01-01   \n",
       "2586    1997     821986     5  2022-01-01   \n",
       "2767    2132    1341268     4  2022-01-01   \n",
       "...      ...        ...   ...         ...   \n",
       "1201     858    1178124     5  2022-10-22   \n",
       "2853    2207    1978132     5  2022-10-22   \n",
       "1390    1021    2013892     4  2022-10-22   \n",
       "3562    2836    2405042     4  2022-10-22   \n",
       "3543    2817    4336504     5  2022-10-22   \n",
       "\n",
       "                                                comment  \n",
       "437   Es un curso bastante entretenido y muy bien ex...  \n",
       "2301                                    Cause it's good  \n",
       "1371  Ótimo curso! O Prof. Willian é bem didático. V...  \n",
       "2586                        useful concepts are covered  \n",
       "2767                                  Boa apresentação.  \n",
       "...                                                 ...  \n",
       "1201  [Español] El curso esta muy bien muy recomenda...  \n",
       "2853  Good elaborations of topics & easy to understa...  \n",
       "1390  I am basically from banking domain with core b...  \n",
       "3562                             細かい部分まで丁寧に教えてくださっています。  \n",
       "3543  Отличный курс, как и все курсы у Дмитрия. Все ...  \n",
       "\n",
       "[3593 rows x 5 columns]"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_index</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>84482.0</td>\n",
       "      <td>ovation public speaking - speaking methods mas...</td>\n",
       "      <td>english public speaking ovation public speakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>84660.0</td>\n",
       "      <td>method lesson 4: speaking with modulation</td>\n",
       "      <td>english public speaking ovation public speakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>84664.0</td>\n",
       "      <td>method lesson 5: speaking with enthusiasm</td>\n",
       "      <td>english public speaking ovation public speakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>84626.0</td>\n",
       "      <td>method lesson 3: speaking with power</td>\n",
       "      <td>english public speaking ovation public speakin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>85752.0</td>\n",
       "      <td>content lesson 2: subject development</td>\n",
       "      <td>english presentation skills ovation public spe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  model_index       id                                              title  \\\n",
       "0           0  84482.0  ovation public speaking - speaking methods mas...   \n",
       "1           1  84660.0          method lesson 4: speaking with modulation   \n",
       "2           2  84664.0          method lesson 5: speaking with enthusiasm   \n",
       "3           3  84626.0               method lesson 3: speaking with power   \n",
       "4           4  85752.0              content lesson 2: subject development   \n",
       "\n",
       "                                         description  \n",
       "0  english public speaking ovation public speakin...  \n",
       "1  english public speaking ovation public speakin...  \n",
       "2  english public speaking ovation public speakin...  \n",
       "3  english public speaking ovation public speakin...  \n",
       "4  english presentation skills ovation public spe...  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample = df_inf[['id', 'title', 'description']]\n",
    "# определите model_index и сделайте его строкой\n",
    "sample = sample.reset_index().rename(columns = {'index': 'model_index'})\n",
    "sample['model_index'] = sample['model_index'].astype(str)\n",
    "sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ovation public speaking - speaking methods master class': 0,\n",
       " 'method lesson 4: speaking with modulation': 1,\n",
       " 'method lesson 5: speaking with enthusiasm': 2,\n",
       " 'method lesson 3: speaking with power': 3,\n",
       " 'content lesson 2: subject development': 4,\n",
       " 'content lesson 1:  writing introductions': 5,\n",
       " 'method lesson 6: speaking with gestures': 6,\n",
       " 'step-by-step mind mastery course': 7,\n",
       " 'game banao - unity 3d/2d game development in اردو / हिन्दी': 8,\n",
       " 'learn the art of horsehair hitching': 9,\n",
       " 'become a master wordsmith: learn 250+ advance level words': 10,\n",
       " 'fundamentals of engineering (fe) exam review': 11,\n",
       " 'next level js - modernes javascript mit es6 und neuer': 12,\n",
       " 'redirecting negative behavior': 13,\n",
       " 'financial literacy - what i wish i learned in school': 14,\n",
       " 'program manager introduction course': 15,\n",
       " 'business analysis: eliciting nonfunctional requirements': 16,\n",
       " 'business analysis: four core models for scoping requirements': 17,\n",
       " 'stress management in 3d: reduce stress and prevent symptoms.': 18,\n",
       " 'grab life by the obstacles - be your own life coach': 19,\n",
       " 'digital signal processing with matlab :การประมวลผลสัญญาณ dsp': 20,\n",
       " 'become master of programming and it concepts & fundamentals': 21,\n",
       " 'curso de comedia: guión e interpretación de monólogos': 22,\n",
       " 'essay writing for students: boost your essay grades to a+': 23,\n",
       " 'public speaking a practical approach': 24,\n",
       " 'discreet mathematics': 25,\n",
       " 'learn how to play now | ukulele beginners course': 26,\n",
       " 'create your own cryptocurrency exchange, token and airdrop': 27,\n",
       " 'erstelle deine eigenen linux lamp web server inkl. wordpress': 28,\n",
       " 'the fruite program': 29,\n",
       " 'must learn programming for testers - python-ruby-shell': 30,\n",
       " 'curso básico de psicogeriatría': 31,\n",
       " 'pmp exam preparation course': 32,\n",
       " 'unix with scripting for newbies': 33,\n",
       " 'bioestatística no epi info': 34,\n",
       " 'investing in direct participation oil and gas programs': 35,\n",
       " 'el lenguaje corporal': 36,\n",
       " 'the best truth about low self esteem': 37,\n",
       " 'inglês básico -  (aulas em inglês e português)': 38,\n",
       " 'matlab course': 39,\n",
       " 'intentional diversity (updated)': 40,\n",
       " 'pl-900 power platform fundamentals': 41,\n",
       " 'smartphone videomaking professionale': 42,\n",
       " 'easy videomaking': 43,\n",
       " 'database testing and sql for testers (top sql interview qs)': 44,\n",
       " 'ingeniería del software: fundamentos de uml usando papyrus': 45,\n",
       " 'top tips for selling at art & craft shows': 46,\n",
       " 'learn white board 3d animations with videoscribe.': 47,\n",
       " 'how to create engaging social media marketing content': 48,\n",
       " 'curso de modelización financiera avanzada': 49,\n",
       " '【おためし版】javaプログラミング入門': 50,\n",
       " 'curso de desenho e caricaturas': 51,\n",
       " 'graphic design mastery with coreldraw (practical projects)': 52,\n",
       " 'prezi in italiano - corso basic': 53,\n",
       " 'cracking the javascript coding interview: practice problems': 54,\n",
       " 'fundamental concepts of algebra': 55,\n",
       " 'kanban do zero': 56,\n",
       " 'snoredown sleep course': 57,\n",
       " 'aprenda jardinagem  caseira': 58,\n",
       " 'fingerstyle guitar songbook - fingerstyle for beginners': 59,\n",
       " 'java beginners course, learn by coding the hangman game': 60,\n",
       " 'ebay profits academy by trieu doan': 61,\n",
       " 'cardio & abs 7 day challenge': 62,\n",
       " 'yeni başlayanlar için 2 dakika temel i̇spanyolca': 63,\n",
       " 'ielts writing band 7+ with functional grammar': 64,\n",
       " 'speed math | cálculo mental rápido y ágil': 65,\n",
       " 'sacral chakra heaiing : an initiation into fun and pleasure': 66,\n",
       " 'air insulated substation design part 2': 67,\n",
       " 'owning the road to pageant royalty': 68,\n",
       " 'exploration of hatha yoga postures part 1': 69,\n",
       " 'impara dalla pratica python 3.10': 70,\n",
       " 'the wiccan rede explained': 71,\n",
       " 'photoshop cc for photographers': 72,\n",
       " 'advanced computer forensics': 73,\n",
       " 'fiber optics testing and measurements': 74,\n",
       " 'curso prático forex': 75,\n",
       " 'implementación de una nube privada con nextcloud': 76,\n",
       " 'event mastery - how to produce your own successful events': 77,\n",
       " 'learn everything in microsoft word (2016/2019/2021/365)': 78,\n",
       " 'uygulamalı stres ve başa çıkma yolları eğitimi': 79,\n",
       " 'the complete autocad for beginners to intermediate': 80,\n",
       " '5 piekielnych technik': 81,\n",
       " 'introdução à escrita científica': 82,\n",
       " 'almacenamiento en la nube: desde cero para principiantes': 83,\n",
       " 'launch a podcast on a budget': 84,\n",
       " 'apply excel vba in industry series - part i': 85,\n",
       " 'hafıza teknikleri ile x10 hızlı & kalıcı öğrenin': 86,\n",
       " 'cold email marketing mastery | sales emails by coursenvy ®': 87,\n",
       " 'hafıza teknikleriyle 2. sinif i̇ngilizce (konu-sözlük-test) 1': 88,\n",
       " 'java programming language': 89,\n",
       " 'curso completo de desenho realista - 3 cursos em 1': 90,\n",
       " 'como desenvolver a cultura digital': 91,\n",
       " 'ultimate facebook ads & marketing masterclass': 92,\n",
       " 'unreal engine - blueprint scripting 101': 93,\n",
       " 'mastering exponential and logarithmic functions': 94,\n",
       " 'programação pic18 com mplabx e compilador xc8': 95,\n",
       " 'seo blogging star networking': 96,\n",
       " 'basic portuguese a1': 97,\n",
       " 'convert any hardcopy system to electronic system using excel': 98,\n",
       " 'büro-yoga- fit am schreibtisch': 99,\n",
       " 'ripple development and training suit - part 1': 100,\n",
       " 'cómo detectar, prevenir y eliminar virus informáticos': 101,\n",
       " 'being  a dementia caregiver': 102,\n",
       " 'şan dersi - ses eğitimi': 103,\n",
       " 'advanced planning & scheduling with primavera p6 (11.5 pdus)': 104,\n",
       " \"intensive ho'oponopono workshop + 3,800 students enrolled!\": 105,\n",
       " 'the essentials of human interactions:': 106,\n",
       " 'building performance analysis course with leed ce hours': 107,\n",
       " 'photoshop : curso de diseño digital de 0 a 100': 108,\n",
       " 'aws solutions architect associate saa-c03 practice test 2022': 109,\n",
       " 'تعلم التصميم باستخدام برنامج الفوتوشوب': 110,\n",
       " 'registre você mesmo sua marca no inpi.': 111,\n",
       " 'instalación de red hat enterprise linux y derivados': 112,\n",
       " 's.o.s matemática': 113,\n",
       " 'effective presentations': 114,\n",
       " 'seguridad con selinux': 115,\n",
       " 'learn the language of quran and sunnah (part one)': 116,\n",
       " 'learn youtube keyword research and seo to grow your channel': 117,\n",
       " 'the ultimate guide to copper electroforming': 118,\n",
       " 'molecular engineering - intro to biomedical engineering': 119,\n",
       " '부자수업 패키지': 120,\n",
       " 'como desenvolver um algoritmo com clareza': 121,\n",
       " 'how to write chinese characters - beginner level - a1 - hsk1': 122,\n",
       " 'la dirección de arte': 123,\n",
       " 'nórdico antiguo | curso online en español del idioma vikingo': 124,\n",
       " 'leadlovers: configurações, email, páginas, ead e integrações': 125,\n",
       " \"le cv, la lettre de motivation et l'entretien efficace\": 126,\n",
       " 'spiritual disciplines of the christian life': 127,\n",
       " 'introducción a la psicología deportiva aplicada a esports': 128,\n",
       " 'consultor óptico profissional': 129,\n",
       " 'customer service 101: all the basics you need to know (2022)': 130,\n",
       " 'the complete introduction to the world of wine': 131,\n",
       " 'songwriting with bandlab - a beginner’s guide': 132,\n",
       " 'adobe premiere pro cc ile müzik klip çekimi ve editi': 133,\n",
       " 'passive income with crypto currency investing': 134,\n",
       " \"unreal engine 5 : création d'un jeu de zombie\": 135,\n",
       " 'nightwatch.js web application test automation': 136,\n",
       " 'vi̇op - vadeli i̇şlem piyasası hakkında her şey': 137,\n",
       " 'stockage en réseau san pour les débutants': 138,\n",
       " 'finanzas personales. hacia la libertad financiera': 139,\n",
       " 'presentation skills: smart presentation method': 140,\n",
       " 'supply chain segmentation analytics with python': 141,\n",
       " 'dual controller area network module and can data logger': 142,\n",
       " 'manual testing crash course for software testers': 143,\n",
       " 'easy asp .net web form for beginners html, css, sql ado .net': 144,\n",
       " 'astrology; reading the birthchart': 145,\n",
       " 'introducción a c++ moderno para programadores': 146,\n",
       " 'introduction to post producing for commercials and music vid': 147,\n",
       " \"dr. gatsby's home study course in experiential reframing\": 148,\n",
       " 'learn how to speak serbian part 2': 149,\n",
       " 'temple run clone for ios & android | top unity and c# course': 150,\n",
       " 'antigen antibody reactions': 151,\n",
       " 'remuneraciones salariales, tipos y formas de calculo': 152,\n",
       " 'expert advisor sem programação para forex e b3': 153,\n",
       " 'leadership mastery - complete guide to being a great leader': 154,\n",
       " 'ged math test: the complete guide': 155,\n",
       " 'transcréation : acquérir les bases de la traduction créative': 156,\n",
       " 'autocad course': 157,\n",
       " 'sap performance analysis for abap, bw, functional consultant': 158,\n",
       " 'curso de elaboración de champús sólidos': 159,\n",
       " 'quantum physics part-1': 160,\n",
       " 'como elaborar una demanda y como presentarla desde cero': 161,\n",
       " \"programming fundamentals - beginner's guide\": 162,\n",
       " 'matemática para iniciantes': 163,\n",
       " 'tekken 7 course: from beginner to advanced player': 164,\n",
       " 'külföldi munkavállalás és letelepedés érthetően': 165,\n",
       " 'trigonometry 1 with the math sorcerer': 166,\n",
       " 'problems on competitive coding': 167,\n",
       " 'what to do before, while, and after you hit record (youtube)': 168,\n",
       " 'curso full de autocad 2d en español': 169,\n",
       " 'fundamentos para la escritura de guiones para radio': 170,\n",
       " 'prevenção e reversão do declínio cognitivo': 171,\n",
       " 'イメージマスター社労士講座（お試し版）右脳を使って効率学習！': 172,\n",
       " 'curso artcam/ carveco em uma semana': 173,\n",
       " 'sócrates y los sofistas:': 174,\n",
       " 'maintenir et réparer  un système informatique  de a à z': 175,\n",
       " 'sıfırdan yapay zeka mühendisi olma kursu': 176,\n",
       " 'super curso de html e css + projetos práticos': 177,\n",
       " 'redação do enem: passo a passo para um bom texto': 178,\n",
       " 'dog agility for newcomers': 179,\n",
       " 'baralho cigano, prático e objetivo - curso completo': 180,\n",
       " '从零开始服装打板': 181,\n",
       " 'discover your career path & land a job you love in 12 weeks': 182,\n",
       " 'aprenda inglês com filmes 1 - pronúncia e conversação': 183,\n",
       " 'egregores y cómo liberarte de ellos': 184,\n",
       " '【swiftui coredata】  icloudと連携させてデータ共有できるメモアプリを作ろう': 185,\n",
       " 'şarampolden zirveye matematik.': 186,\n",
       " 'python data course: python for data analysis & visualization': 187,\n",
       " 'conceitos devops': 188,\n",
       " 'eletrônica prática: aprenda multisim com circuitos elétricos': 189,\n",
       " 'contabilidad para el régimen especial': 190,\n",
       " 'hr 4.0 - human resources management in industry 4.0': 191,\n",
       " 'food 4.0 - the food & beverage industry in industry 4.0': 192,\n",
       " 'adobe illustrator cc ¡desde cero!: curso para principiantes': 193,\n",
       " 'calcul des assemblages des structures métalliques': 194,\n",
       " 'adoption challenges and drivers of implementing industry 4.0': 195,\n",
       " 'water 4.0 - water management in industry 4.0': 196,\n",
       " 'get clients and make money as a holistic practitioner': 197,\n",
       " 'learn autocad 2d drafting': 198,\n",
       " 'manage project risks impeccably (12 pdus, pmi pmp renewal)': 199,\n",
       " 'corporate actions': 200,\n",
       " 'einkaufscontrolling - kennzahlen zur optimierung des einkauf': 201,\n",
       " 'como usar bem a vírgula e outros sinais.': 202,\n",
       " \"learn from lloyd: the do's & don'ts of numerology\": 203,\n",
       " 'cfps exam strategies': 204,\n",
       " '【scratch(スクラッチ）６】ゲーム入門（level1）。ゲームづくりの手順と手法がわかるゲームづくり入門コース': 205,\n",
       " 'work 4.0 - impact of industry 4.0 on workforce & workplace': 206,\n",
       " 'calcul paratique des ponts avec le logiciel st1': 207,\n",
       " 'how to build a sales funnel for your subscription business': 208,\n",
       " 'crea increíbles sitios web, en: php, mysql, ajax, + poo mvc!': 209,\n",
       " \"what's your story?\": 210,\n",
       " 'project cost management using google sheet with daily update': 211,\n",
       " \"programmation en python 3: du débutant à l'expert\": 212,\n",
       " 'hydraulics 102 - hydraulic components in depth': 213,\n",
       " 'technical sales presentations - upstream oil and gas': 214,\n",
       " '作曲・音楽制作のために知っておくべき初級音楽理論': 215,\n",
       " 'complete it support specialist course: networking': 216,\n",
       " 'feminine and masculine energy:the secret between energies': 217,\n",
       " 'mern stack 2022 - build ultimate cms (wordpress clone)': 218,\n",
       " 'become a aha certified cpr instructor for extra side income': 219,\n",
       " 'mastering mediumship': 220,\n",
       " '数千人の患者さんを診て完成した  整体・ストレッチ・マッサージ・カイロプラクティック・テーピング・治療講座': 221,\n",
       " '実際に日本一のお店、全国組織、資格を作った起業講座・ビジネスも人間関係もポジショニングが９割': 222,\n",
       " 'aptitude training for all competitive entrance exams': 223,\n",
       " 'cryptography: a hands-on approach': 224,\n",
       " 'guitarra moderna 2.0': 225,\n",
       " 'bootstrap４でかんたんキレイなウェブサイトを作ろう': 226,\n",
       " 'português para concursos - 1.2 sintaxe': 227,\n",
       " 'comunicação eficaz 2.0': 228,\n",
       " 'intro to database app dev w/spring boot, angular, postgres': 229,\n",
       " '数列を理解するための数学講座[なんとなくや公式丸暗記とはサヨナラ]': 230,\n",
       " 'supergerente - torne-se um gerente de sucesso': 231,\n",
       " 'comienza con excel: curso destacado para principiantes': 232,\n",
       " '7 practical steps to reach your goals.': 233,\n",
       " 'spiritual weight loss': 234,\n",
       " 'employee manual handling for coffee shops,': 235,\n",
       " 'digital marketing data: dmps, device graphs, and regulation': 236,\n",
       " 'persönlichkeitsentwicklung 2.0': 237,\n",
       " 'aprenda a lançar produtos, serviços e infoprodutos digitais': 238,\n",
       " 'european portuguese: basic tenses': 239,\n",
       " 'sql server para analistas de negocios': 240,\n",
       " 'learn ethical hacking on any computer, mobile, account': 241,\n",
       " 'profi instagram marketing 2022: igtv, instagram reels & live': 242,\n",
       " 'basics of mechanical design engineering (2022)': 243,\n",
       " 'user experience design用戶體驗設計思考與實作': 244,\n",
       " 'intermediate fusion 360': 245,\n",
       " 'assertiveness masterclass - how to be assertive & likeable': 246,\n",
       " 'الكورس العربي الاول في كتابة البحوث العلمية': 247,\n",
       " 'dress for success with 5 elements theory': 248,\n",
       " 'the pursuit of wonder: tools for philosophical literacy': 249,\n",
       " 'corso microsoft excel per principianti, facile e veloce.': 250,\n",
       " 'neurofitness - bem estar mental': 251,\n",
       " 'know your worth: set boundaries with self-confidence': 252,\n",
       " 'uipath - the complete rpa training course (2022)': 253,\n",
       " 'product photographer: ecommerce product photography course': 254,\n",
       " 'hashcat for penetration testers': 255,\n",
       " 'inglês básico - oscar english': 256,\n",
       " 'smart negotiating masterclass 2.0': 257,\n",
       " 'formation en naturopathie certifiante': 258,\n",
       " 'initiation au clair-ressenti, énergie & conscience': 259,\n",
       " 'the path of christian mysticism and meditation': 260,\n",
       " \"biotechnology: a beginner's guide to gene editing\": 261,\n",
       " 'how to mark your presence in group discussion': 262,\n",
       " 'comic art master class: the crimson cat | inking & coloring': 263,\n",
       " 'declaração de imposto de renda pessoa física 2022': 264,\n",
       " 'aplique duplique': 265,\n",
       " 'ahmet aday - klasik gitar dersleri': 266,\n",
       " 'kyc know your customer bootcamp (5 hours, new for 2022)': 267,\n",
       " 'marxism: english literature': 268,\n",
       " 'humanities: academic writing and publishing': 269,\n",
       " 'como criar seu release com poucos passos': 270,\n",
       " 'digital electronics': 2413,\n",
       " 'spark it: crowdfunding possibilities into reality': 272,\n",
       " 'la alimentación es terapia milagrosa': 273,\n",
       " 'estilo de vida saludable': 274,\n",
       " 'pte academic: strategies & test hacks: advanced': 275,\n",
       " 'complete shopify dropshipping millionaire course 2.0': 276,\n",
       " 'curso de vim desde principiantes a avanzado': 277,\n",
       " 'c語言引路人 ： 從零開始，向下紮根': 278,\n",
       " 'micro-autologous fat transplantation introduction - japanese': 279,\n",
       " 'selenium webdriver con python + pytest desde cero': 280,\n",
       " 'c# meisterkurs: lerne c# programmierung von a-z in 4 wochen': 281,\n",
       " 'modern baker. delicious and healthy bread.': 282,\n",
       " 'affinity photo-tutorial – grundlagen': 283,\n",
       " 'ux interviews masterclass': 284,\n",
       " 'sıfırdan i̇rəli səviyyəyə python proqramlaşdırma dili': 285,\n",
       " 'android automotive - custom rom - aosp - car launcher- kiosk': 286,\n",
       " 'best gre math 4-week course | target gre 165+': 287,\n",
       " 'arayavart complete tutorial of shopify app development': 288,\n",
       " 'sql for beginners: write queries and create database': 289,\n",
       " 'sales pro mate guide or how to sell any idea to anyone.': 290,\n",
       " \"lire et ecrire l'arabe en seulement 15 leçons\": 291,\n",
       " 'aprende a aplicar la neurociencia en tus clases.': 292,\n",
       " 'resilient work culture to prevent sexual harassment': 293,\n",
       " 'java ee : devenez développeur des applications entreprises': 294,\n",
       " 'master any song using hindustani classical tools': 295,\n",
       " 'scratch w nauce programowania - przykłady gier i skryptów.': 296,\n",
       " 'احترف تصميم واجهات المواقع والتطبيقات ui/ux with adobe xd': 297,\n",
       " 'design and manifest better life': 298,\n",
       " 'psychology in a nutshell': 299,\n",
       " '轮滑高级教程': 300,\n",
       " 'small engine diagnostics': 301,\n",
       " 'design of wastewater treatment plants for onsite projects': 302,\n",
       " 'certified cost professional(ccp)- exam preparation': 303,\n",
       " 'bpm, bpmn, ferramentas e modelagem de processos com bizagi': 304,\n",
       " 'behavior mastery: 5 tools 2 transform all behaviors inc adhd': 305,\n",
       " 'tsuboki japanese foot massage': 306,\n",
       " '【scratch（スクラッチ）5】数式を使って、通学路を登校させる、持久走対決をする、空き地に作図する': 307,\n",
       " 'realidad virtual 360 aplicada al diseño e interiorismo': 308,\n",
       " 'best chess course for complete beginner': 309,\n",
       " 'maîtriser node.js et son écosystème (npm, express, mongo, …)': 310,\n",
       " 'social network analysis': 311,\n",
       " 'how to sell when your clients don’t look like you': 312,\n",
       " 'learn practical palm reading : how to read palm lines': 313,\n",
       " 'the cloud course': 314,\n",
       " 'impressão 3d profissional - do polímero ao produto': 315,\n",
       " '【adobe  premiere pro】premiere pro徹底解説～基本から実践的なテクニックまで': 316,\n",
       " 'curso básico de redes': 317,\n",
       " 'solidworks simulation parça ve montaj analiz eğitimi': 318,\n",
       " 'creating the afro dance body beginner 2': 319,\n",
       " 'meditation - a way of being': 320,\n",
       " 'art of photography with hands-on trainings in urdu/hindi': 321,\n",
       " 'دليل المبتدئين فى الربح من الإنترنت': 322,\n",
       " 'pentesting y hacking con windows': 323,\n",
       " 'intelligence analysis (comprehensive - levels 1, 2 and 3)': 324,\n",
       " 'learning cfd with ansys fluent (workbench)': 325,\n",
       " 'situation awareness 360': 326,\n",
       " 'الكورس التعليمى لتجاره الخيارات الثنائيه': 327,\n",
       " \"formation systeme io l'outil clé pour vos formations\": 328,\n",
       " 'how to write a resume for australian market?': 329,\n",
       " 'beginning jazz improvisation': 330,\n",
       " 'sketchnotes - einführung in die welt der visuellen notizen': 331,\n",
       " 'introducción a la milenaria sabiduría de la kabalá hebrea': 332,\n",
       " 'инфаструктура открытых ключей (pki). часть № 3.': 333,\n",
       " 'how to make an amazon affiliate store with wix': 334,\n",
       " 'actualización en ortodoncia y ortopedia maxilar': 335,\n",
       " 'unconscious bias - die basics': 336,\n",
       " 'sql injection for beginners': 337,\n",
       " 'weight loss ~ keep fit & healthy, follow natural lifestyle': 338,\n",
       " 'life transformation course': 339,\n",
       " 'medición de cantidades de obra y presupuestos': 340,\n",
       " 'машинное обучение в python: machine learning & data science': 341,\n",
       " \"paragraph writing simplified - level one - beginner's guide!\": 342,\n",
       " 'intro to watercolor florals': 343,\n",
       " \"pro tools - the beginner's guide\": 344,\n",
       " '10 passos para se comunicar melhor': 345,\n",
       " 'spring security': 346,\n",
       " 'hafıza teknikleriyle 3. sinif i̇ngilizce (konu-sözlük-test) 1': 347,\n",
       " 'acentuación de las palabras': 348,\n",
       " 'complete wordpress web development mastery course.': 349,\n",
       " 'curso básico de canva 2022: crea diseños gráficos desde cero': 350,\n",
       " 'instagram marketing for local business: the complete guide': 351,\n",
       " 'key stage 2 maths': 352,\n",
       " 'devops - mão na massa!': 353,\n",
       " 'let nature to help you heal': 354,\n",
       " 'mega course - vmware vsphere 7.0 boot camp - part 1 w. ebook': 355,\n",
       " 'curso de marketing digital completo 2022': 356,\n",
       " 'get ready for kindergarten, learn english phonics (part 1)': 357,\n",
       " 'programación en lenguaje c orientado a microcontroladores': 358,\n",
       " 'power system protection: transformer protection': 359,\n",
       " 'esp32 and internet of things for absolute beginners': 360,\n",
       " 'applied sport psychology - psychological skills training': 361,\n",
       " 'azure app service - aplicações web app e containers docker': 362,\n",
       " 'learn to teach online like a pro': 363,\n",
       " 'test your fundamentals : digital logic design': 364,\n",
       " 'aktywny senior - trening dedykowany dla osób starszych': 365,\n",
       " 'spring framework tutorial in hindi | spring core': 366,\n",
       " 'estratégia quant: equilíbrio das cores em gráficos de preços': 367,\n",
       " 'python 1500: practice missions': 368,\n",
       " 'introduction into supply chain analytics': 369,\n",
       " 'diksiyon eğitimi': 370,\n",
       " 'wedding mc masterclass from beginners to professional part 2': 371,\n",
       " 'la via della libertà - benessere psicofisico applicato': 372,\n",
       " 'bbc microbit : learner to programmer 2022': 373,\n",
       " 'stress management - transform your relationship with stress': 374,\n",
       " 'how to grow your tiktok account - ultimate guide': 375,\n",
       " 'how to make god known': 376,\n",
       " 'the patient experience matters': 377,\n",
       " 'a level genetics - any level student welcome': 378,\n",
       " 'check point troubleshooting course [for ccse & ccta] -2022': 379,\n",
       " 'iso 50001:2018 energy management systems': 380,\n",
       " 'python machine learning || build real world projects': 381,\n",
       " 'agile scrum repair guide: how to reboot your scrum team': 382,\n",
       " 'aprenda sobre matriz gut (gravidade - urgência - tendência)': 383,\n",
       " 'angular in arabic the complete guide (2022)': 384,\n",
       " 'master de modelado de props para series animadas': 385,\n",
       " 'treasures of the hive...from bees to recipes!': 386,\n",
       " 'ultimate beginners guide to power bi - part 1': 387,\n",
       " 'hedef belirleme ve sonuç alma': 388,\n",
       " 'master the new 1003 as a mortgage loan processor, mlo, & uw': 389,\n",
       " 'introducere în bioetică și deontologia medicală': 390,\n",
       " 'curso de tarot dos anjos': 391,\n",
       " 'cisco duo zero trust - mfa vpn authentication': 392,\n",
       " 'alimentación y ejercicio físico: microbiota fortalecida.': 393,\n",
       " 'ms project 2019': 394,\n",
       " 'microsoft excel: formulas for beginners': 395,\n",
       " 'amci introduction to medical coding (i2mc) course': 396,\n",
       " 'curso intensivo de autocad desde cero-actualizado 2021-2022': 397,\n",
       " 'small business financials': 398,\n",
       " 'select mutual funds without advisor in 30 mins (blueprint).': 399,\n",
       " 'an introduction to google earth engine (gee)': 400,\n",
       " 'creación de cursos online rentables y escalables': 401,\n",
       " 'remagic your self': 402,\n",
       " 'modern computer vision™ pytorch, tensorflow2 keras & opencv4': 403,\n",
       " 'algebra: prodotti notevoli': 404,\n",
       " 'scala at light speed': 405,\n",
       " 'narrativas de videojuegos soulsborne': 406,\n",
       " 'covid tracker application in java spring boot for beginners': 407,\n",
       " 'adım adım hac rehberi': 408,\n",
       " '11 days of self-discovery': 409,\n",
       " 'manufacturing execution system (mes) - fundamentals': 410,\n",
       " 'understanding html and css': 411,\n",
       " \"your resilient brain and you, a user's guide\": 412,\n",
       " 'how to get the lowest interest rate on a mortgage': 413,\n",
       " 'technical writing demystified': 414,\n",
       " 'a guide to types of buildings and activities performed in it': 415,\n",
       " 'mapas mentais - a estratégia dos gênios | passo a passo': 416,\n",
       " 'the power of body language and public speaking': 417,\n",
       " 'fiverr freelancing for 3d product designers and engineers': 418,\n",
       " 'coping with stress': 419,\n",
       " 'sql server programming and stored procedures fundamentals': 420,\n",
       " 'tabla of advance level part 1 -- teentaal rau': 421,\n",
       " 'oficina de feitio de rapé': 422,\n",
       " 'sibelius pour débutants': 423,\n",
       " 'sap fiori para desenvolvedores': 424,\n",
       " '【2022最新版】adobe premiere pro 超入門！初心者でもサクサクわかるプレミアプロ入門コース': 425,\n",
       " 'master the basics of pyqt': 426,\n",
       " 'full stack series - [free segment of  part1]': 427,\n",
       " 'curso basico de cctv': 428,\n",
       " 'harmonogramowanie produkcji - cz. 1': 429,\n",
       " 'safety pin - how to improve your self-esteem': 430,\n",
       " 'freecad for mechanical engineers : technical 2d drawing': 431,\n",
       " 'narzędzia canvas': 432,\n",
       " 'phytothérapie moderne de la théorie à la pratique': 433,\n",
       " 'как создать подкаст с нуля': 434,\n",
       " 'master critical financial calculations in 1 month': 435,\n",
       " 'karma astrolojiye giriş ve dört önemli sabit yıldız': 436,\n",
       " 'convolution neural network with data augmentation': 437,\n",
       " 'curso de elaboración de planos con solidworks | jc': 438,\n",
       " 'corso scrittura professionale': 439,\n",
       " 'c++ programming essentials for beginners': 440,\n",
       " 'programming for non-programmers': 441,\n",
       " 'structural design of foundations for structures': 442,\n",
       " 'book marketing: amazon launch success & superior socialmedia': 443,\n",
       " 'capm: certified associate in project management (pmi-100)': 444,\n",
       " 'introduction to mechanical ventilator modes': 445,\n",
       " 'ccsk practice exam and exam reference links': 446,\n",
       " 'microneedling scars wrinkles hair growth & deepchemical peel': 447,\n",
       " 'python for beginners': 26224,\n",
       " 'dropshiping : formation complète de a à z pour réussir': 449,\n",
       " 'pintura de mestre': 450,\n",
       " 'another view of debian linux and security hardening + ebook': 451,\n",
       " 'email marketing tools ultimate guide أدوات التسويق بالإيميل': 452,\n",
       " 'advance self hypnosis & hypnotherapy training': 453,\n",
       " 'tinkercad ve arduino uygulamalı başlangıç seviyesi eğitim': 454,\n",
       " 'how to become a web designer from scratch - phase 1 of 3': 455,\n",
       " 'play winning chess': 456,\n",
       " 'fundamentos de bioquímica,química analítica laboratorio clí.': 457,\n",
       " 'crear una aplicación para escritorio para gestión médica': 458,\n",
       " 'introduction to forex trading for beginners': 459,\n",
       " '¡master en cae y análisis mecánico!': 460,\n",
       " 'vital communication: learn fundamental interpersonal skills.': 461,\n",
       " 'google ile i̇hracat': 462,\n",
       " 'nomenclatura y formulación de alcanos alquenos y alquinos': 463,\n",
       " 'master en react: aprender reactjs, hooks, mern, nodejs, jwt+': 464,\n",
       " 'learn to how to play the otamatone!': 465,\n",
       " 'learn revit architecture- with full project': 466,\n",
       " 'exempt market proficiency course (emp) part - one': 467,\n",
       " 'python para trade de criptomoedas (bitcoin, ethereum, etc.)': 468,\n",
       " 'curso de modelo e manequim online': 469,\n",
       " 'paint the ocean you want to see': 470,\n",
       " 'rediscover your life purpose, confidence and the real you !': 471,\n",
       " 'introduction to sales channels.': 472,\n",
       " 'how to paint a monochromatic portrait  in oils': 473,\n",
       " 'experto en articulate storyline 3 y storyline 360': 474,\n",
       " 'excel business modeling - the in-depth street smart toolkit.': 475,\n",
       " 'x86 assembly programming': 476,\n",
       " \"how to be a pastor's wife\": 477,\n",
       " 'how to make handmade jewelry': 478,\n",
       " 'curso de piano para tocar himnos': 479,\n",
       " 'aprende inglés desde cero': 480,\n",
       " 'the ultimate meditation pracise': 481,\n",
       " 'professional game background graphics design in photoshop': 482,\n",
       " 'desenho artístico do zero': 483,\n",
       " 'positive mental attitude': 484,\n",
       " 'anti-trafficking 101 + allied issues & laws - trafficking': 485,\n",
       " 'learn business case development': 486,\n",
       " 'máster en dax y power pivot de la a a la z': 487,\n",
       " 'an introduction to safeguarding children': 488,\n",
       " 'become a jewelry designer, design your own piece of jewelry': 489,\n",
       " 'how to become a handbag designer & how to sew a handbag': 490,\n",
       " 'l.a.s.d. annual orientation': 491,\n",
       " 'curso de caracterización teatral': 492,\n",
       " 'elasticsearch 101': 493,\n",
       " '¿cómo mejorar la relación de pareja?: el amor que perdura.': 494,\n",
       " '微信行銷課程-初階班 wechat marketing basic class(english subtitles)': 495,\n",
       " 'powerapps for beginners': 496,\n",
       " 'intro ao ayurveda': 497,\n",
       " 'marketing digital de serviços': 498,\n",
       " 'akbar & birbal stories - அக்பர் & பீர்பால் கதைகள் (தமிழில்)': 499,\n",
       " 'klasik arapça eğitim seti, 100 ders, 1000 kelime': 500,\n",
       " 'comandos elétricos em sistemas de refrigeração': 501,\n",
       " 'tradingview ile riskinizi azaltın, kazancınızı arttırın': 502,\n",
       " 'retenções de tributos do zero a profissional- essencial': 503,\n",
       " 'create financial abundance': 504,\n",
       " 'powerpoint para apple. de cero a experto.': 505,\n",
       " 'leer nederlands in het nederlands 1: voor absolute beginners': 506,\n",
       " 'curs de franceză în franceză 2: cele mai frecvente cuvinte': 507,\n",
       " 'contract administration for engineers إدارة العقود للمهندسين': 508,\n",
       " 'olandeză în olandeză 3: cele mai folosite 1000 de cuvinte': 509,\n",
       " 'introduction to intelligence': 510,\n",
       " 'scoring 5 above target (pmi-rmp)® exam': 511,\n",
       " 'la justice pénale internationale (droit international pénal)': 512,\n",
       " 'filmora x (10.5) - video editing masterclass for beginners': 513,\n",
       " 'homeschool launch course': 514,\n",
       " 'create the most memorable and valuable brand experience': 515,\n",
       " 'automatización avanzada de pruebas de software': 516,\n",
       " 'spring batch com microservices para etl': 517,\n",
       " 'die hypno-mental-diät...': 518,\n",
       " 'sıfırdan i̇leri seviyeye korece kursu 3': 519,\n",
       " 'modal verbs. a practical grammar course': 520,\n",
       " 'google slides for beginners': 521,\n",
       " 'gestão para negócio online com cursos': 522,\n",
       " 'montage vidéo débutant': 523,\n",
       " 'existência e reflexão': 524,\n",
       " '3000 días para conseguir salud para toda la vida': 525,\n",
       " 'métodos de cálculo muestral y muestreo para investigación': 526,\n",
       " 'grade 10 maths: linear systems and matrices (k-12 / uae)': 527,\n",
       " 'cantonese for complete beginners (free taster course)': 528,\n",
       " 'montage vidéo intermédiaire': 529,\n",
       " 'vegas pro 18': 530,\n",
       " 'python malayalam - beginner to expert': 531,\n",
       " '職場で困ったときも怖くない！ 自分で解決する力を身につけるための事例で学ぶit入門講座': 532,\n",
       " 'python for financial markets analysis': 533,\n",
       " 'montage vidéo expert': 534,\n",
       " 'power query para power bi y excel': 535,\n",
       " 'virtual reality and mental healthcare': 536,\n",
       " 'internetworking and subnetting mcq practice (networking)': 537,\n",
       " 'digital electronics - complete course (72+ hours)': 538,\n",
       " 'how to create a website with wordpress and elementor in 2022': 539,\n",
       " 'french language course: learn french/ speak it like natives': 540,\n",
       " '[new]learn javascript for web development by examples': 541,\n",
       " 'video marketing | an introduction': 542,\n",
       " 'tableau financial reporting & financial analysis masterclass': 543,\n",
       " 'correlations, association & hypothesis testing (with python)': 544,\n",
       " 'yoruba speaking made easy- speak yoruba language in 30 days': 545,\n",
       " 'negotiation with chinese 101: your essential business skills': 546,\n",
       " 'business development and b2b sales mastery': 547,\n",
       " 'إنشاء محتوي تعليمي تفاعلي باستخدام ستوري لاين 360': 548,\n",
       " 'အခြေခံ ပရိုဂရမ်ရေးခြင်း (programming fundamentals)': 549,\n",
       " 'violão iniciante para educadores infantis': 550,\n",
       " 'application virtualization మరియు app-v': 551,\n",
       " 'curso de libras!': 552,\n",
       " 'o curso definitivo para alta performance': 553,\n",
       " 'curso práctico para programación de avr -curso 3': 554,\n",
       " 'certificate in professional dowsing - accredited masterclass': 555,\n",
       " 'essential business analysis for consultants': 556,\n",
       " 'latvian mythology': 557,\n",
       " 'chinese allegories : your smart/easy option to learn chinese': 558,\n",
       " 'food safety for : chefs and food handlers': 559,\n",
       " 'vue 3 incl. router, vuex, pinia, composition api & firebase': 560,\n",
       " 'matemática (para leigos): regra de três simples e composta': 561,\n",
       " 'microsoft office excel: niveau introduction au niveau avancé': 562,\n",
       " 'marketing psychology, emotional intelligence, neuromarketing': 563,\n",
       " 'complete study of card payments process in digital banking': 564,\n",
       " 'administração para area de exatas': 565,\n",
       " 'mastering ielts speaking: the model answers edition': 566,\n",
       " 'master full stack .net architecture using c#': 567,\n",
       " 'topografía y explanaciones con autodesk civil 3d': 568,\n",
       " 'fundamentals of the algorithm in programming languages': 569,\n",
       " 'baştan sona akıllı kesit tasarımı eğitimi /(section design)': 570,\n",
       " 'learn baby massage with baby college': 571,\n",
       " \"unreal engine 4 - the absolute beginner's guide\": 572,\n",
       " '3 modern acrylic impressionistic landscape paintings': 573,\n",
       " 'learn game design free': 574,\n",
       " 'complete python bootcamp for everyone from zero to hero 2023': 575,\n",
       " 'python beginner to expert': 576,\n",
       " 'beginning object-oriented programming with c#': 577,\n",
       " 'linux administration 2022 - deep dive': 578,\n",
       " 'a practical introduction to opc ua': 579,\n",
       " 'nice guy syndrome recovery': 580,\n",
       " 'qgis essentials: beginner to practitioner': 581,\n",
       " 'meraki in depth': 582,\n",
       " 'how to define a brand identity': 583,\n",
       " 'treinamento seguidor comprador': 584,\n",
       " 'كورس النجاح بعلامة 140+ في اختبار الدوولينجو خلال 14 يوماً': 585,\n",
       " 'object oriented programming c#': 586,\n",
       " 'biomecánica aplicada al tenis': 587,\n",
       " 'big data and machine learning fundamentals': 588,\n",
       " \"animate'de basit animasyonlar yapmak çok kolay.\": 589,\n",
       " 'knime analytics platform per data scientists, corso base': 590,\n",
       " 'piano para adultos iniciantes': 591,\n",
       " 'english for german speakers': 592,\n",
       " 'complete autocad 2021 course : [both 2d and 3d]-mechanical': 593,\n",
       " 'learn social media on 7+ platforms': 594,\n",
       " 'learn facebook ads & facebook marketing': 595,\n",
       " 'make it stick, write it down: a journaled philosophy.': 596,\n",
       " '5 segredos para transformar seu ensino': 597,\n",
       " 'html, css, javascript and bootstrap for web designers': 598,\n",
       " 'statistics : computer oriented statistical methods': 599,\n",
       " 'amazon seller training in malayalam- ആമസോൺ ട്രെയിനിങ്  2022': 600,\n",
       " 'linguagem r: módulo básico com aplicações florestais': 601,\n",
       " 'low-poly character modeling & animation in blender for unity': 602,\n",
       " 'tips&tricks to make &upload your course on udemy-unofficial-': 603,\n",
       " '入門 amazon eks：在一天內開啟你的 kubernetes on aws 旅程': 604,\n",
       " 'learn instagram ads & marketing': 605,\n",
       " 'learn twitter ads & marketing': 606,\n",
       " 'learn linkedin ads & marketing': 607,\n",
       " 'learn youtube ads & marketing': 608,\n",
       " 'learn pinterest ads & marketing': 609,\n",
       " 'sleep special techniques:': 610,\n",
       " 'learn tumblr ads & marketing': 611,\n",
       " 'substation grounding': 612,\n",
       " 'be aware of data science': 613,\n",
       " 'aaa game 3d character art course module 01 - body': 614,\n",
       " 'alfabetização infantil, método fônico para crianças': 615,\n",
       " 'cardiac anatomy and physiology': 616,\n",
       " 'catia v5r20 fundamentos': 617,\n",
       " 'promob plus cozinha planejada': 618,\n",
       " 'nx 12 fundamentos': 619,\n",
       " 'nx 11 - fundamentos': 620,\n",
       " 'master agile estimation: top techniques for scrum & kanban': 621,\n",
       " 'pentesting fundamentals for beginners': 622,\n",
       " 'fashion 101 | design & styling- textures, shaping, & more!': 623,\n",
       " 'technical analysis course in hindi': 624,\n",
       " 'curso internacional de supervivencia': 625,\n",
       " 'cold emailing and cold calling mastery': 626,\n",
       " 'sql bootcamp: zero to hero using microsoft sql server': 627,\n",
       " 'docker + security + aws basics + jenkins': 628,\n",
       " 'crisp-ml(q) - data pre-processing using python': 629,\n",
       " 'shell scripting': 630,\n",
       " '【初心者向け】初めてのvrew（ブリュー）入門。動画編集にかかる時間を一気に減らし、効率良く字幕テロップも付けよう！': 631,\n",
       " '【macではじめる】初めてのecammlive入門！本格的なライブ配信や動画撮影に使えるプロ動画配信環境を手に入れよう': 632,\n",
       " 'управление проектами: полное погружение в pmbok7-2021': 633,\n",
       " 'tarô; arcanos maiores e menores - curso completo': 634,\n",
       " 'asterisk pbx with database/api driven call center solution': 635,\n",
       " 'mastering the it networking fundamentals: full course': 636,\n",
       " 'criando um controle de vendas desktop com c# e mysql': 637,\n",
       " 'facebook messenger chatbots for business': 638,\n",
       " 'digital communications strategy for non profits': 639,\n",
       " 'gradle plugin masterclass': 640,\n",
       " 'master python programming (arabic)': 641,\n",
       " 'curso básico de typescript - empieza con este lenguaje': 642,\n",
       " 'curso básico de react - empieza con este framework js': 643,\n",
       " 'unleash your avatar - discover & embody your higher self': 644,\n",
       " 'corso base di programmazione per principianti': 645,\n",
       " 'android clean architecture:mvvm, jetpack components + kotlin': 646,\n",
       " 'masterclass on building a business budget': 647,\n",
       " 'pizza italiana 100% - basic course (english version)': 648,\n",
       " 'lash extensions beginners course': 649,\n",
       " 'shamballa multidimensional healing': 650,\n",
       " 'how to create a nice looking sales website in 1 hour or less': 651,\n",
       " 'how to create your content for social media': 652,\n",
       " 'introduction to online selling: sales funnel': 653,\n",
       " 'email marketing for business owners': 654,\n",
       " 'get an offer at a faang company (behavioral interviewing)': 655,\n",
       " 'getting more customers for your business on google search.': 656,\n",
       " 'supply chain distribution channel strategies': 657,\n",
       " 'module 5 : supply chain aggregate planning and s&op planning': 658,\n",
       " 'learn medical anatomy': 659,\n",
       " 'how to be a voice actor': 660,\n",
       " 'managing your files and document with simple tech tools': 661,\n",
       " 'creatively designing your social media content': 662,\n",
       " 'improving result in your marketing & business': 663,\n",
       " 'google analytics beginner class': 664,\n",
       " 'managing your business activities with simple tech tools': 665,\n",
       " 'internal structure to put in your business': 666,\n",
       " 'how to create a marketing plan for your business': 667,\n",
       " 'get algebra 1 ready!': 668,\n",
       " 'udemy course creation for web developers (unofficial)': 669,\n",
       " 'gérer tous ses mots de passe avec dashlane (100% sécurisé)': 670,\n",
       " 'professional scrum master-psm1 practice exam questions 2022': 31375,\n",
       " 'tricentis tosca tutorial': 672,\n",
       " 'introducción a fortran | lenguaje de alto rendimiento': 673,\n",
       " 'the complete guide to american english pronunciation': 674,\n",
       " 'liquidator profit system-surplus arbitrage affiliate insight': 675,\n",
       " 'pop piano for fun': 676,\n",
       " 'learn punjabi reading & writing in an hour': 677,\n",
       " 'skupovi': 678,\n",
       " 'learn to build php api': 679,\n",
       " '7-day confidence building challenge': 680,\n",
       " 'turco nivel 1 para hispanohablantes': 681,\n",
       " 'learn javascript for beginners (full course with examples)': 682,\n",
       " 'fivem lua scripting': 683,\n",
       " 'ピアノ初心者入門コース\\u3000\\u3000ピアノで弾こう！\\u3000日本の歌 ～「故郷（ふるさと）」～': 684,\n",
       " 'fiverr mastery: become a freelancer and sell like the top 1%': 685,\n",
       " 'skincare made simple: the solution to radiant skin': 686,\n",
       " 'curso certificado de masaje relajante isla verde spa': 687,\n",
       " 'finanças pessoais - como conquistar o emprego dos sonhos?': 688,\n",
       " 'kurs tworzenia zapytań w języku sql z użyciem postgresql': 689,\n",
       " 'ableton live - sound design bass masterclass.': 690,\n",
       " 'business autopilot and marketing automation': 691,\n",
       " 'simple options strategy to trade stock earnings & volatility': 692,\n",
       " 'algebra 1 mastered': 693,\n",
       " '《拆解情緒地雷—當孩子的心靈捕手》音頻課程': 694,\n",
       " 'microsoft excel 2019 associate': 695,\n",
       " 'edição pra redes sociais com davinci resolve': 696,\n",
       " 'lean management und industrie 4.0 erfolgreich kombinieren': 697,\n",
       " 'lei da atração desmistificada': 698,\n",
       " 'formation python 2022 - débutant à expert': 699,\n",
       " 'energy management economics': 700,\n",
       " 'chip next generation sequencing analysis: complete a to z': 701,\n",
       " 'excel 3 niveles completo - desde inicial a profesional': 702,\n",
       " 'el camino ágil a la mejora de procesos': 703,\n",
       " 'biomedical engineering career overview & industrial trends': 704,\n",
       " 'salsa basics und 2 figuren': 705,\n",
       " 'robotica e ros - learn by doing! manipolatori': 706,\n",
       " 'course 6: leadership with urgency': 707,\n",
       " 'werden sie zur führungspersönlichkeit der zukunft': 708,\n",
       " 'masaje tailandés': 709,\n",
       " 'excel fundamentals for healthcare': 710,\n",
       " 'salsa showfigur 2': 711,\n",
       " 'nas storage: rockstor server': 712,\n",
       " 'curso gratuito troca de telas de celulares': 713,\n",
       " 'optmyzr 101: monitoring accounts, managing keywords and ads': 714,\n",
       " 'excel completo - desde principiante a avanzado': 715,\n",
       " '【m&a交渉ゲームインストラクター養成】バリュエーション用excelツールで体得するm&a価格交渉疑似訓練\\u3000基礎理論編': 716,\n",
       " 'masaje en silla': 717,\n",
       " 'creating an mp3 player using unity and c#': 718,\n",
       " 'criar uma visita virtual': 719,\n",
       " 'sitio web con html y bootstrap desde cero': 720,\n",
       " 'aprende gv mapper desde cero': 721,\n",
       " 'hepatitis c pharmacology': 722,\n",
       " 'ms powerapps 中級編【sharepointで、実務で使える業務アプリの作り方：勤怠管理アプリ編】': 723,\n",
       " 'make an e-commerce website with blazor webassembly in .net 6': 724,\n",
       " 'elementi dello stato. forme di stato e di governo': 725,\n",
       " 'happiness and life lessons (from your 101 year old self!)': 726,\n",
       " '【エクセル不要！電卓による手計算で本質を理解】ファイナンス＆dcf法による企業価値評価基礎マスター講座': 727,\n",
       " 'software business analysis': 728,\n",
       " 'power system protection: protective relay logic': 729,\n",
       " 'trend micro apex one professional certified q&a': 730,\n",
       " 'indian cooking variety of lentils': 731,\n",
       " 'curso teórico práctico de criptomonedas nivel 1. cznd btc': 732,\n",
       " 'the complete forex trading course zero to hero ++': 733,\n",
       " 'курс компьютерной грамотности для детей в возрасте 7+': 734,\n",
       " 'computer network infrastructure basics': 735,\n",
       " 'webanwedungen mit spring, thymeleaf und hibernate erstellen': 736,\n",
       " 'تطبيقات تقنية النانو  \" applications of nanotechnology \"': 737,\n",
       " \"learn public speaking: 'from the scratch'\": 738,\n",
       " 'reto de 21 días para elevar el amor propio': 739,\n",
       " 'excel come a lavoro - solito videocorso? no, grazie.': 740,\n",
       " 'create a notepad application  with c#  and visual studio': 741,\n",
       " 'fat fit x kilograms lost': 742,\n",
       " 'psl-1100: python fast-path': 743,\n",
       " 'financial accounting(english example-arabic explanation)': 744,\n",
       " 'the complete wordpress website full course - dashboard': 745,\n",
       " 'the complete sql bootcamp for data analysis – level 1': 746,\n",
       " 'how to use a sewing machine & make your own pattern for sew.': 747,\n",
       " 'how to make bar soap, turmeric soap & jelly soap.': 748,\n",
       " 'python programming fundamentals for beginners': 749,\n",
       " 'learn and practice the skills needed to write a term paper': 750,\n",
       " 'snowpro core certification preparation': 751,\n",
       " 'premiere pro video editing for beginners in tamil (தமிழ்)': 752,\n",
       " 'french language for beginner level': 753,\n",
       " 'тейпирование лица': 754,\n",
       " 'private lending course for investors & mortgage brokers': 755,\n",
       " 'energy mechanics course': 756,\n",
       " 'comunicare con stile': 757,\n",
       " 'strength of materials- part-ii': 758,\n",
       " 'nr 23 - proteção contra incêndio (básico)': 759,\n",
       " 'scopri il sogno della tua vita attraverso il viaggio': 760,\n",
       " 'a beginners guide to online book launch': 761,\n",
       " 'the complete sql bootcamp for data analysis – level 2': 762,\n",
       " '15 street smart tactics for negotiation': 763,\n",
       " 'yoga-quickies für jeden tag: das 4-wochen-rundum-programm': 764,\n",
       " 'tyt/ayt/kpss/dgs matematik | 600+ kritik soru i̇le problemler': 765,\n",
       " 'c_cpi_14  sap cloud platform integration practice exam': 766,\n",
       " 'introduction to innovation management': 767,\n",
       " 'in 10 wochen besser sehen - teil 2': 768,\n",
       " 'flutter chat application with firebase': 769,\n",
       " 'website speed optimization online training course': 770,\n",
       " 'اختبار اختراقات sql injection': 771,\n",
       " 'tarot card reading - become a professional tarot card reader': 772,\n",
       " 'arcgis temel, orta ve harita yapımı eğitimi': 773,\n",
       " 'english language basics for beginners & intermediate levels': 774,\n",
       " 'speaker recognition | by award winning textbook author': 775,\n",
       " '.net microservices: cqrs & event sourcing with kafka': 776,\n",
       " 'in home recording steps to pro sound': 777,\n",
       " 'motivation: unlock powerful self motivation': 778,\n",
       " 'english grammar detailed course  (urdu & hindi)': 779,\n",
       " \"how to protect your child's developing brain\": 780,\n",
       " 'برنامج التأهيل الوظيفي': 781,\n",
       " 'istqb agile technical tester - advanced level': 782,\n",
       " 'the captain’s experience: leadership & team development': 783,\n",
       " 'android rom - ui aosp - phone launcher - kiosk app': 784,\n",
       " 'the complete quicksight course 2022: from zero to expert!': 785,\n",
       " 'insurance, growth hacking, risk mitigation: 3 in 1': 786,\n",
       " 'laravel : pemula sampai mahir': 787,\n",
       " 'fundamentals of analog electronic devices  & circuit design': 788,\n",
       " 'learn to play bridge': 789,\n",
       " 'introdução à mecânica quântica': 790,\n",
       " \"apprendre l'anglais professionnel\": 791,\n",
       " 'sap s/4hana manufacturing (production planning)': 792,\n",
       " 'fitness & nutrition: for kids and teens: certificate course': 793,\n",
       " 'aprende lenguaje c de cero a experto': 794,\n",
       " \"a beginner's guide to becoming a travel director\": 795,\n",
       " 'reiki 1 course for children.  mommy and me reiki 1 course.': 796,\n",
       " 'mpls fundamentals and networking using cisco ios': 797,\n",
       " 'اصنع ثروتك بعقلك': 798,\n",
       " 'buy a business with nothing down - entrepreneur mindset': 799,\n",
       " 'understanding colors: choosing the right colors for your art': 800,\n",
       " 'basic to advance knowledge of seo': 801,\n",
       " '3dsmax para producción audiovisual (vol. 2)': 802,\n",
       " 'я youtuber - как сделать успешный youtube канал': 803,\n",
       " 'create your first e-commerce store without needing paid ads': 804,\n",
       " 'how to configure a high availability system in postgresql': 805,\n",
       " 'aprenda excel básico - iniciando com a ferramenta': 806,\n",
       " 'başlangıçtan orta seviyeye - backend kursu c#': 807,\n",
       " 'story-building: a creative writing course for kids': 808,\n",
       " 'come aprire un sito web gratuito completamente online': 809,\n",
       " 'mini event management course for beginners': 810,\n",
       " 'construindo a sua loja virtual com a plataforma irroba': 811,\n",
       " 'grant writing': 812,\n",
       " 'executive legal liability': 813,\n",
       " 'learn how to make desktop documentary with stock footage': 814,\n",
       " 'learn deep reinforcement learning fast': 815,\n",
       " 'maitrisez le e.commerce, webdesign & business +études de cas': 816,\n",
       " 'gene editing crispr in detail and other dna repairtechniques': 817,\n",
       " 'uipath rpa - complete fundamentals bootcamp with 8 exercises': 818,\n",
       " 'la compression audio : le cours complet': 819,\n",
       " 'эффективный руководитель: управление приоритетами и командой': 820,\n",
       " 'haciendo gráficas impactantes en rstudio con tidyverse': 821,\n",
       " '幼稚園の先生が２５年間で得た知識をお届けする「子育てから学ぶ自分育て」': 822,\n",
       " 'meditation for personal resilience and peak performance': 823,\n",
       " 'curso de pug desde cero: el más completo en español': 824,\n",
       " 'protéger sa vie privée sur internet comme un·e pro': 825,\n",
       " 'youth2unite': 826,\n",
       " 'aprende programación con ejercicios en lenguaje c': 827,\n",
       " 'انٹرنیٹ کے بغیر مقامی ہوسٹنگ': 828,\n",
       " 'debugging tips for sap abapers and functional consultants': 829,\n",
       " 'стопа. сброс до заводских настроек': 830,\n",
       " 'pregnancy mindfulness yoga course': 831,\n",
       " 'فیکٹری لگانے کے لئے پانچ ستون | رضوان فیکٹری والا': 832,\n",
       " 'hackea tu año': 833,\n",
       " 'creación páginas web profesionales desde cero con wordpress': 834,\n",
       " 'kezdő programozás a becube-tól': 835,\n",
       " 'صابن فیکٹری | رضوان فیکٹری والا': 836,\n",
       " 'ludo game tutorial | how to create ludo game': 837,\n",
       " 'stick up for yourself and build confidence': 838,\n",
       " 'cis servicenow sam -  software asset management.': 839,\n",
       " 'python primer': 840,\n",
       " 'next js & wordpress: build rapid nextjs sites with next & wp': 841,\n",
       " 'igualdad salarial - ley 30709, perú': 842,\n",
       " 'importance of teamwork | umair maqbool': 843,\n",
       " 'aprende matlab de cero a experto': 844,\n",
       " 'el perro senior': 845,\n",
       " 'android app course 1 | ubaid ur rehman': 846,\n",
       " \"improve your student's test prep math\": 847,\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 1': 848,\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 2': 849,\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 3': 850,\n",
       " 'linkedin para venta b2b para emprendedores': 851,\n",
       " 'improvisation lernen am piano in 432 hz': 852,\n",
       " '【ソフトウェア開発業界  専門講座】強いチームを作れるプロジェクトマネージャー養成プログラム\\u3000心動くドラマ法則を皮切りに': 853,\n",
       " 'the all-inclusive stock trading course for beginners': 854,\n",
       " 'learn javascript with koans - beginner friendly programming': 855,\n",
       " 'data management for beginners - main principles': 856,\n",
       " 'heart-centered-kids -early childhood education & empowerment': 857,\n",
       " 'photoshop 2022': 858,\n",
       " \"cryptomonnaies et blockchain: passez à l'action !\": 859,\n",
       " 'how to become a cio (chief information officer)': 860,\n",
       " 'certified: the definitive guide to intuitive aura reading': 861,\n",
       " 'sap debugger completo en español': 862,\n",
       " 'project management foundations': 863,\n",
       " '\"fear of information security\" and \"fear of cyber attacks\"': 864,\n",
       " 'practical database guide with rdbms(mysql) & nosql(mongodb)': 865,\n",
       " 'email computer forensics fundamentals': 866,\n",
       " 'introducción a programación con python: crea tu portafolio': 867,\n",
       " 'the 10 secrets to creative writing success!': 868,\n",
       " 'how to be successful on tiktok': 869,\n",
       " 'dólar smart - opere dólar com um método simples e lucrativo': 870,\n",
       " 'la perspective dans le dessin': 871,\n",
       " 'gestão de processos completo': 872,\n",
       " 'أساسيات التعليم الإلكتروني': 873,\n",
       " 'photography 101': 874,\n",
       " 'introduction  to geomarketing - maps for business': 875,\n",
       " 'from worry-ing to wow-ing': 876,\n",
       " 'peer to peer kredite': 877,\n",
       " 'art & science of photography': 878,\n",
       " 'لنکڈ سے کاروبار بڑھانے کا طریقہ | علی رضا پنجوانی': 879,\n",
       " 'vivendo de consultoria organizacional': 880,\n",
       " 'data visualization hands-on using tableau desktop': 881,\n",
       " 'curso de bases de datos y sql': 882,\n",
       " 'vastu shastra - the art of harmonious living in your house': 883,\n",
       " 'azure devops and continuous delivery with git': 884,\n",
       " 'google for education do zero ao avançado - professor digital': 885,\n",
       " 'ملازمت کے درخواست دہندہ کا اندازہ کیسے لگائیں؟ | عمیر مقبول': 886,\n",
       " 'beginners cryptocurrency wealth building and investmenting.': 887,\n",
       " 'ملٹی ٹاسکنگ کا انتظام کیسے کریں؟ | عمیر مقبول': 888,\n",
       " 'live prevention tips - 60 videos in 60 min - the mini course': 889,\n",
       " 'drupal 9 -  einfach & komplett masterclass': 890,\n",
       " 'developing in dronekit with python': 891,\n",
       " 'wow afrikaans is easy! - vocabulary workshop (course 2)': 892,\n",
       " \"polish in 3''' minutes\": 893,\n",
       " \"wordpress & blocksy : créez votre site d'agence facilement.\": 894,\n",
       " 'bash fu full course for basic recon & more fun hacking': 895,\n",
       " 'english grammar for b2 & c1 learners': 896,\n",
       " 'introducción a la economía': 897,\n",
       " 'data science on sustainable development goals (sdgs)': 898,\n",
       " 'the tao of mysticism': 899,\n",
       " 'barberia profesional nivel 1': 900,\n",
       " 'launch your game career': 901,\n",
       " 'fabrication du savon à froid saf': 902,\n",
       " 'primeiros passos para superar o medo de falar em público': 903,\n",
       " 'crea una tienda o e-commerce profesional de 0 a 100': 904,\n",
       " 'gmail: como organizar a caixa de entrada e ser produtivo(a)': 905,\n",
       " 'öabt i̇ngilizce öğrt. approches, methods and techniques': 906,\n",
       " 'crea y vende láminas personalizadas con photoshop': 907,\n",
       " 'hungarian language - the ultimate crash course': 908,\n",
       " 'crea tu negocio online con wordpress woocommerce desde cero': 909,\n",
       " '文部科学省後援「色彩検定2級」対策講座【2020年度改訂対応版】': 910,\n",
       " 'copywriting: texte schreiben, um wie ein profi zu verkaufen': 911,\n",
       " 'resolving parental resentment in 3 steps': 912,\n",
       " 'administrador de salesforce - certifícate - 60 preguntas': 913,\n",
       " \"become your child's academic mentor\": 914,\n",
       " 'ジェンダー基礎講座 │ sdgs目標5「ジェンダー平等を実現しよう」': 915,\n",
       " 'gift framework masterclass: digital marketing partnerships': 916,\n",
       " 'visionboard für mehr klarheit in deinem leben.': 917,\n",
       " 'introdução a sql e banco relacional com postgresql': 918,\n",
       " 'google cloud architect certification 2022': 919,\n",
       " 'photoshop focado em mídias sociais': 920,\n",
       " 'cambia la tua vita con dio in crescita personale cristiana': 921,\n",
       " 'guitarra': 922,\n",
       " 'speed publishing meisterkurs amazon kdp low & no content': 923,\n",
       " 'project management - how to schedule management plan': 924,\n",
       " 'project management - how to scope management plan': 925,\n",
       " 'project management - how to risk management plan': 926,\n",
       " 'gerenciamento de facilities do básico ao avançado': 927,\n",
       " 'programa  pleroma transformação de vida - trimestre 1- mod 1': 928,\n",
       " 'desenvolvimento de games 2d mobile com unity': 929,\n",
       " 'project steambot': 930,\n",
       " 'certified course on \"how to make a diet plan\"': 931,\n",
       " 'curso de desenho anime realce artes': 932,\n",
       " 'sat & act complete math | high score for top colleges': 933,\n",
       " 'learn noorani qaida online - arabic reading practice': 934,\n",
       " \"apprenez l'art floral avec une fleuriste professionnelle\": 935,\n",
       " 'profibus pa': 936,\n",
       " 'unity game asset creation in blender: textured 3d models': 937,\n",
       " 'iso 27001 für startups und kmu': 938,\n",
       " 'secrets of strong memory retention formula सुपर पावर मेमोरी': 939,\n",
       " 'destrave sua recolocação.': 940,\n",
       " 'how to budget and forecast for your business': 941,\n",
       " '情報セキュリティ戦略マネジメントの国際知識\\u3000【一問一答】': 942,\n",
       " 'aprender a leer en ruso desde 0': 943,\n",
       " 'zero to hero network engineer': 944,\n",
       " 'professionelle präsentationen mit prezi': 945,\n",
       " 'これであなたも文字美人！美文字ペン字マスタークラス（漢字応用編）': 946,\n",
       " 'lean six sigma white belt:  intro to root cause analysis': 947,\n",
       " \"formation d'excel tous niveaux\": 948,\n",
       " '自分軸を取り戻す\\u3000セルフヒーリング\\u3000〜\\u3000地に足ついたスピリチュアル': 949,\n",
       " 'songwriting: write your first (or next) great song now': 950,\n",
       " 'independencia para personas con autismo: 3 estrategias': 951,\n",
       " 'lógica de programação com javascript': 952,\n",
       " 'manifesting mastery - turn your dreams into reality': 953,\n",
       " 'latein lernen für anfänger grundlagen leicht erklärt!': 954,\n",
       " 'chinese ecommerce': 955,\n",
       " 'azure devops jumpstart': 956,\n",
       " 'canva大百科事典【2022年最新版】9時間半の特大ボリューム完全網羅パーフェクトコース': 957,\n",
       " 'backtest quantitative trading strategies from scratch': 958,\n",
       " 'máster de nessus - escaneo de vulnerabilidades de 0 a 100!': 959,\n",
       " 'máster de nmap - escaneo de redes desde 0 hasta avanzado!': 960,\n",
       " 'liderança e gestão de equipes': 961,\n",
       " 'ergonomics for hr': 962,\n",
       " 'excel basico desde cero.': 963,\n",
       " 'praxiskurs optionen für anfänger': 964,\n",
       " 'planning using primavera p6': 965,\n",
       " 'authentic kerala cooking - the south indian cuisine': 966,\n",
       " 'basic to advanced content writing: become a content writer': 967,\n",
       " 'python gui automation (हिंदी )': 968,\n",
       " 'modelado de gondola para trade marketing en blender 3d': 969,\n",
       " 'قواعد الموسيقى العالمية - 1 the music theory 1': 970,\n",
       " 'comunicar, convencer y vender \"siempre\"': 971,\n",
       " 'cómo cultivar una mente hacker [creativa]': 972,\n",
       " 'afacan art  school': 973,\n",
       " 'دورة جلين دومان دليلك لاكتشاف عبقرية طفلك': 974,\n",
       " 'sql server course for beginners with 100+ examples': 975,\n",
       " 'prüfungswissen für ihk-prüfung zum verkäuferin': 976,\n",
       " 'desenvolvimento com apache nifi': 977,\n",
       " 'how to make huge online giveaway competitions & sweepstakes!': 978,\n",
       " '11 तरीके पैसे ऑनलाइन बनाने के लिए': 979,\n",
       " 'learn automotive diagnostics': 980,\n",
       " 'programming with kotlin - masterclass | complete course': 981,\n",
       " 'yin yoga for fun': 982,\n",
       " 'become an email marketing master (for both b2b & b2c)': 983,\n",
       " 'كورس حل كل اسئلة واختبارات اى كيو بالاجابات النموذجية': 984,\n",
       " 'daraz ecommerce selling 2022- become a seller on daraz store': 985,\n",
       " \"création d'un site web moderne de a à z avec wordpress\": 986,\n",
       " 'secrets to a bucket full of happiness': 987,\n",
       " 'ms outlook: vom anfänger zum fortgeschrittenen': 988,\n",
       " 'harbor - trusted cloud native repository for kubernetes': 989,\n",
       " 'curso de vendas para atendente de farmácia - método natural': 990,\n",
       " 'barashada cilaan mariska heer professional ah': 991,\n",
       " 'cfd analysis of nrel phase vi wind turbine': 992,\n",
       " 'mindset ágil - desenvolva o seu': 993,\n",
       " 'the ultimate guide to python programming with python 3.10': 994,\n",
       " 'crispr cas system: applications in gene editing and beyond': 995,\n",
       " '(تدريب-تغذية-ألعاب تنافسية)  اللياقة البدنية للأطفال': 996,\n",
       " 'bilimsel araştırma /  tez planlamaya giriş': 997,\n",
       " 'employability skills for autistic students and graduates': 998,\n",
       " 'little women masterclass: louisa may alcott and goethe': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_inv_mapper = dict(zip(sample['title'].str.lower(), sample['model_index'].astype(int)))\n",
    "movies_inv_mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "def word_tokenize_clean(doc: str, stop_words: list):\n",
    "    '''\n",
    "    Tokenize, clean and lemmatize English text\n",
    "    '''\n",
    "    # Приводим в нижний регистр и удаляем не-буквы\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc.lower())\n",
    "    \n",
    "    # Токенизация\n",
    "    tokens = word_tokenize(doc)\n",
    "    \n",
    "    # Фильтрация: убираем стоп-слова, пунктуацию, лемматизируем\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word.isalpha() and word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TaggedDocument(words=['english', 'public', 'speaking', 'ovation', 'public', 'speaking', 'business', 'communication', 'public', 'speaking', 'crucial', 'skill', 'order', 'make', 'advancement', 'education', 'career', 'personal', 'life', 'ovation', 'public', 'speaking', 'speaking', 'method', 'master', 'class'], tags=['0'])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_corpus = [re.sub(r'[^a-zA-Z\\s]', ' ', x) for x in df_inf['description']] \n",
    "tags_doc = [TaggedDocument(words=word_tokenize_clean(doc, stop_words), tags=[str(i)])\n",
    "            for i, doc in enumerate(tags_corpus)]\n",
    "\n",
    "tags_doc[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Doc2Vec(vector_size = 50,\n",
    "                alpha = .02, \n",
    "                min_alpha = .0003,\n",
    "                min_count = 5,\n",
    "                dm = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tags_doc)\n",
    "model.train(tags_doc,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample['vector'] = [model.dv[str(i)] for i in range(len(sample))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ovation public speaking - speaking methods master class',\n",
       " 'method lesson 4: speaking with modulation',\n",
       " 'method lesson 5: speaking with enthusiasm',\n",
       " 'method lesson 3: speaking with power',\n",
       " 'content lesson 2: subject development',\n",
       " 'content lesson 1:  writing introductions',\n",
       " 'method lesson 6: speaking with gestures',\n",
       " 'step-by-step mind mastery course',\n",
       " 'game banao - unity 3d/2d game development in اردو / हिन्दी',\n",
       " 'learn the art of horsehair hitching',\n",
       " 'become a master wordsmith: learn 250+ advance level words',\n",
       " 'fundamentals of engineering (fe) exam review',\n",
       " 'next level js - modernes javascript mit es6 und neuer',\n",
       " 'redirecting negative behavior',\n",
       " 'financial literacy - what i wish i learned in school',\n",
       " 'program manager introduction course',\n",
       " 'business analysis: eliciting nonfunctional requirements',\n",
       " 'business analysis: four core models for scoping requirements',\n",
       " 'stress management in 3d: reduce stress and prevent symptoms.',\n",
       " 'grab life by the obstacles - be your own life coach',\n",
       " 'digital signal processing with matlab :การประมวลผลสัญญาณ dsp',\n",
       " 'become master of programming and it concepts & fundamentals',\n",
       " 'curso de comedia: guión e interpretación de monólogos',\n",
       " 'essay writing for students: boost your essay grades to a+',\n",
       " 'public speaking a practical approach',\n",
       " 'discreet mathematics',\n",
       " 'learn how to play now | ukulele beginners course',\n",
       " 'create your own cryptocurrency exchange, token and airdrop',\n",
       " 'erstelle deine eigenen linux lamp web server inkl. wordpress',\n",
       " 'the fruite program',\n",
       " 'must learn programming for testers - python-ruby-shell',\n",
       " 'curso básico de psicogeriatría',\n",
       " 'pmp exam preparation course',\n",
       " 'unix with scripting for newbies',\n",
       " 'bioestatística no epi info',\n",
       " 'investing in direct participation oil and gas programs',\n",
       " 'el lenguaje corporal',\n",
       " 'the best truth about low self esteem',\n",
       " 'inglês básico -  (aulas em inglês e português)',\n",
       " 'matlab course',\n",
       " 'intentional diversity (updated)',\n",
       " 'pl-900 power platform fundamentals',\n",
       " 'smartphone videomaking professionale',\n",
       " 'easy videomaking',\n",
       " 'database testing and sql for testers (top sql interview qs)',\n",
       " 'ingeniería del software: fundamentos de uml usando papyrus',\n",
       " 'top tips for selling at art & craft shows',\n",
       " 'learn white board 3d animations with videoscribe.',\n",
       " 'how to create engaging social media marketing content',\n",
       " 'curso de modelización financiera avanzada',\n",
       " '【おためし版】javaプログラミング入門',\n",
       " 'curso de desenho e caricaturas',\n",
       " 'graphic design mastery with coreldraw (practical projects)',\n",
       " 'prezi in italiano - corso basic',\n",
       " 'cracking the javascript coding interview: practice problems',\n",
       " 'fundamental concepts of algebra',\n",
       " 'kanban do zero',\n",
       " 'snoredown sleep course',\n",
       " 'aprenda jardinagem  caseira',\n",
       " 'fingerstyle guitar songbook - fingerstyle for beginners',\n",
       " 'java beginners course, learn by coding the hangman game',\n",
       " 'ebay profits academy by trieu doan',\n",
       " 'cardio & abs 7 day challenge',\n",
       " 'yeni başlayanlar için 2 dakika temel i̇spanyolca',\n",
       " 'ielts writing band 7+ with functional grammar',\n",
       " 'speed math | cálculo mental rápido y ágil',\n",
       " 'sacral chakra heaiing : an initiation into fun and pleasure',\n",
       " 'air insulated substation design part 2',\n",
       " 'owning the road to pageant royalty',\n",
       " 'exploration of hatha yoga postures part 1',\n",
       " 'impara dalla pratica python 3.10',\n",
       " 'the wiccan rede explained',\n",
       " 'photoshop cc for photographers',\n",
       " 'advanced computer forensics',\n",
       " 'fiber optics testing and measurements',\n",
       " 'curso prático forex',\n",
       " 'implementación de una nube privada con nextcloud',\n",
       " 'event mastery - how to produce your own successful events',\n",
       " 'learn everything in microsoft word (2016/2019/2021/365)',\n",
       " 'uygulamalı stres ve başa çıkma yolları eğitimi',\n",
       " 'the complete autocad for beginners to intermediate',\n",
       " '5 piekielnych technik',\n",
       " 'introdução à escrita científica',\n",
       " 'almacenamiento en la nube: desde cero para principiantes',\n",
       " 'launch a podcast on a budget',\n",
       " 'apply excel vba in industry series - part i',\n",
       " 'hafıza teknikleri ile x10 hızlı & kalıcı öğrenin',\n",
       " 'cold email marketing mastery | sales emails by coursenvy ®',\n",
       " 'hafıza teknikleriyle 2. sinif i̇ngilizce (konu-sözlük-test) 1',\n",
       " 'java programming language',\n",
       " 'curso completo de desenho realista - 3 cursos em 1',\n",
       " 'como desenvolver a cultura digital',\n",
       " 'ultimate facebook ads & marketing masterclass',\n",
       " 'unreal engine - blueprint scripting 101',\n",
       " 'mastering exponential and logarithmic functions',\n",
       " 'programação pic18 com mplabx e compilador xc8',\n",
       " 'seo blogging star networking',\n",
       " 'basic portuguese a1',\n",
       " 'convert any hardcopy system to electronic system using excel',\n",
       " 'büro-yoga- fit am schreibtisch',\n",
       " 'ripple development and training suit - part 1',\n",
       " 'cómo detectar, prevenir y eliminar virus informáticos',\n",
       " 'being  a dementia caregiver',\n",
       " 'şan dersi - ses eğitimi',\n",
       " 'advanced planning & scheduling with primavera p6 (11.5 pdus)',\n",
       " \"intensive ho'oponopono workshop + 3,800 students enrolled!\",\n",
       " 'the essentials of human interactions:',\n",
       " 'building performance analysis course with leed ce hours',\n",
       " 'photoshop : curso de diseño digital de 0 a 100',\n",
       " 'aws solutions architect associate saa-c03 practice test 2022',\n",
       " 'تعلم التصميم باستخدام برنامج الفوتوشوب',\n",
       " 'registre você mesmo sua marca no inpi.',\n",
       " 'instalación de red hat enterprise linux y derivados',\n",
       " 's.o.s matemática',\n",
       " 'effective presentations',\n",
       " 'seguridad con selinux',\n",
       " 'learn the language of quran and sunnah (part one)',\n",
       " 'learn youtube keyword research and seo to grow your channel',\n",
       " 'the ultimate guide to copper electroforming',\n",
       " 'molecular engineering - intro to biomedical engineering',\n",
       " '부자수업 패키지',\n",
       " 'como desenvolver um algoritmo com clareza',\n",
       " 'how to write chinese characters - beginner level - a1 - hsk1',\n",
       " 'la dirección de arte',\n",
       " 'nórdico antiguo | curso online en español del idioma vikingo',\n",
       " 'leadlovers: configurações, email, páginas, ead e integrações',\n",
       " \"le cv, la lettre de motivation et l'entretien efficace\",\n",
       " 'spiritual disciplines of the christian life',\n",
       " 'introducción a la psicología deportiva aplicada a esports',\n",
       " 'consultor óptico profissional',\n",
       " 'customer service 101: all the basics you need to know (2022)',\n",
       " 'the complete introduction to the world of wine',\n",
       " 'songwriting with bandlab - a beginner’s guide',\n",
       " 'adobe premiere pro cc ile müzik klip çekimi ve editi',\n",
       " 'passive income with crypto currency investing',\n",
       " \"unreal engine 5 : création d'un jeu de zombie\",\n",
       " 'nightwatch.js web application test automation',\n",
       " 'vi̇op - vadeli i̇şlem piyasası hakkında her şey',\n",
       " 'stockage en réseau san pour les débutants',\n",
       " 'finanzas personales. hacia la libertad financiera',\n",
       " 'presentation skills: smart presentation method',\n",
       " 'supply chain segmentation analytics with python',\n",
       " 'dual controller area network module and can data logger',\n",
       " 'manual testing crash course for software testers',\n",
       " 'easy asp .net web form for beginners html, css, sql ado .net',\n",
       " 'astrology; reading the birthchart',\n",
       " 'introducción a c++ moderno para programadores',\n",
       " 'introduction to post producing for commercials and music vid',\n",
       " \"dr. gatsby's home study course in experiential reframing\",\n",
       " 'learn how to speak serbian part 2',\n",
       " 'temple run clone for ios & android | top unity and c# course',\n",
       " 'antigen antibody reactions',\n",
       " 'remuneraciones salariales, tipos y formas de calculo',\n",
       " 'expert advisor sem programação para forex e b3',\n",
       " 'leadership mastery - complete guide to being a great leader',\n",
       " 'ged math test: the complete guide',\n",
       " 'transcréation : acquérir les bases de la traduction créative',\n",
       " 'autocad course',\n",
       " 'sap performance analysis for abap, bw, functional consultant',\n",
       " 'curso de elaboración de champús sólidos',\n",
       " 'quantum physics part-1',\n",
       " 'como elaborar una demanda y como presentarla desde cero',\n",
       " \"programming fundamentals - beginner's guide\",\n",
       " 'matemática para iniciantes',\n",
       " 'tekken 7 course: from beginner to advanced player',\n",
       " 'külföldi munkavállalás és letelepedés érthetően',\n",
       " 'trigonometry 1 with the math sorcerer',\n",
       " 'problems on competitive coding',\n",
       " 'what to do before, while, and after you hit record (youtube)',\n",
       " 'curso full de autocad 2d en español',\n",
       " 'fundamentos para la escritura de guiones para radio',\n",
       " 'prevenção e reversão do declínio cognitivo',\n",
       " 'イメージマスター社労士講座（お試し版）右脳を使って効率学習！',\n",
       " 'curso artcam/ carveco em uma semana',\n",
       " 'sócrates y los sofistas:',\n",
       " 'maintenir et réparer  un système informatique  de a à z',\n",
       " 'sıfırdan yapay zeka mühendisi olma kursu',\n",
       " 'super curso de html e css + projetos práticos',\n",
       " 'redação do enem: passo a passo para um bom texto',\n",
       " 'dog agility for newcomers',\n",
       " 'baralho cigano, prático e objetivo - curso completo',\n",
       " '从零开始服装打板',\n",
       " 'discover your career path & land a job you love in 12 weeks',\n",
       " 'aprenda inglês com filmes 1 - pronúncia e conversação',\n",
       " 'egregores y cómo liberarte de ellos',\n",
       " '【swiftui coredata】  icloudと連携させてデータ共有できるメモアプリを作ろう',\n",
       " 'şarampolden zirveye matematik.',\n",
       " 'python data course: python for data analysis & visualization',\n",
       " 'conceitos devops',\n",
       " 'eletrônica prática: aprenda multisim com circuitos elétricos',\n",
       " 'contabilidad para el régimen especial',\n",
       " 'hr 4.0 - human resources management in industry 4.0',\n",
       " 'food 4.0 - the food & beverage industry in industry 4.0',\n",
       " 'adobe illustrator cc ¡desde cero!: curso para principiantes',\n",
       " 'calcul des assemblages des structures métalliques',\n",
       " 'adoption challenges and drivers of implementing industry 4.0',\n",
       " 'water 4.0 - water management in industry 4.0',\n",
       " 'get clients and make money as a holistic practitioner',\n",
       " 'learn autocad 2d drafting',\n",
       " 'manage project risks impeccably (12 pdus, pmi pmp renewal)',\n",
       " 'corporate actions',\n",
       " 'einkaufscontrolling - kennzahlen zur optimierung des einkauf',\n",
       " 'como usar bem a vírgula e outros sinais.',\n",
       " \"learn from lloyd: the do's & don'ts of numerology\",\n",
       " 'cfps exam strategies',\n",
       " '【scratch(スクラッチ）６】ゲーム入門（level1）。ゲームづくりの手順と手法がわかるゲームづくり入門コース',\n",
       " 'work 4.0 - impact of industry 4.0 on workforce & workplace',\n",
       " 'calcul paratique des ponts avec le logiciel st1',\n",
       " 'how to build a sales funnel for your subscription business',\n",
       " 'crea increíbles sitios web, en: php, mysql, ajax, + poo mvc!',\n",
       " \"what's your story?\",\n",
       " 'project cost management using google sheet with daily update',\n",
       " \"programmation en python 3: du débutant à l'expert\",\n",
       " 'hydraulics 102 - hydraulic components in depth',\n",
       " 'technical sales presentations - upstream oil and gas',\n",
       " '作曲・音楽制作のために知っておくべき初級音楽理論',\n",
       " 'complete it support specialist course: networking',\n",
       " 'feminine and masculine energy:the secret between energies',\n",
       " 'mern stack 2022 - build ultimate cms (wordpress clone)',\n",
       " 'become a aha certified cpr instructor for extra side income',\n",
       " 'mastering mediumship',\n",
       " '数千人の患者さんを診て完成した  整体・ストレッチ・マッサージ・カイロプラクティック・テーピング・治療講座',\n",
       " '実際に日本一のお店、全国組織、資格を作った起業講座・ビジネスも人間関係もポジショニングが９割',\n",
       " 'aptitude training for all competitive entrance exams',\n",
       " 'cryptography: a hands-on approach',\n",
       " 'guitarra moderna 2.0',\n",
       " 'bootstrap４でかんたんキレイなウェブサイトを作ろう',\n",
       " 'português para concursos - 1.2 sintaxe',\n",
       " 'comunicação eficaz 2.0',\n",
       " 'intro to database app dev w/spring boot, angular, postgres',\n",
       " '数列を理解するための数学講座[なんとなくや公式丸暗記とはサヨナラ]',\n",
       " 'supergerente - torne-se um gerente de sucesso',\n",
       " 'comienza con excel: curso destacado para principiantes',\n",
       " '7 practical steps to reach your goals.',\n",
       " 'spiritual weight loss',\n",
       " 'employee manual handling for coffee shops,',\n",
       " 'digital marketing data: dmps, device graphs, and regulation',\n",
       " 'persönlichkeitsentwicklung 2.0',\n",
       " 'aprenda a lançar produtos, serviços e infoprodutos digitais',\n",
       " 'european portuguese: basic tenses',\n",
       " 'sql server para analistas de negocios',\n",
       " 'learn ethical hacking on any computer, mobile, account',\n",
       " 'profi instagram marketing 2022: igtv, instagram reels & live',\n",
       " 'basics of mechanical design engineering (2022)',\n",
       " 'user experience design用戶體驗設計思考與實作',\n",
       " 'intermediate fusion 360',\n",
       " 'assertiveness masterclass - how to be assertive & likeable',\n",
       " 'الكورس العربي الاول في كتابة البحوث العلمية',\n",
       " 'dress for success with 5 elements theory',\n",
       " 'the pursuit of wonder: tools for philosophical literacy',\n",
       " 'corso microsoft excel per principianti, facile e veloce.',\n",
       " 'neurofitness - bem estar mental',\n",
       " 'know your worth: set boundaries with self-confidence',\n",
       " 'uipath - the complete rpa training course (2022)',\n",
       " 'product photographer: ecommerce product photography course',\n",
       " 'hashcat for penetration testers',\n",
       " 'inglês básico - oscar english',\n",
       " 'smart negotiating masterclass 2.0',\n",
       " 'formation en naturopathie certifiante',\n",
       " 'initiation au clair-ressenti, énergie & conscience',\n",
       " 'the path of christian mysticism and meditation',\n",
       " \"biotechnology: a beginner's guide to gene editing\",\n",
       " 'how to mark your presence in group discussion',\n",
       " 'comic art master class: the crimson cat | inking & coloring',\n",
       " 'declaração de imposto de renda pessoa física 2022',\n",
       " 'aplique duplique',\n",
       " 'ahmet aday - klasik gitar dersleri',\n",
       " 'kyc know your customer bootcamp (5 hours, new for 2022)',\n",
       " 'marxism: english literature',\n",
       " 'humanities: academic writing and publishing',\n",
       " 'como criar seu release com poucos passos',\n",
       " 'digital electronics',\n",
       " 'spark it: crowdfunding possibilities into reality',\n",
       " 'la alimentación es terapia milagrosa',\n",
       " 'estilo de vida saludable',\n",
       " 'pte academic: strategies & test hacks: advanced',\n",
       " 'complete shopify dropshipping millionaire course 2.0',\n",
       " 'curso de vim desde principiantes a avanzado',\n",
       " 'c語言引路人 ： 從零開始，向下紮根',\n",
       " 'micro-autologous fat transplantation introduction - japanese',\n",
       " 'selenium webdriver con python + pytest desde cero',\n",
       " 'c# meisterkurs: lerne c# programmierung von a-z in 4 wochen',\n",
       " 'modern baker. delicious and healthy bread.',\n",
       " 'affinity photo-tutorial – grundlagen',\n",
       " 'ux interviews masterclass',\n",
       " 'sıfırdan i̇rəli səviyyəyə python proqramlaşdırma dili',\n",
       " 'android automotive - custom rom - aosp - car launcher- kiosk',\n",
       " 'best gre math 4-week course | target gre 165+',\n",
       " 'arayavart complete tutorial of shopify app development',\n",
       " 'sql for beginners: write queries and create database',\n",
       " 'sales pro mate guide or how to sell any idea to anyone.',\n",
       " \"lire et ecrire l'arabe en seulement 15 leçons\",\n",
       " 'aprende a aplicar la neurociencia en tus clases.',\n",
       " 'resilient work culture to prevent sexual harassment',\n",
       " 'java ee : devenez développeur des applications entreprises',\n",
       " 'master any song using hindustani classical tools',\n",
       " 'scratch w nauce programowania - przykłady gier i skryptów.',\n",
       " 'احترف تصميم واجهات المواقع والتطبيقات ui/ux with adobe xd',\n",
       " 'design and manifest better life',\n",
       " 'psychology in a nutshell',\n",
       " '轮滑高级教程',\n",
       " 'small engine diagnostics',\n",
       " 'design of wastewater treatment plants for onsite projects',\n",
       " 'certified cost professional(ccp)- exam preparation',\n",
       " 'bpm, bpmn, ferramentas e modelagem de processos com bizagi',\n",
       " 'behavior mastery: 5 tools 2 transform all behaviors inc adhd',\n",
       " 'tsuboki japanese foot massage',\n",
       " '【scratch（スクラッチ）5】数式を使って、通学路を登校させる、持久走対決をする、空き地に作図する',\n",
       " 'realidad virtual 360 aplicada al diseño e interiorismo',\n",
       " 'best chess course for complete beginner',\n",
       " 'maîtriser node.js et son écosystème (npm, express, mongo, …)',\n",
       " 'social network analysis',\n",
       " 'how to sell when your clients don’t look like you',\n",
       " 'learn practical palm reading : how to read palm lines',\n",
       " 'the cloud course',\n",
       " 'impressão 3d profissional - do polímero ao produto',\n",
       " '【adobe  premiere pro】premiere pro徹底解説～基本から実践的なテクニックまで',\n",
       " 'curso básico de redes',\n",
       " 'solidworks simulation parça ve montaj analiz eğitimi',\n",
       " 'creating the afro dance body beginner 2',\n",
       " 'meditation - a way of being',\n",
       " 'art of photography with hands-on trainings in urdu/hindi',\n",
       " 'دليل المبتدئين فى الربح من الإنترنت',\n",
       " 'pentesting y hacking con windows',\n",
       " 'intelligence analysis (comprehensive - levels 1, 2 and 3)',\n",
       " 'learning cfd with ansys fluent (workbench)',\n",
       " 'situation awareness 360',\n",
       " 'الكورس التعليمى لتجاره الخيارات الثنائيه',\n",
       " \"formation systeme io l'outil clé pour vos formations\",\n",
       " 'how to write a resume for australian market?',\n",
       " 'beginning jazz improvisation',\n",
       " 'sketchnotes - einführung in die welt der visuellen notizen',\n",
       " 'introducción a la milenaria sabiduría de la kabalá hebrea',\n",
       " 'инфаструктура открытых ключей (pki). часть № 3.',\n",
       " 'how to make an amazon affiliate store with wix',\n",
       " 'actualización en ortodoncia y ortopedia maxilar',\n",
       " 'unconscious bias - die basics',\n",
       " 'sql injection for beginners',\n",
       " 'weight loss ~ keep fit & healthy, follow natural lifestyle',\n",
       " 'life transformation course',\n",
       " 'medición de cantidades de obra y presupuestos',\n",
       " 'машинное обучение в python: machine learning & data science',\n",
       " \"paragraph writing simplified - level one - beginner's guide!\",\n",
       " 'intro to watercolor florals',\n",
       " \"pro tools - the beginner's guide\",\n",
       " '10 passos para se comunicar melhor',\n",
       " 'spring security',\n",
       " 'hafıza teknikleriyle 3. sinif i̇ngilizce (konu-sözlük-test) 1',\n",
       " 'acentuación de las palabras',\n",
       " 'complete wordpress web development mastery course.',\n",
       " 'curso básico de canva 2022: crea diseños gráficos desde cero',\n",
       " 'instagram marketing for local business: the complete guide',\n",
       " 'key stage 2 maths',\n",
       " 'devops - mão na massa!',\n",
       " 'let nature to help you heal',\n",
       " 'mega course - vmware vsphere 7.0 boot camp - part 1 w. ebook',\n",
       " 'curso de marketing digital completo 2022',\n",
       " 'get ready for kindergarten, learn english phonics (part 1)',\n",
       " 'programación en lenguaje c orientado a microcontroladores',\n",
       " 'power system protection: transformer protection',\n",
       " 'esp32 and internet of things for absolute beginners',\n",
       " 'applied sport psychology - psychological skills training',\n",
       " 'azure app service - aplicações web app e containers docker',\n",
       " 'learn to teach online like a pro',\n",
       " 'test your fundamentals : digital logic design',\n",
       " 'aktywny senior - trening dedykowany dla osób starszych',\n",
       " 'spring framework tutorial in hindi | spring core',\n",
       " 'estratégia quant: equilíbrio das cores em gráficos de preços',\n",
       " 'python 1500: practice missions',\n",
       " 'introduction into supply chain analytics',\n",
       " 'diksiyon eğitimi',\n",
       " 'wedding mc masterclass from beginners to professional part 2',\n",
       " 'la via della libertà - benessere psicofisico applicato',\n",
       " 'bbc microbit : learner to programmer 2022',\n",
       " 'stress management - transform your relationship with stress',\n",
       " 'how to grow your tiktok account - ultimate guide',\n",
       " 'how to make god known',\n",
       " 'the patient experience matters',\n",
       " 'a level genetics - any level student welcome',\n",
       " 'check point troubleshooting course [for ccse & ccta] -2022',\n",
       " 'iso 50001:2018 energy management systems',\n",
       " 'python machine learning || build real world projects',\n",
       " 'agile scrum repair guide: how to reboot your scrum team',\n",
       " 'aprenda sobre matriz gut (gravidade - urgência - tendência)',\n",
       " 'angular in arabic the complete guide (2022)',\n",
       " 'master de modelado de props para series animadas',\n",
       " 'treasures of the hive...from bees to recipes!',\n",
       " 'ultimate beginners guide to power bi - part 1',\n",
       " 'hedef belirleme ve sonuç alma',\n",
       " 'master the new 1003 as a mortgage loan processor, mlo, & uw',\n",
       " 'introducere în bioetică și deontologia medicală',\n",
       " 'curso de tarot dos anjos',\n",
       " 'cisco duo zero trust - mfa vpn authentication',\n",
       " 'alimentación y ejercicio físico: microbiota fortalecida.',\n",
       " 'ms project 2019',\n",
       " 'microsoft excel: formulas for beginners',\n",
       " 'amci introduction to medical coding (i2mc) course',\n",
       " 'curso intensivo de autocad desde cero-actualizado 2021-2022',\n",
       " 'small business financials',\n",
       " 'select mutual funds without advisor in 30 mins (blueprint).',\n",
       " 'an introduction to google earth engine (gee)',\n",
       " 'creación de cursos online rentables y escalables',\n",
       " 'remagic your self',\n",
       " 'modern computer vision™ pytorch, tensorflow2 keras & opencv4',\n",
       " 'algebra: prodotti notevoli',\n",
       " 'scala at light speed',\n",
       " 'narrativas de videojuegos soulsborne',\n",
       " 'covid tracker application in java spring boot for beginners',\n",
       " 'adım adım hac rehberi',\n",
       " '11 days of self-discovery',\n",
       " 'manufacturing execution system (mes) - fundamentals',\n",
       " 'understanding html and css',\n",
       " \"your resilient brain and you, a user's guide\",\n",
       " 'how to get the lowest interest rate on a mortgage',\n",
       " 'technical writing demystified',\n",
       " 'a guide to types of buildings and activities performed in it',\n",
       " 'mapas mentais - a estratégia dos gênios | passo a passo',\n",
       " 'the power of body language and public speaking',\n",
       " 'fiverr freelancing for 3d product designers and engineers',\n",
       " 'coping with stress',\n",
       " 'sql server programming and stored procedures fundamentals',\n",
       " 'tabla of advance level part 1 -- teentaal rau',\n",
       " 'oficina de feitio de rapé',\n",
       " 'sibelius pour débutants',\n",
       " 'sap fiori para desenvolvedores',\n",
       " '【2022最新版】adobe premiere pro 超入門！初心者でもサクサクわかるプレミアプロ入門コース',\n",
       " 'master the basics of pyqt',\n",
       " 'full stack series - [free segment of  part1]',\n",
       " 'curso basico de cctv',\n",
       " 'harmonogramowanie produkcji - cz. 1',\n",
       " 'safety pin - how to improve your self-esteem',\n",
       " 'freecad for mechanical engineers : technical 2d drawing',\n",
       " 'narzędzia canvas',\n",
       " 'phytothérapie moderne de la théorie à la pratique',\n",
       " 'как создать подкаст с нуля',\n",
       " 'master critical financial calculations in 1 month',\n",
       " 'karma astrolojiye giriş ve dört önemli sabit yıldız',\n",
       " 'convolution neural network with data augmentation',\n",
       " 'curso de elaboración de planos con solidworks | jc',\n",
       " 'corso scrittura professionale',\n",
       " 'c++ programming essentials for beginners',\n",
       " 'programming for non-programmers',\n",
       " 'structural design of foundations for structures',\n",
       " 'book marketing: amazon launch success & superior socialmedia',\n",
       " 'capm: certified associate in project management (pmi-100)',\n",
       " 'introduction to mechanical ventilator modes',\n",
       " 'ccsk practice exam and exam reference links',\n",
       " 'microneedling scars wrinkles hair growth & deepchemical peel',\n",
       " 'python for beginners',\n",
       " 'dropshiping : formation complète de a à z pour réussir',\n",
       " 'pintura de mestre',\n",
       " 'another view of debian linux and security hardening + ebook',\n",
       " 'email marketing tools ultimate guide أدوات التسويق بالإيميل',\n",
       " 'advance self hypnosis & hypnotherapy training',\n",
       " 'tinkercad ve arduino uygulamalı başlangıç seviyesi eğitim',\n",
       " 'how to become a web designer from scratch - phase 1 of 3',\n",
       " 'play winning chess',\n",
       " 'fundamentos de bioquímica,química analítica laboratorio clí.',\n",
       " 'crear una aplicación para escritorio para gestión médica',\n",
       " 'introduction to forex trading for beginners',\n",
       " '¡master en cae y análisis mecánico!',\n",
       " 'vital communication: learn fundamental interpersonal skills.',\n",
       " 'google ile i̇hracat',\n",
       " 'nomenclatura y formulación de alcanos alquenos y alquinos',\n",
       " 'master en react: aprender reactjs, hooks, mern, nodejs, jwt+',\n",
       " 'learn to how to play the otamatone!',\n",
       " 'learn revit architecture- with full project',\n",
       " 'exempt market proficiency course (emp) part - one',\n",
       " 'python para trade de criptomoedas (bitcoin, ethereum, etc.)',\n",
       " 'curso de modelo e manequim online',\n",
       " 'paint the ocean you want to see',\n",
       " 'rediscover your life purpose, confidence and the real you !',\n",
       " 'introduction to sales channels.',\n",
       " 'how to paint a monochromatic portrait  in oils',\n",
       " 'experto en articulate storyline 3 y storyline 360',\n",
       " 'excel business modeling - the in-depth street smart toolkit.',\n",
       " 'x86 assembly programming',\n",
       " \"how to be a pastor's wife\",\n",
       " 'how to make handmade jewelry',\n",
       " 'curso de piano para tocar himnos',\n",
       " 'aprende inglés desde cero',\n",
       " 'the ultimate meditation pracise',\n",
       " 'professional game background graphics design in photoshop',\n",
       " 'desenho artístico do zero',\n",
       " 'positive mental attitude',\n",
       " 'anti-trafficking 101 + allied issues & laws - trafficking',\n",
       " 'learn business case development',\n",
       " 'máster en dax y power pivot de la a a la z',\n",
       " 'an introduction to safeguarding children',\n",
       " 'become a jewelry designer, design your own piece of jewelry',\n",
       " 'how to become a handbag designer & how to sew a handbag',\n",
       " 'l.a.s.d. annual orientation',\n",
       " 'curso de caracterización teatral',\n",
       " 'elasticsearch 101',\n",
       " '¿cómo mejorar la relación de pareja?: el amor que perdura.',\n",
       " '微信行銷課程-初階班 wechat marketing basic class(english subtitles)',\n",
       " 'powerapps for beginners',\n",
       " 'intro ao ayurveda',\n",
       " 'marketing digital de serviços',\n",
       " 'akbar & birbal stories - அக்பர் & பீர்பால் கதைகள் (தமிழில்)',\n",
       " 'klasik arapça eğitim seti, 100 ders, 1000 kelime',\n",
       " 'comandos elétricos em sistemas de refrigeração',\n",
       " 'tradingview ile riskinizi azaltın, kazancınızı arttırın',\n",
       " 'retenções de tributos do zero a profissional- essencial',\n",
       " 'create financial abundance',\n",
       " 'powerpoint para apple. de cero a experto.',\n",
       " 'leer nederlands in het nederlands 1: voor absolute beginners',\n",
       " 'curs de franceză în franceză 2: cele mai frecvente cuvinte',\n",
       " 'contract administration for engineers إدارة العقود للمهندسين',\n",
       " 'olandeză în olandeză 3: cele mai folosite 1000 de cuvinte',\n",
       " 'introduction to intelligence',\n",
       " 'scoring 5 above target (pmi-rmp)® exam',\n",
       " 'la justice pénale internationale (droit international pénal)',\n",
       " 'filmora x (10.5) - video editing masterclass for beginners',\n",
       " 'homeschool launch course',\n",
       " 'create the most memorable and valuable brand experience',\n",
       " 'automatización avanzada de pruebas de software',\n",
       " 'spring batch com microservices para etl',\n",
       " 'die hypno-mental-diät...',\n",
       " 'sıfırdan i̇leri seviyeye korece kursu 3',\n",
       " 'modal verbs. a practical grammar course',\n",
       " 'google slides for beginners',\n",
       " 'gestão para negócio online com cursos',\n",
       " 'montage vidéo débutant',\n",
       " 'existência e reflexão',\n",
       " '3000 días para conseguir salud para toda la vida',\n",
       " 'métodos de cálculo muestral y muestreo para investigación',\n",
       " 'grade 10 maths: linear systems and matrices (k-12 / uae)',\n",
       " 'cantonese for complete beginners (free taster course)',\n",
       " 'montage vidéo intermédiaire',\n",
       " 'vegas pro 18',\n",
       " 'python malayalam - beginner to expert',\n",
       " '職場で困ったときも怖くない！ 自分で解決する力を身につけるための事例で学ぶit入門講座',\n",
       " 'python for financial markets analysis',\n",
       " 'montage vidéo expert',\n",
       " 'power query para power bi y excel',\n",
       " 'virtual reality and mental healthcare',\n",
       " 'internetworking and subnetting mcq practice (networking)',\n",
       " 'digital electronics - complete course (72+ hours)',\n",
       " 'how to create a website with wordpress and elementor in 2022',\n",
       " 'french language course: learn french/ speak it like natives',\n",
       " '[new]learn javascript for web development by examples',\n",
       " 'video marketing | an introduction',\n",
       " 'tableau financial reporting & financial analysis masterclass',\n",
       " 'correlations, association & hypothesis testing (with python)',\n",
       " 'yoruba speaking made easy- speak yoruba language in 30 days',\n",
       " 'negotiation with chinese 101: your essential business skills',\n",
       " 'business development and b2b sales mastery',\n",
       " 'إنشاء محتوي تعليمي تفاعلي باستخدام ستوري لاين 360',\n",
       " 'အခြေခံ ပရိုဂရမ်ရေးခြင်း (programming fundamentals)',\n",
       " 'violão iniciante para educadores infantis',\n",
       " 'application virtualization మరియు app-v',\n",
       " 'curso de libras!',\n",
       " 'o curso definitivo para alta performance',\n",
       " 'curso práctico para programación de avr -curso 3',\n",
       " 'certificate in professional dowsing - accredited masterclass',\n",
       " 'essential business analysis for consultants',\n",
       " 'latvian mythology',\n",
       " 'chinese allegories : your smart/easy option to learn chinese',\n",
       " 'food safety for : chefs and food handlers',\n",
       " 'vue 3 incl. router, vuex, pinia, composition api & firebase',\n",
       " 'matemática (para leigos): regra de três simples e composta',\n",
       " 'microsoft office excel: niveau introduction au niveau avancé',\n",
       " 'marketing psychology, emotional intelligence, neuromarketing',\n",
       " 'complete study of card payments process in digital banking',\n",
       " 'administração para area de exatas',\n",
       " 'mastering ielts speaking: the model answers edition',\n",
       " 'master full stack .net architecture using c#',\n",
       " 'topografía y explanaciones con autodesk civil 3d',\n",
       " 'fundamentals of the algorithm in programming languages',\n",
       " 'baştan sona akıllı kesit tasarımı eğitimi /(section design)',\n",
       " 'learn baby massage with baby college',\n",
       " \"unreal engine 4 - the absolute beginner's guide\",\n",
       " '3 modern acrylic impressionistic landscape paintings',\n",
       " 'learn game design free',\n",
       " 'complete python bootcamp for everyone from zero to hero 2023',\n",
       " 'python beginner to expert',\n",
       " 'beginning object-oriented programming with c#',\n",
       " 'linux administration 2022 - deep dive',\n",
       " 'a practical introduction to opc ua',\n",
       " 'nice guy syndrome recovery',\n",
       " 'qgis essentials: beginner to practitioner',\n",
       " 'meraki in depth',\n",
       " 'how to define a brand identity',\n",
       " 'treinamento seguidor comprador',\n",
       " 'كورس النجاح بعلامة 140+ في اختبار الدوولينجو خلال 14 يوماً',\n",
       " 'object oriented programming c#',\n",
       " 'biomecánica aplicada al tenis',\n",
       " 'big data and machine learning fundamentals',\n",
       " \"animate'de basit animasyonlar yapmak çok kolay.\",\n",
       " 'knime analytics platform per data scientists, corso base',\n",
       " 'piano para adultos iniciantes',\n",
       " 'english for german speakers',\n",
       " 'complete autocad 2021 course : [both 2d and 3d]-mechanical',\n",
       " 'learn social media on 7+ platforms',\n",
       " 'learn facebook ads & facebook marketing',\n",
       " 'make it stick, write it down: a journaled philosophy.',\n",
       " '5 segredos para transformar seu ensino',\n",
       " 'html, css, javascript and bootstrap for web designers',\n",
       " 'statistics : computer oriented statistical methods',\n",
       " 'amazon seller training in malayalam- ആമസോൺ ട്രെയിനിങ്  2022',\n",
       " 'linguagem r: módulo básico com aplicações florestais',\n",
       " 'low-poly character modeling & animation in blender for unity',\n",
       " 'tips&tricks to make &upload your course on udemy-unofficial-',\n",
       " '入門 amazon eks：在一天內開啟你的 kubernetes on aws 旅程',\n",
       " 'learn instagram ads & marketing',\n",
       " 'learn twitter ads & marketing',\n",
       " 'learn linkedin ads & marketing',\n",
       " 'learn youtube ads & marketing',\n",
       " 'learn pinterest ads & marketing',\n",
       " 'sleep special techniques:',\n",
       " 'learn tumblr ads & marketing',\n",
       " 'substation grounding',\n",
       " 'be aware of data science',\n",
       " 'aaa game 3d character art course module 01 - body',\n",
       " 'alfabetização infantil, método fônico para crianças',\n",
       " 'cardiac anatomy and physiology',\n",
       " 'catia v5r20 fundamentos',\n",
       " 'promob plus cozinha planejada',\n",
       " 'nx 12 fundamentos',\n",
       " 'nx 11 - fundamentos',\n",
       " 'master agile estimation: top techniques for scrum & kanban',\n",
       " 'pentesting fundamentals for beginners',\n",
       " 'fashion 101 | design & styling- textures, shaping, & more!',\n",
       " 'technical analysis course in hindi',\n",
       " 'curso internacional de supervivencia',\n",
       " 'cold emailing and cold calling mastery',\n",
       " 'sql bootcamp: zero to hero using microsoft sql server',\n",
       " 'docker + security + aws basics + jenkins',\n",
       " 'crisp-ml(q) - data pre-processing using python',\n",
       " 'shell scripting',\n",
       " '【初心者向け】初めてのvrew（ブリュー）入門。動画編集にかかる時間を一気に減らし、効率良く字幕テロップも付けよう！',\n",
       " '【macではじめる】初めてのecammlive入門！本格的なライブ配信や動画撮影に使えるプロ動画配信環境を手に入れよう',\n",
       " 'управление проектами: полное погружение в pmbok7-2021',\n",
       " 'tarô; arcanos maiores e menores - curso completo',\n",
       " 'asterisk pbx with database/api driven call center solution',\n",
       " 'mastering the it networking fundamentals: full course',\n",
       " 'criando um controle de vendas desktop com c# e mysql',\n",
       " 'facebook messenger chatbots for business',\n",
       " 'digital communications strategy for non profits',\n",
       " 'gradle plugin masterclass',\n",
       " 'master python programming (arabic)',\n",
       " 'curso básico de typescript - empieza con este lenguaje',\n",
       " 'curso básico de react - empieza con este framework js',\n",
       " 'unleash your avatar - discover & embody your higher self',\n",
       " 'corso base di programmazione per principianti',\n",
       " 'android clean architecture:mvvm, jetpack components + kotlin',\n",
       " 'masterclass on building a business budget',\n",
       " 'pizza italiana 100% - basic course (english version)',\n",
       " 'lash extensions beginners course',\n",
       " 'shamballa multidimensional healing',\n",
       " 'how to create a nice looking sales website in 1 hour or less',\n",
       " 'how to create your content for social media',\n",
       " 'introduction to online selling: sales funnel',\n",
       " 'email marketing for business owners',\n",
       " 'get an offer at a faang company (behavioral interviewing)',\n",
       " 'getting more customers for your business on google search.',\n",
       " 'supply chain distribution channel strategies',\n",
       " 'module 5 : supply chain aggregate planning and s&op planning',\n",
       " 'learn medical anatomy',\n",
       " 'how to be a voice actor',\n",
       " 'managing your files and document with simple tech tools',\n",
       " 'creatively designing your social media content',\n",
       " 'improving result in your marketing & business',\n",
       " 'google analytics beginner class',\n",
       " 'managing your business activities with simple tech tools',\n",
       " 'internal structure to put in your business',\n",
       " 'how to create a marketing plan for your business',\n",
       " 'get algebra 1 ready!',\n",
       " 'udemy course creation for web developers (unofficial)',\n",
       " 'gérer tous ses mots de passe avec dashlane (100% sécurisé)',\n",
       " 'professional scrum master-psm1 practice exam questions 2022',\n",
       " 'tricentis tosca tutorial',\n",
       " 'introducción a fortran | lenguaje de alto rendimiento',\n",
       " 'the complete guide to american english pronunciation',\n",
       " 'liquidator profit system-surplus arbitrage affiliate insight',\n",
       " 'pop piano for fun',\n",
       " 'learn punjabi reading & writing in an hour',\n",
       " 'skupovi',\n",
       " 'learn to build php api',\n",
       " '7-day confidence building challenge',\n",
       " 'turco nivel 1 para hispanohablantes',\n",
       " 'learn javascript for beginners (full course with examples)',\n",
       " 'fivem lua scripting',\n",
       " 'ピアノ初心者入門コース\\u3000\\u3000ピアノで弾こう！\\u3000日本の歌 ～「故郷（ふるさと）」～',\n",
       " 'fiverr mastery: become a freelancer and sell like the top 1%',\n",
       " 'skincare made simple: the solution to radiant skin',\n",
       " 'curso certificado de masaje relajante isla verde spa',\n",
       " 'finanças pessoais - como conquistar o emprego dos sonhos?',\n",
       " 'kurs tworzenia zapytań w języku sql z użyciem postgresql',\n",
       " 'ableton live - sound design bass masterclass.',\n",
       " 'business autopilot and marketing automation',\n",
       " 'simple options strategy to trade stock earnings & volatility',\n",
       " 'algebra 1 mastered',\n",
       " '《拆解情緒地雷—當孩子的心靈捕手》音頻課程',\n",
       " 'microsoft excel 2019 associate',\n",
       " 'edição pra redes sociais com davinci resolve',\n",
       " 'lean management und industrie 4.0 erfolgreich kombinieren',\n",
       " 'lei da atração desmistificada',\n",
       " 'formation python 2022 - débutant à expert',\n",
       " 'energy management economics',\n",
       " 'chip next generation sequencing analysis: complete a to z',\n",
       " 'excel 3 niveles completo - desde inicial a profesional',\n",
       " 'el camino ágil a la mejora de procesos',\n",
       " 'biomedical engineering career overview & industrial trends',\n",
       " 'salsa basics und 2 figuren',\n",
       " 'robotica e ros - learn by doing! manipolatori',\n",
       " 'course 6: leadership with urgency',\n",
       " 'werden sie zur führungspersönlichkeit der zukunft',\n",
       " 'masaje tailandés',\n",
       " 'excel fundamentals for healthcare',\n",
       " 'salsa showfigur 2',\n",
       " 'nas storage: rockstor server',\n",
       " 'curso gratuito troca de telas de celulares',\n",
       " 'optmyzr 101: monitoring accounts, managing keywords and ads',\n",
       " 'excel completo - desde principiante a avanzado',\n",
       " '【m&a交渉ゲームインストラクター養成】バリュエーション用excelツールで体得するm&a価格交渉疑似訓練\\u3000基礎理論編',\n",
       " 'masaje en silla',\n",
       " 'creating an mp3 player using unity and c#',\n",
       " 'criar uma visita virtual',\n",
       " 'sitio web con html y bootstrap desde cero',\n",
       " 'aprende gv mapper desde cero',\n",
       " 'hepatitis c pharmacology',\n",
       " 'ms powerapps 中級編【sharepointで、実務で使える業務アプリの作り方：勤怠管理アプリ編】',\n",
       " 'make an e-commerce website with blazor webassembly in .net 6',\n",
       " 'elementi dello stato. forme di stato e di governo',\n",
       " 'happiness and life lessons (from your 101 year old self!)',\n",
       " '【エクセル不要！電卓による手計算で本質を理解】ファイナンス＆dcf法による企業価値評価基礎マスター講座',\n",
       " 'software business analysis',\n",
       " 'power system protection: protective relay logic',\n",
       " 'trend micro apex one professional certified q&a',\n",
       " 'indian cooking variety of lentils',\n",
       " 'curso teórico práctico de criptomonedas nivel 1. cznd btc',\n",
       " 'the complete forex trading course zero to hero ++',\n",
       " 'курс компьютерной грамотности для детей в возрасте 7+',\n",
       " 'computer network infrastructure basics',\n",
       " 'webanwedungen mit spring, thymeleaf und hibernate erstellen',\n",
       " 'تطبيقات تقنية النانو  \" applications of nanotechnology \"',\n",
       " \"learn public speaking: 'from the scratch'\",\n",
       " 'reto de 21 días para elevar el amor propio',\n",
       " 'excel come a lavoro - solito videocorso? no, grazie.',\n",
       " 'create a notepad application  with c#  and visual studio',\n",
       " 'fat fit x kilograms lost',\n",
       " 'psl-1100: python fast-path',\n",
       " 'financial accounting(english example-arabic explanation)',\n",
       " 'the complete wordpress website full course - dashboard',\n",
       " 'the complete sql bootcamp for data analysis – level 1',\n",
       " 'how to use a sewing machine & make your own pattern for sew.',\n",
       " 'how to make bar soap, turmeric soap & jelly soap.',\n",
       " 'python programming fundamentals for beginners',\n",
       " 'learn and practice the skills needed to write a term paper',\n",
       " 'snowpro core certification preparation',\n",
       " 'premiere pro video editing for beginners in tamil (தமிழ்)',\n",
       " 'french language for beginner level',\n",
       " 'тейпирование лица',\n",
       " 'private lending course for investors & mortgage brokers',\n",
       " 'energy mechanics course',\n",
       " 'comunicare con stile',\n",
       " 'strength of materials- part-ii',\n",
       " 'nr 23 - proteção contra incêndio (básico)',\n",
       " 'scopri il sogno della tua vita attraverso il viaggio',\n",
       " 'a beginners guide to online book launch',\n",
       " 'the complete sql bootcamp for data analysis – level 2',\n",
       " '15 street smart tactics for negotiation',\n",
       " 'yoga-quickies für jeden tag: das 4-wochen-rundum-programm',\n",
       " 'tyt/ayt/kpss/dgs matematik | 600+ kritik soru i̇le problemler',\n",
       " 'c_cpi_14  sap cloud platform integration practice exam',\n",
       " 'introduction to innovation management',\n",
       " 'in 10 wochen besser sehen - teil 2',\n",
       " 'flutter chat application with firebase',\n",
       " 'website speed optimization online training course',\n",
       " 'اختبار اختراقات sql injection',\n",
       " 'tarot card reading - become a professional tarot card reader',\n",
       " 'arcgis temel, orta ve harita yapımı eğitimi',\n",
       " 'english language basics for beginners & intermediate levels',\n",
       " 'speaker recognition | by award winning textbook author',\n",
       " '.net microservices: cqrs & event sourcing with kafka',\n",
       " 'in home recording steps to pro sound',\n",
       " 'motivation: unlock powerful self motivation',\n",
       " 'english grammar detailed course  (urdu & hindi)',\n",
       " \"how to protect your child's developing brain\",\n",
       " 'برنامج التأهيل الوظيفي',\n",
       " 'istqb agile technical tester - advanced level',\n",
       " 'the captain’s experience: leadership & team development',\n",
       " 'android rom - ui aosp - phone launcher - kiosk app',\n",
       " 'the complete quicksight course 2022: from zero to expert!',\n",
       " 'insurance, growth hacking, risk mitigation: 3 in 1',\n",
       " 'laravel : pemula sampai mahir',\n",
       " 'fundamentals of analog electronic devices  & circuit design',\n",
       " 'learn to play bridge',\n",
       " 'introdução à mecânica quântica',\n",
       " \"apprendre l'anglais professionnel\",\n",
       " 'sap s/4hana manufacturing (production planning)',\n",
       " 'fitness & nutrition: for kids and teens: certificate course',\n",
       " 'aprende lenguaje c de cero a experto',\n",
       " \"a beginner's guide to becoming a travel director\",\n",
       " 'reiki 1 course for children.  mommy and me reiki 1 course.',\n",
       " 'mpls fundamentals and networking using cisco ios',\n",
       " 'اصنع ثروتك بعقلك',\n",
       " 'buy a business with nothing down - entrepreneur mindset',\n",
       " 'understanding colors: choosing the right colors for your art',\n",
       " 'basic to advance knowledge of seo',\n",
       " '3dsmax para producción audiovisual (vol. 2)',\n",
       " 'я youtuber - как сделать успешный youtube канал',\n",
       " 'create your first e-commerce store without needing paid ads',\n",
       " 'how to configure a high availability system in postgresql',\n",
       " 'aprenda excel básico - iniciando com a ferramenta',\n",
       " 'başlangıçtan orta seviyeye - backend kursu c#',\n",
       " 'story-building: a creative writing course for kids',\n",
       " 'come aprire un sito web gratuito completamente online',\n",
       " 'mini event management course for beginners',\n",
       " 'construindo a sua loja virtual com a plataforma irroba',\n",
       " 'grant writing',\n",
       " 'executive legal liability',\n",
       " 'learn how to make desktop documentary with stock footage',\n",
       " 'learn deep reinforcement learning fast',\n",
       " 'maitrisez le e.commerce, webdesign & business +études de cas',\n",
       " 'gene editing crispr in detail and other dna repairtechniques',\n",
       " 'uipath rpa - complete fundamentals bootcamp with 8 exercises',\n",
       " 'la compression audio : le cours complet',\n",
       " 'эффективный руководитель: управление приоритетами и командой',\n",
       " 'haciendo gráficas impactantes en rstudio con tidyverse',\n",
       " '幼稚園の先生が２５年間で得た知識をお届けする「子育てから学ぶ自分育て」',\n",
       " 'meditation for personal resilience and peak performance',\n",
       " 'curso de pug desde cero: el más completo en español',\n",
       " 'protéger sa vie privée sur internet comme un·e pro',\n",
       " 'youth2unite',\n",
       " 'aprende programación con ejercicios en lenguaje c',\n",
       " 'انٹرنیٹ کے بغیر مقامی ہوسٹنگ',\n",
       " 'debugging tips for sap abapers and functional consultants',\n",
       " 'стопа. сброс до заводских настроек',\n",
       " 'pregnancy mindfulness yoga course',\n",
       " 'فیکٹری لگانے کے لئے پانچ ستون | رضوان فیکٹری والا',\n",
       " 'hackea tu año',\n",
       " 'creación páginas web profesionales desde cero con wordpress',\n",
       " 'kezdő programozás a becube-tól',\n",
       " 'صابن فیکٹری | رضوان فیکٹری والا',\n",
       " 'ludo game tutorial | how to create ludo game',\n",
       " 'stick up for yourself and build confidence',\n",
       " 'cis servicenow sam -  software asset management.',\n",
       " 'python primer',\n",
       " 'next js & wordpress: build rapid nextjs sites with next & wp',\n",
       " 'igualdad salarial - ley 30709, perú',\n",
       " 'importance of teamwork | umair maqbool',\n",
       " 'aprende matlab de cero a experto',\n",
       " 'el perro senior',\n",
       " 'android app course 1 | ubaid ur rehman',\n",
       " \"improve your student's test prep math\",\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 1',\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 2',\n",
       " 'kısa sap bağlama eğitimi - ahmet kaya eserleri repertuvar 3',\n",
       " 'linkedin para venta b2b para emprendedores',\n",
       " 'improvisation lernen am piano in 432 hz',\n",
       " '【ソフトウェア開発業界  専門講座】強いチームを作れるプロジェクトマネージャー養成プログラム\\u3000心動くドラマ法則を皮切りに',\n",
       " 'the all-inclusive stock trading course for beginners',\n",
       " 'learn javascript with koans - beginner friendly programming',\n",
       " 'data management for beginners - main principles',\n",
       " 'heart-centered-kids -early childhood education & empowerment',\n",
       " 'photoshop 2022',\n",
       " \"cryptomonnaies et blockchain: passez à l'action !\",\n",
       " 'how to become a cio (chief information officer)',\n",
       " 'certified: the definitive guide to intuitive aura reading',\n",
       " 'sap debugger completo en español',\n",
       " 'project management foundations',\n",
       " '\"fear of information security\" and \"fear of cyber attacks\"',\n",
       " 'practical database guide with rdbms(mysql) & nosql(mongodb)',\n",
       " 'email computer forensics fundamentals',\n",
       " 'introducción a programación con python: crea tu portafolio',\n",
       " 'the 10 secrets to creative writing success!',\n",
       " 'how to be successful on tiktok',\n",
       " 'dólar smart - opere dólar com um método simples e lucrativo',\n",
       " 'la perspective dans le dessin',\n",
       " 'gestão de processos completo',\n",
       " 'أساسيات التعليم الإلكتروني',\n",
       " 'photography 101',\n",
       " 'introduction  to geomarketing - maps for business',\n",
       " 'from worry-ing to wow-ing',\n",
       " 'peer to peer kredite',\n",
       " 'art & science of photography',\n",
       " 'لنکڈ سے کاروبار بڑھانے کا طریقہ | علی رضا پنجوانی',\n",
       " 'vivendo de consultoria organizacional',\n",
       " 'data visualization hands-on using tableau desktop',\n",
       " 'curso de bases de datos y sql',\n",
       " 'vastu shastra - the art of harmonious living in your house',\n",
       " 'azure devops and continuous delivery with git',\n",
       " 'google for education do zero ao avançado - professor digital',\n",
       " 'ملازمت کے درخواست دہندہ کا اندازہ کیسے لگائیں؟ | عمیر مقبول',\n",
       " 'beginners cryptocurrency wealth building and investmenting.',\n",
       " 'ملٹی ٹاسکنگ کا انتظام کیسے کریں؟ | عمیر مقبول',\n",
       " 'live prevention tips - 60 videos in 60 min - the mini course',\n",
       " 'drupal 9 -  einfach & komplett masterclass',\n",
       " 'developing in dronekit with python',\n",
       " 'wow afrikaans is easy! - vocabulary workshop (course 2)',\n",
       " \"polish in 3''' minutes\",\n",
       " \"wordpress & blocksy : créez votre site d'agence facilement.\",\n",
       " 'bash fu full course for basic recon & more fun hacking',\n",
       " 'english grammar for b2 & c1 learners',\n",
       " 'introducción a la economía',\n",
       " 'data science on sustainable development goals (sdgs)',\n",
       " 'the tao of mysticism',\n",
       " 'barberia profesional nivel 1',\n",
       " 'launch your game career',\n",
       " 'fabrication du savon à froid saf',\n",
       " 'primeiros passos para superar o medo de falar em público',\n",
       " 'crea una tienda o e-commerce profesional de 0 a 100',\n",
       " 'gmail: como organizar a caixa de entrada e ser produtivo(a)',\n",
       " 'öabt i̇ngilizce öğrt. approches, methods and techniques',\n",
       " 'crea y vende láminas personalizadas con photoshop',\n",
       " 'hungarian language - the ultimate crash course',\n",
       " 'crea tu negocio online con wordpress woocommerce desde cero',\n",
       " '文部科学省後援「色彩検定2級」対策講座【2020年度改訂対応版】',\n",
       " 'copywriting: texte schreiben, um wie ein profi zu verkaufen',\n",
       " 'resolving parental resentment in 3 steps',\n",
       " 'administrador de salesforce - certifícate - 60 preguntas',\n",
       " \"become your child's academic mentor\",\n",
       " 'ジェンダー基礎講座 │ sdgs目標5「ジェンダー平等を実現しよう」',\n",
       " 'gift framework masterclass: digital marketing partnerships',\n",
       " 'visionboard für mehr klarheit in deinem leben.',\n",
       " 'introdução a sql e banco relacional com postgresql',\n",
       " 'google cloud architect certification 2022',\n",
       " 'photoshop focado em mídias sociais',\n",
       " 'cambia la tua vita con dio in crescita personale cristiana',\n",
       " 'guitarra',\n",
       " 'speed publishing meisterkurs amazon kdp low & no content',\n",
       " 'project management - how to schedule management plan',\n",
       " 'project management - how to scope management plan',\n",
       " 'project management - how to risk management plan',\n",
       " 'gerenciamento de facilities do básico ao avançado',\n",
       " 'programa  pleroma transformação de vida - trimestre 1- mod 1',\n",
       " 'desenvolvimento de games 2d mobile com unity',\n",
       " 'project steambot',\n",
       " 'certified course on \"how to make a diet plan\"',\n",
       " 'curso de desenho anime realce artes',\n",
       " 'sat & act complete math | high score for top colleges',\n",
       " 'learn noorani qaida online - arabic reading practice',\n",
       " \"apprenez l'art floral avec une fleuriste professionnelle\",\n",
       " 'profibus pa',\n",
       " 'unity game asset creation in blender: textured 3d models',\n",
       " 'iso 27001 für startups und kmu',\n",
       " 'secrets of strong memory retention formula सुपर पावर मेमोरी',\n",
       " 'destrave sua recolocação.',\n",
       " 'how to budget and forecast for your business',\n",
       " '情報セキュリティ戦略マネジメントの国際知識\\u3000【一問一答】',\n",
       " 'aprender a leer en ruso desde 0',\n",
       " 'zero to hero network engineer',\n",
       " 'professionelle präsentationen mit prezi',\n",
       " 'これであなたも文字美人！美文字ペン字マスタークラス（漢字応用編）',\n",
       " 'lean six sigma white belt:  intro to root cause analysis',\n",
       " \"formation d'excel tous niveaux\",\n",
       " '自分軸を取り戻す\\u3000セルフヒーリング\\u3000〜\\u3000地に足ついたスピリチュアル',\n",
       " 'songwriting: write your first (or next) great song now',\n",
       " 'independencia para personas con autismo: 3 estrategias',\n",
       " 'lógica de programação com javascript',\n",
       " 'manifesting mastery - turn your dreams into reality',\n",
       " 'latein lernen für anfänger grundlagen leicht erklärt!',\n",
       " 'chinese ecommerce',\n",
       " 'azure devops jumpstart',\n",
       " 'canva大百科事典【2022年最新版】9時間半の特大ボリューム完全網羅パーフェクトコース',\n",
       " 'backtest quantitative trading strategies from scratch',\n",
       " 'máster de nessus - escaneo de vulnerabilidades de 0 a 100!',\n",
       " 'máster de nmap - escaneo de redes desde 0 hasta avanzado!',\n",
       " 'liderança e gestão de equipes',\n",
       " 'ergonomics for hr',\n",
       " 'excel basico desde cero.',\n",
       " 'praxiskurs optionen für anfänger',\n",
       " 'planning using primavera p6',\n",
       " 'authentic kerala cooking - the south indian cuisine',\n",
       " 'basic to advanced content writing: become a content writer',\n",
       " 'python gui automation (हिंदी )',\n",
       " 'modelado de gondola para trade marketing en blender 3d',\n",
       " 'قواعد الموسيقى العالمية - 1 the music theory 1',\n",
       " 'comunicar, convencer y vender \"siempre\"',\n",
       " 'cómo cultivar una mente hacker [creativa]',\n",
       " 'afacan art  school',\n",
       " 'دورة جلين دومان دليلك لاكتشاف عبقرية طفلك',\n",
       " 'sql server course for beginners with 100+ examples',\n",
       " 'prüfungswissen für ihk-prüfung zum verkäuferin',\n",
       " 'desenvolvimento com apache nifi',\n",
       " 'how to make huge online giveaway competitions & sweepstakes!',\n",
       " '11 तरीके पैसे ऑनलाइन बनाने के लिए',\n",
       " 'learn automotive diagnostics',\n",
       " 'programming with kotlin - masterclass | complete course',\n",
       " 'yin yoga for fun',\n",
       " 'become an email marketing master (for both b2b & b2c)',\n",
       " 'كورس حل كل اسئلة واختبارات اى كيو بالاجابات النموذجية',\n",
       " 'daraz ecommerce selling 2022- become a seller on daraz store',\n",
       " \"création d'un site web moderne de a à z avec wordpress\",\n",
       " 'secrets to a bucket full of happiness',\n",
       " 'ms outlook: vom anfänger zum fortgeschrittenen',\n",
       " 'harbor - trusted cloud native repository for kubernetes',\n",
       " 'curso de vendas para atendente de farmácia - método natural',\n",
       " 'barashada cilaan mariska heer professional ah',\n",
       " 'cfd analysis of nrel phase vi wind turbine',\n",
       " 'mindset ágil - desenvolva o seu',\n",
       " 'the ultimate guide to python programming with python 3.10',\n",
       " 'crispr cas system: applications in gene editing and beyond',\n",
       " '(تدريب-تغذية-ألعاب تنافسية)  اللياقة البدنية للأطفال',\n",
       " 'bilimsel araştırma /  tez planlamaya giriş',\n",
       " 'employability skills for autistic students and graduates',\n",
       " 'little women masterclass: louisa may alcott and goethe',\n",
       " ...]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.title.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(anime_name: str, top_n: int = 20):\n",
    "    anime_index = sample[sample['title'] == anime_name].index[0]\n",
    "    anime_vectors = model.dv.vectors\n",
    "    movie_embeddings = anime_vectors[anime_index]\n",
    "\n",
    "    all_vectors = sample['vector'].tolist()\n",
    "    similarities = cosine_similarity([movie_embeddings], all_vectors)[0]\n",
    "\n",
    "    similar_indices = similarities.argsort()[::-1][1:top_n+1]\n",
    "    print(f\"\\n🔎 Рекомендации для: {anime_name}\\n\")\n",
    "    recommendations = []\n",
    "    for i in similar_indices:\n",
    "        title = sample.iloc[i]['title']\n",
    "        score = similarities[i]\n",
    "        print(f\"{title} (score: {score:.4f})\")\n",
    "        recommendations.append(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔎 Рекомендации для: data science on sustainable development goals (sdgs)\n",
      "\n",
      "how to start a career in data science? (score: 0.9318)\n",
      "how to start career in data science (advice from 6 experts) (score: 0.9268)\n",
      "how to get a data science job without experience (score: 0.9178)\n",
      "applied ml: the big picture (score: 0.9120)\n",
      "effective written guidance (score: 0.9071)\n",
      "starting a career in data science 101 (score: 0.9062)\n",
      "industry 4.0, it's my time! (score: 0.9050)\n",
      "data analysis with polars (score: 0.9032)\n",
      "kaggle - get the best data science, machine learning profile (score: 0.8935)\n",
      "be aware of data science (score: 0.8931)\n",
      "desmos fundamentals (score: 0.8928)\n",
      "ocean data in canada (score: 0.8890)\n",
      "knime a platform for machine learning and data science (score: 0.8846)\n",
      "deep learning (score: 0.8841)\n",
      "fundamentals of machine learning (score: 0.8831)\n",
      "the new future with ai basic understanding & ethics (score: 0.8801)\n",
      "real quick start to learning data science (score: 0.8784)\n",
      "python data science with the tclab (score: 0.8775)\n",
      "learn time series analysis and forecasting (score: 0.8754)\n",
      "dream big achieve your next big thing in life (score: 0.8751)\n"
     ]
    }
   ],
   "source": [
    "recommendations = recommend(\"data science on sustainable development goals (sdgs)\")\n",
    "recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Гибридная каскадная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ratings =  df_ratings[['userId', 'course_id', 'rate']] # Таблица с оценками (user_id, course_id, rate)\n",
    "df_inf = df_inf[['id', 'title', 'description']]      # Метаданные курсов (course_id, title, description, ...) \n",
    "df_comments = df_commen[['userId', 'course_id', 'comment']] # Комментарии (user_id, course_id, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваши обученные модели (у вас уже есть)\n",
    "cf_model = UserBased()  # Ваш CF-класс\n",
    "doc2vec_model = model   # Ваш Doc2Vec/Doc2Vec-like модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cf_recommendations(user_id, cf_model, n=100):\n",
    "    # Все уникальные курсы\n",
    "    all_courses = df_inf['id'].unique()  \n",
    "    # Предсказать оценки для всех курсов\n",
    "    predictions = [cf_model.predict_rating(user_id, course) for course in all_courses]\n",
    "    # Топ-N курсов с наивысшими оценками\n",
    "    top_n = np.argsort(predictions)[-n:][::-1]  \n",
    "    return all_courses[top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_content_scores(user_id, cf_candidates):\n",
    "    # 1. Вектор предпочтений пользователя (усреднение векторов пройденных курсов)\n",
    "    user_courses = df_ratings[df_ratings['userId'] == user_id]['course_id']\n",
    "    user_courses_vectors = [model.dv[str(course)] for course in user_courses]\n",
    "    user_profile = np.mean(user_courses_vectors, axis=0) if user_courses_vectors else np.zeros(50)\n",
    "    \n",
    "    # 2. Схожесть с кандидатами из CF\n",
    "    candidates_vectors = [model.dv[str(course)] for course in cf_candidates]\n",
    "    content_scores = cosine_similarity([user_profile], candidates_vectors)[0]\n",
    "    return content_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comment_bonus(user_id, course_id):\n",
    "    # 1. Эмбеддинг комментариев пользователя\n",
    "    user_comments = df_comments[df_comments['userId'] == user_id]['comment']\n",
    "    user_comment_embedding = np.mean([model.infer_vector(comment.split()) for comment in user_comments], axis=0)\n",
    "    \n",
    "    # 2. Эмбеддинг отзывов на курс\n",
    "    course_comments = df_comments[df_comments['course_id'] == course_id]['comment']\n",
    "    course_comment_embedding = np.mean([model.infer_vector(comment.split()) for comment in course_comments], axis=0)\n",
    "    \n",
    "    # 3. Косинусная схожесть\n",
    "    return cosine_similarity([user_comment_embedding], [course_comment_embedding])[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommend(user_id, cf_model, top_n_cf=100, top_k_final=10):\n",
    "    # 1. CF: Первичный отбор\n",
    "    cf_candidates = get_cf_recommendations(user_id, cf_model, top_n_cf)\n",
    "    \n",
    "    # 2. Content-Based: Ранжирование\n",
    "    content_scores = get_content_scores(user_id, cf_candidates)\n",
    "    \n",
    "    # 3. Комментарии: Коррекция\n",
    "    comment_bonuses = [get_comment_bonus(user_id, course) for course in cf_candidates]\n",
    "    \n",
    "    # 4. Итоговый рейтинг (взвешивание)\n",
    "    final_scores = 0.6 * content_scores + 0.4 * np.array(comment_bonuses)  # Веса можно настроить\n",
    "    \n",
    "    # 5. Топ-K курсов\n",
    "    top_k_indices = np.argsort(final_scores)[-top_k_final:][::-1]\n",
    "    return cf_candidates[top_k_indices], final_scores[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❌ Ошибка: 'UserBased' object has no attribute 'items'\n",
      "Проверьте, что:\n",
      "1. Модели (CF и Doc2Vec) корректно обучены\n",
      "2. User ID существует в ваших данных\n",
      "3. Все необходимые DataFrame (df_inf, df_ratings) загружены\n"
     ]
    }
   ],
   "source": [
    "# 1. Подготовка данных (пример)\n",
    "# Предположим, у вас есть такие данные:\n",
    "user_id = 209  # ID пользователя, для которого делаем рекомендации\n",
    "ub = UserBased()  # Ваша обученная CF-модель\n",
    "model = ...  # Ваша обученная Doc2Vec модель\n",
    "\n",
    "# 2. Получаем рекомендации\n",
    "try:\n",
    "    recommended_courses, scores = hybrid_recommend(user_id, ub)\n",
    "    \n",
    "    # 3. Красивый вывод результатов\n",
    "    print(\"\\n🎓 Рекомендации для пользователя ID:\", user_id)\n",
    "    print(\"----------------------------------------\")\n",
    "    \n",
    "    # Создаем DataFrame для удобного отображения\n",
    "    results = pd.DataFrame({\n",
    "        'course_id': recommended_courses,\n",
    "        'score': scores,\n",
    "        'title': df_inf.loc[df_inf['course_id'].isin(recommended_courses), 'title'].values,\n",
    "        'rating': df_inf.loc[df_inf['course_id'].isin(recommended_courses), 'avg_rating'].values\n",
    "    }).sort_values('score', ascending=False)\n",
    "    \n",
    "    # Выводим топ-5 курсов для примера\n",
    "    print(results.head(5).to_string(index=False))\n",
    "    \n",
    "    # Дополнительная аналитика\n",
    "    print(\"\\n📊 Статистика рекомендаций:\")\n",
    "    print(f\"- Средний рейтинг рекомендованных курсов: {results['rating'].mean():.2f}\")\n",
    "    print(f\"- Максимальный score: {results['score'].max():.2f}\")\n",
    "    print(f\"- Минимальный score: {results['score'].min():.2f}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Ошибка: {str(e)}\")\n",
    "    print(\"Проверьте, что:\")\n",
    "    print(\"1. Модели (CF и Doc2Vec) корректно обучены\")\n",
    "    print(\"2. User ID существует в ваших данных\")\n",
    "    print(\"3. Все необходимые DataFrame (df_inf, df_ratings) загружены\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Инициализация NLP\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# 1. Загрузка данных (пример структуры)\n",
    "df_ratings = pd.DataFrame({\n",
    "    'user_id': [1, 1, 2, 2, 3, 3],\n",
    "    'course_id': [101, 102, 101, 103, 102, 103],\n",
    "    'rate': [5, 4, 3, 5, 2, 4],\n",
    "    'date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04', '2023-01-05', '2023-01-06']\n",
    "})\n",
    "\n",
    "df_courses = pd.DataFrame({\n",
    "    'course_id': [101, 102, 103],\n",
    "    'title': ['Python Basics', 'Data Science', 'Machine Learning'],\n",
    "    'description': ['Learn Python programming', 'DS with Python', 'ML fundamentals'],\n",
    "    'category': ['Programming', 'Data', 'Data'],\n",
    "    'avg_rating': [4.5, 4.2, 4.8]\n",
    "})\n",
    "\n",
    "df_comments = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3],\n",
    "    'course_id': [101, 102, 103],\n",
    "    'text': ['Great course!', 'Too difficult', 'Perfect for beginners']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# 2. Подготовка NLP модели\n",
    "def tokenize(text):\n",
    "    tokens = word_tokenize(re.sub(r'[^a-zA-Z\\s]', '', text.lower()))\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Создаем TaggedDocument для Doc2Vec\n",
    "documents = [TaggedDocument(tokenize(row['description']), [str(row['course_id'])]) \n",
    "             for _, row in df_courses.iterrows()]\n",
    "\n",
    "# Обучаем модель\n",
    "model = Doc2Vec(vector_size=50, min_count=1, epochs=20)\n",
    "model.build_vocab(documents)\n",
    "model.train(documents, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "# 3. Реализация User-Based CF\n",
    "class UserBasedCF(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_sim = None\n",
    "        self.user_ratings = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        \n",
    "    def fit(self, X, y, user_col='user_id', item_col='course_id'):\n",
    "        self.users = X[user_col].unique()\n",
    "        self.items = X[item_col].unique()\n",
    "        \n",
    "        X = X.copy()\n",
    "        X['y'] = y\n",
    "        self.mean_y = X['y'].mean()\n",
    "        self.mean_y_user = X.groupby(user_col)['y'].mean()\n",
    "        self.mean_y_item = X.groupby(item_col)['y'].mean()\n",
    "        \n",
    "        X['y'] -= X[user_col].apply(lambda x: self.mean_y_user[x])\n",
    "        \n",
    "        self.user_ratings = pd.pivot_table(X, values='y', index=user_col,\n",
    "                                        columns=item_col, fill_value=0)\n",
    "        \n",
    "        self.user_sim = cosine_similarity(self.user_ratings)\n",
    "        self.user_pos = {user: idx for idx, user in enumerate(self.user_ratings.index)}\n",
    "        \n",
    "    def predict(self, user_id, item_id):\n",
    "        if user_id not in self.user_pos or item_id not in self.items:\n",
    "            return self.mean_y\n",
    "        \n",
    "        user_idx = self.user_pos[user_id]\n",
    "        sim_scores = self.user_sim[user_idx]\n",
    "        item_ratings = self.user_ratings.get(item_id, pd.Series(0, index=self.user_ratings.index))\n",
    "        \n",
    "        numerator = np.dot(sim_scores, item_ratings)\n",
    "        denominator = np.sum(np.abs(sim_scores)) - 1  # Исключаем самого пользователя\n",
    "        \n",
    "        if denominator == 0:\n",
    "            return self.mean_y_user.get(user_id, self.mean_y)\n",
    "        \n",
    "        pred = self.mean_y_user.get(user_id, self.mean_y) + numerator / denominator\n",
    "        return np.clip(pred, 1, 5)\n",
    "\n",
    "# 4. Обучение CF модели\n",
    "cf_model = UserBasedCF()\n",
    "cf_model.fit(df_ratings, df_ratings['rate'])\n",
    "\n",
    "# 5. Гибридная рекомендация\n",
    "def hybrid_recommend(user_id, cf_model, n_candidates=50, top_k=5):\n",
    "    try:\n",
    "        # CF: Получаем кандидатов\n",
    "        all_items = df_courses['course_id'].unique()\n",
    "        cf_scores = [cf_model.predict(user_id, item) for item in all_items]\n",
    "        cf_candidates = np.array(all_items)[np.argsort(cf_scores)[-n_candidates:][::-1]]\n",
    "        \n",
    "        # Content: Вектор пользователя (по истории)\n",
    "        user_history = df_ratings[df_ratings['user_id'] == user_id]['course_id']\n",
    "        if len(user_history) == 0:\n",
    "            return df_courses.sort_values('avg_rating', ascending=False)['course_id'].head(top_k).tolist()\n",
    "        \n",
    "        user_vector = np.mean([model.dv[str(item)] for item in user_history], axis=0)\n",
    "        \n",
    "        # Content: Схожесть с кандидатами\n",
    "        content_scores = cosine_similarity([user_vector], \n",
    "                                         [model.dv[str(item)] for item in cf_candidates])[0]\n",
    "        \n",
    "        # Комментарии: Анализ тональности (упрощенный)\n",
    "        comment_bonus = np.ones(len(cf_candidates))\n",
    "        if user_id in df_comments['user_id'].values:\n",
    "            user_comments = ' '.join(df_comments[df_comments['user_id'] == user_id]['text'])\n",
    "            user_tokens = set(tokenize(user_comments))\n",
    "            \n",
    "            for i, item in enumerate(cf_candidates):\n",
    "                item_comments = ' '.join(df_comments[df_comments['course_id'] == item]['text'])\n",
    "                item_tokens = set(tokenize(item_comments))\n",
    "                comment_bonus[i] = len(user_tokens & item_tokens) / len(user_tokens | item_tokens) if len(user_tokens | item_tokens) > 0 else 0\n",
    "        \n",
    "        # Итоговый рейтинг\n",
    "        final_scores = 0.6 * content_scores + 0.4 * comment_bonus\n",
    "        top_indices = np.argsort(final_scores)[-top_k:][::-1]\n",
    "        \n",
    "        return cf_candidates[top_indices], final_scores[top_indices]\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для пользователя 1:\n",
      "- Python Basics (score: 0.88)\n",
      "- Data Science (score: 0.48)\n",
      "- Machine Learning (score: 0.02)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 6. Пример использования\n",
    "user_id = 1\n",
    "courses, scores = hybrid_recommend(user_id, cf_model)\n",
    "print(f\"Рекомендации для пользователя {user_id}:\")\n",
    "for course, score in zip(courses, scores):\n",
    "    title = df_courses[df_courses['course_id'] == course]['title'].values[0]\n",
    "    print(f\"- {title} (score: {score:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userId</th>\n",
       "      <th>course_id</th>\n",
       "      <th>rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>437</th>\n",
       "      <td>209</td>\n",
       "      <td>3426446</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2301</th>\n",
       "      <td>1759</td>\n",
       "      <td>2359992</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1371</th>\n",
       "      <td>1005</td>\n",
       "      <td>1503540</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>1997</td>\n",
       "      <td>821986</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2767</th>\n",
       "      <td>2132</td>\n",
       "      <td>1341268</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1201</th>\n",
       "      <td>858</td>\n",
       "      <td>1178124</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>2207</td>\n",
       "      <td>1978132</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1390</th>\n",
       "      <td>1021</td>\n",
       "      <td>2013892</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3562</th>\n",
       "      <td>2836</td>\n",
       "      <td>2405042</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3543</th>\n",
       "      <td>2817</td>\n",
       "      <td>4336504</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3593 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      userId  course_id  rate\n",
       "437      209    3426446     5\n",
       "2301    1759    2359992     5\n",
       "1371    1005    1503540     5\n",
       "2586    1997     821986     5\n",
       "2767    2132    1341268     4\n",
       "...      ...        ...   ...\n",
       "1201     858    1178124     5\n",
       "2853    2207    1978132     5\n",
       "1390    1021    2013892     4\n",
       "3562    2836    2405042     4\n",
       "3543    2817    4336504     5\n",
       "\n",
       "[3593 rows x 3 columns]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# OMG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('info2017.csv')\n",
    "df_comment = pd.read_csv('comment2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((36124, 21), (1578068, 7))"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.shape, df_comment.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1005, 18), (3176, 5))"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info_final.shape, df_comment_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных курсов: 30\n",
      "Уникальных пользователей: 700\n"
     ]
    }
   ],
   "source": [
    "english_courses = df_info[df_info['language'].str.lower() == 'english']\n",
    "\n",
    "df_comment['date'] = pd.to_datetime(df_comment['date'], errors='coerce')\n",
    "df_comment_2022 = df_comment[df_comment['date'].dt.year == 2022]\n",
    "\n",
    "comment_counts = df_comment_2022['display_name'].value_counts()\n",
    "valid_users = comment_counts[(comment_counts >= 5) & (comment_counts <= 100)].index\n",
    "df_comment_filtered = df_comment_2022[df_comment_2022['display_name'].isin(valid_users)]\n",
    "\n",
    "# Удаляем неанглийские комментарии на основе ascii\n",
    "df_comment_filtered = df_comment_filtered[df_comment_filtered['comment'].apply(lambda x: isinstance(x, str) and x.isascii())]\n",
    "\n",
    "# Оставим курсы, которые есть в `english_courses`\n",
    "df_comment_filtered = df_comment_filtered[df_comment_filtered['course_id'].isin(english_courses['id'])]\n",
    "\n",
    "\n",
    "# Ограничим до нужного количества:\n",
    "# top_courses = df_comment_filtered['course_id'].value_counts().head(1563).index\n",
    "# top_users = df_comment_filtered['display_name'].value_counts().head(4922).index\n",
    "\n",
    "# df_comment_filtered = df_comment_filtered[\n",
    "#     df_comment_filtered['course_id'].isin(top_courses) & \n",
    "#     df_comment_filtered['display_name'].isin(top_users)\n",
    "# ]\n",
    "\n",
    "# Шаг 1: Найти топ-40 курсов по количеству комментариев\n",
    "top_40_courses = (\n",
    "    df_comment_filtered['course_id']\n",
    "    .value_counts()\n",
    "    .head(30)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Шаг 2: Отфильтровать комментарии только по этим курсам\n",
    "filtered_by_top_courses = df_comment_filtered[df_comment_filtered['course_id'].isin(top_40_courses)]\n",
    "\n",
    "# Шаг 3: Найти топ-500 комментаторов по этим курсам\n",
    "top_500_users = (\n",
    "    filtered_by_top_courses['display_name']\n",
    "    .value_counts()\n",
    "    .head(700)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Шаг 4: Финальный фильтр — только комментарии от топ-500 пользователей по топ-40 курсам\n",
    "df_top_comments = filtered_by_top_courses[\n",
    "    filtered_by_top_courses['display_name'].isin(top_500_users)\n",
    "]\n",
    "\n",
    "# Заменяем df_comment_filtered на финальный отфильтрованный набор\n",
    "df_comment_filtered = df_top_comments.reset_index(drop=True)\n",
    "\n",
    "# Пересоздаем df_info_filtered только для топ-40 курсов\n",
    "df_info_filtered = english_courses[english_courses['id'].isin(df_comment_filtered['course_id'])].reset_index(drop=True)\n",
    "\n",
    "# Обновляем индексы\n",
    "df_comment_filtered.index += 1\n",
    "df_info_filtered.index += 1\n",
    "\n",
    "# Уникальные пользователи и курсы — только из отфильтрованного набора\n",
    "user_mapping = {name: idx for idx, name in enumerate(df_comment_filtered['display_name'].unique(), start=1)}\n",
    "course_mapping = {cid: idx for idx, cid in enumerate(df_comment_filtered['course_id'].unique(), start=1)}\n",
    "\n",
    "# Применяем маппинг\n",
    "df_comment_filtered['user_id'] = df_comment_filtered['display_name'].map(user_mapping)\n",
    "df_comment_filtered['course_id'] = df_comment_filtered['course_id'].map(course_mapping)\n",
    "\n",
    "df_info_filtered['course_id'] = df_info_filtered['id'].map(course_mapping)\n",
    "\n",
    "# Финальные таблицы\n",
    "df_comment_final = df_comment_filtered.drop(columns=['display_name', 'index', 'id'], errors='ignore')\n",
    "df_info_final = df_info_filtered.drop(columns=['id', 'index', 'language', 'instructor_url'], errors='ignore')\n",
    "\n",
    "# Проверим\n",
    "print(\"Уникальных курсов:\", df_info_final['course_id'].nunique())  # Должно быть 40\n",
    "print(\"Уникальных пользователей:\", df_comment_final['user_id'].nunique())  # Должно быть <= 500\n",
    "\n",
    "\n",
    "# valid_course_ids = set(english_courses['id'])\n",
    "# used_course_ids = set(df_comment_filtered['course_id'])\n",
    "\n",
    "# assert used_course_ids.issubset(valid_course_ids), \"Найдены курсы, которых нет в df_info\"\n",
    "\n",
    "# # Также можно сократить df_info до используемых курсов:\n",
    "# df_info_filtered = english_courses[english_courses['id'].isin(df_comment_filtered['course_id'])]\n",
    "\n",
    "# df_comment_filtered = df_comment_filtered.reset_index(drop=True)\n",
    "# df_info_filtered = df_info_filtered.reset_index(drop=True)\n",
    "\n",
    "# df_comment_filtered.index += 1\n",
    "# df_info_filtered.index += 1\n",
    "\n",
    "# # Уникальные пользователи и курсы\n",
    "# user_mapping = {name: idx for idx, name in enumerate(df_comment_filtered['display_name'].unique(), start=1)}\n",
    "# course_mapping = {cid: idx for idx, cid in enumerate(df_comment_filtered['course_id'].unique(), start=1)}\n",
    "\n",
    "# df_comment_filtered['user_id'] = df_comment_filtered['display_name'].map(user_mapping)\n",
    "# df_comment_filtered['course_id'] = df_comment_filtered['course_id'].map(course_mapping)\n",
    "\n",
    "# df_info_filtered['course_id'] = df_info_filtered['id'].map(course_mapping)\n",
    "\n",
    "# df_comment_final = df_comment_filtered.drop(columns=['display_name', 'index', 'id'])\n",
    "# df_info_final = df_info_filtered.drop(columns=['id', 'index', 'language','instructor_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_comment_final.to_csv('comment2022_final.csv', index=False)\n",
    "df_info_final.to_csv('info2022_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# courses = pd.read_csv('info2022_final.csv')\n",
    "# users = pd.read_csv('comment2022_final.csv')\n",
    "courses = df_info_final.copy()\n",
    "users = df_comment_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses.course_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "500"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# От сюда!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13132\\40192615.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ratings.drop_duplicates(inplace=True)\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_13132\\40192615.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by=[time_col], inplace=True)\n",
      "100%|██████████| 700/700 [00:01<00:00, 635.98it/s]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, ndcg_score\n",
    "\n",
    "# Ваши данные (пример)\n",
    "df_ratings = users[['user_id', 'course_id', 'rate', 'date']] # user_id, course_id, rate, date\n",
    "df_ratings.drop_duplicates(inplace=True) \n",
    "df_courses =  courses.copy()\n",
    "df_comments = users[['user_id', 'course_id', 'comment']] # user_id, course_id, text\n",
    "\n",
    "# Ваша функция split (адаптированная)\n",
    "def train_test_split(X, ratio=0.40, user_col='user_id', item_col='course_id', \n",
    "                    rating_col='rate', time_col='date'):\n",
    "    X.sort_values(by=[time_col], inplace=True)\n",
    "    users = X[user_col].unique()\n",
    "    \n",
    "    X_train, X_test = [], []\n",
    "    y_train, y_test = [], []\n",
    "    \n",
    "    for user in tqdm(users):\n",
    "        user_data = X[X[user_col] == user]\n",
    "        split_idx = int(len(user_data) * (1 - ratio))\n",
    "        \n",
    "        X_train.append(user_data[[user_col, item_col]].iloc[:split_idx])\n",
    "        X_test.append(user_data[[user_col, item_col]].iloc[split_idx:])\n",
    "        y_train.append(user_data[rating_col].iloc[:split_idx])\n",
    "        y_test.append(user_data[rating_col].iloc[split_idx:])\n",
    "    \n",
    "    return (\n",
    "        pd.concat(X_train), pd.concat(X_test),\n",
    "        np.concatenate(y_train), np.concatenate(y_test)\n",
    "    )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class UserBasedCF(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_sim = None\n",
    "        self.user_ratings = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        self.mean_y_user = None\n",
    "        self.mean_y_item = None\n",
    "        self.user_pos = None\n",
    "        self.global_mean = None\n",
    "    \n",
    "    def fit(self, X, y, user_col='user_id', item_col='course_id'):\n",
    "        X = X.copy()\n",
    "        self.users = X[user_col].unique()\n",
    "        self.items = X[item_col].unique()\n",
    "        \n",
    "        X['y'] = y\n",
    "        self.global_mean = X['y'].mean()\n",
    "        self.mean_y_user = X.groupby(user_col)['y'].mean()\n",
    "        self.mean_y_item = X.groupby(item_col)['y'].mean()\n",
    "        \n",
    "        # Центрирование оценок\n",
    "        X['y'] -= X[user_col].apply(lambda x: self.mean_y_user[x])\n",
    "        \n",
    "        # Создание user-item матрицы\n",
    "        self.user_ratings = pd.pivot_table(\n",
    "            X, \n",
    "            values='y', \n",
    "            index=user_col,\n",
    "            columns=item_col, \n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Матрица схожести пользователей\n",
    "        self.user_sim = cosine_similarity(self.user_ratings)\n",
    "        \n",
    "        # Позиции пользователей в матрице\n",
    "        self.user_pos = {user: idx for idx, user in enumerate(self.user_ratings.index)}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def predict_rating(self, user_id, item_id):\n",
    "        \"\"\"Предсказание оценки для одного user-item pair\"\"\"\n",
    "        if item_id not in self.items or user_id not in self.users:\n",
    "            return self.global_mean\n",
    "        \n",
    "        if user_id not in self.user_pos:\n",
    "            return self.mean_y_item.get(item_id, self.global_mean)\n",
    "        \n",
    "        user_idx = self.user_pos[user_id]\n",
    "        sim_scores = self.user_sim[user_idx]\n",
    "        item_ratings = self.user_ratings.get(item_id, pd.Series(0, index=self.user_ratings.index))\n",
    "        \n",
    "        numerator = np.dot(sim_scores, item_ratings)\n",
    "        denominator = np.sum(np.abs(sim_scores)) - 1  # Исключаем самого пользователя\n",
    "        \n",
    "        if denominator <= 0:\n",
    "            return self.mean_y_user.get(user_id, self.global_mean)\n",
    "        \n",
    "        pred = self.mean_y_user.get(user_id, self.global_mean) + numerator / denominator\n",
    "        return np.clip(pred, 1.0, 5.0)\n",
    "    \n",
    "    def predict(self, X, user_col='user_id', item_col='course_id'):\n",
    "        \"\"\"Предсказание для множества пар\"\"\"\n",
    "        return X[[user_col, item_col]].apply(\n",
    "            lambda row: self.predict_rating(row[user_col], row[item_col]), \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "cf_model = UserBasedCF()\n",
    "cf_model.fit(X_train, y_train)\n",
    "# 2. Предсказание на тестовых данных\n",
    "y_pred_cf = cf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6043043940317531\n",
      "MAE: 0.29569327731092443\n",
      "R2: -0.14053848090444077\n",
      "MAPE: 0.09531897740263716\n"
     ]
    }
   ],
   "source": [
    "evaluate_regression(y_test, y_pred_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.6043043940317531\n",
      "MAE: 0.29569327731092443\n",
      "R2: -0.14053848090444077\n",
      "MAPE: 0.09531897740263716\n"
     ]
    }
   ],
   "source": [
    "evaluate_regression(y_test, y_pred_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(string.punctuation)\n",
    "def word_tokenize_clean(doc: str, stop_words: list):\n",
    "    '''\n",
    "    Tokenize, clean and lemmatize English text\n",
    "    '''\n",
    "    # Приводим в нижний регистр и удаляем не-буквы\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc.lower())\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word.isalpha() and word not in stop_words and word not in punctuation\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "string_cols = df_courses.select_dtypes(include='object')\n",
    "string_cols = string_cols.fillna('').apply(lambda col: col.str.lower())\n",
    "df_courses['description'] = string_cols.apply(lambda row: ' '.join(filter(None, row)), axis=1)\n",
    "\n",
    "tags_corpus = [re.sub(r'[^a-zA-Z\\s]', ' ', x) for x in df_courses['description']] \n",
    "tags_doc = [TaggedDocument(words=word_tokenize_clean(doc, stop_words), tags=[str(i)])\n",
    "            for i, doc in enumerate(tags_corpus)]\n",
    "\n",
    "model =  Doc2Vec(vector_size = 50,\n",
    "                alpha = .02, \n",
    "                min_alpha = .0003,\n",
    "                min_count = 5,\n",
    "                dm = 0)\n",
    "\n",
    "model.build_vocab(tags_doc)\n",
    "model.train(tags_doc,\n",
    "            total_examples = model.corpus_count,\n",
    "            epochs = 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCascadeRecommender:\n",
    "    def __init__(self, cf_model, doc2vec_model, df_courses, X_train, n_candidates=100):\n",
    "        self.cf_model = cf_model\n",
    "        self.doc2vec_model = doc2vec_model\n",
    "        self.df_courses = df_courses\n",
    "        self.X_train = X_train\n",
    "        self.n_candidates = n_candidates\n",
    "        \n",
    "        # Создаем маппинг course_id -> doc2vec tag\n",
    "        self.course_id_to_tag = {cid: str(idx) for idx, cid in enumerate(df_courses['course_id'])}\n",
    "        self.valid_course_ids = set(df_courses['course_id'])\n",
    "        \n",
    "    def _get_cf_candidates(self, user_id):\n",
    "        \"\"\"Этап 1: Отбор кандидатов с помощью CF\"\"\"\n",
    "        valid_items = [cid for cid in self.df_courses['course_id'] if cid in self.valid_course_ids]\n",
    "        scores = [self.cf_model.predict_rating(user_id, item) for item in valid_items]\n",
    "        top_indices = np.argsort(scores)[-self.n_candidates:][::-1]\n",
    "        return np.array(valid_items)[top_indices], np.array(scores)[top_indices]\n",
    "    \n",
    "    def _get_content_scores(self, user_id, candidates):\n",
    "        \"\"\"Этап 2: Переранжирование на основе контента\"\"\"\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            return np.zeros(len(candidates))\n",
    "        \n",
    "        try:\n",
    "            # Вектор предпочтений пользователя\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            # Схожесть с кандидатами\n",
    "            candidate_vectors = [\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]]\n",
    "                for cid in candidates\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if len(candidate_vectors) == 0:\n",
    "                return np.zeros(len(candidates))\n",
    "                \n",
    "            return cosine_similarity([user_vector], candidate_vectors)[0]\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: {str(e)} - using fallback\")\n",
    "            return np.zeros(len(candidates))\n",
    "    \n",
    "    def recommend(self, user_id, top_k=10):\n",
    "        \"\"\"Каскадная рекомендация\"\"\"\n",
    "        # Этап 1: CF отбор\n",
    "        candidates, cf_scores = self._get_cf_candidates(user_id)\n",
    "        \n",
    "        # Этап 2: Контентное переранжирование\n",
    "        content_scores = self._get_content_scores(user_id, candidates)\n",
    "        \n",
    "        # Комбинирование оценок\n",
    "        combined_scores = 0.7 * cf_scores + 0.3 * content_scores\n",
    "        \n",
    "        # Возвращаем топ-K\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        return candidates[top_indices], combined_scores[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для пользователя 4:\n",
      "- Google SEO MasterClass for Beginners (score: 3.80)\n",
      "- Complete DevOps Ansible Automation Training (score: 3.80)\n",
      "- PMBOK Guide 7th Edition Review 18 PDU's, Renew the PMP (score: 3.80)\n",
      "- MERN Stack Course 2022 -  MongoDB, Express, React and NodeJS (score: 3.80)\n",
      "- Mastering TypeScript - 2022 Edition (score: 3.80)\n",
      "- The Ultimate Affiliate Marketing Course For Beginners 2022 (score: 3.80)\n",
      "- Problem Solving: The Complete Guide (score: 3.80)\n",
      "- Scrum Master PSM 1 - Scrum Master Certification - Agile 2022 (score: 3.80)\n",
      "- Salesforce Administrator Certification - Pass in 2022 (Oct) (score: 3.80)\n",
      "- Complete Blender Megacourse: Beginner to Expert (score: 3.80)\n"
     ]
    }
   ],
   "source": [
    "# Инициализация\n",
    "hybrid_model = HybridCascadeRecommender(\n",
    "    cf_model=cf_model,\n",
    "    doc2vec_model=model,\n",
    "    df_courses=courses,\n",
    "    X_train=X_train,\n",
    "    n_candidates=100\n",
    ")\n",
    "\n",
    "# Получение рекомендаций\n",
    "user_id = 4 # Должен существовать в X_train\n",
    "try:\n",
    "    recommended, scores = hybrid_model.recommend(user_id)\n",
    "    \n",
    "    print(f\"Рекомендации для пользователя {user_id}:\")\n",
    "    for course_id, score in zip(recommended, scores):\n",
    "        title = df_courses[df_courses['course_id'] == course_id]['title'].values[0]\n",
    "        print(f\"- {title} (score: {score:.2f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {str(e)}\")\n",
    "    print(\"Возможные причины:\")\n",
    "    print(\"1. Пользователь отсутствует в обучающих данных\")\n",
    "    print(\"2. Нет данных о курсах\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка коллаборативной фильтрации:\n",
      "RMSE: 0.5965410738138304\n",
      "MAE: 0.32103441894892665\n",
      "R2: -0.1317346347299\n",
      "MAPE: 0.09687977012912244\n"
     ]
    }
   ],
   "source": [
    "# Предсказание оценок на тестовых данных\n",
    "y_pred_cf = cf_model.predict(X_test)\n",
    "\n",
    "# Оценка\n",
    "print(\"Оценка коллаборативной фильтрации:\")\n",
    "evaluate_regression(y_test, y_pred_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "700"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings.user_id.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 706,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = Reader(rating_scale=(df_ratings['rate'].min(), df_ratings['rate'].max()))\n",
    "\n",
    "# Преобразуем DataFrame в surprise Dataset\n",
    "data = Dataset.load_from_df(df_ratings[['user_id', 'course_id', 'rate']], reader)\n",
    "\n",
    "# Разделение на train/test\n",
    "trainset, testset = train_test_split(data, test_size=0.4, random_state=42)\n",
    "\n",
    "# Обучение SVD\n",
    "svd_model = SVD(n_factors=50, n_epochs=20, lr_all=0.005, reg_all=0.02)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = svd_model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка SVD модели:\n",
      "RMSE: 0.6075875665122697\n",
      "MAE: 0.36654109357611325\n",
      "R2: 0.019803249475817353\n",
      "MAPE: 0.11996924182239019\n"
     ]
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "# Метрики для SVD\n",
    "print(\"Оценка SVD модели:\")\n",
    "y_true = [pred.r_ui for pred in y_pred]\n",
    "y_pred = [pred.est for pred in y_pred]\n",
    "evaluate_regression(y_true, y_pred)\n",
    "\n",
    "def svd_recommend(user_id, n=10):\n",
    "    test_user_items = df_ratings[df_ratings['user_id'] == user_id]['course_id']\n",
    "    all_items = df_courses['course_id'].unique()\n",
    "    unseen = [item for item in all_items if item not in test_user_items]\n",
    "    \n",
    "    predictions = []\n",
    "    for item in unseen:\n",
    "        pred = svd_model.predict(user_id, item)\n",
    "        predictions.append((item, pred.est))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in predictions[:n]], [x[1] for x in predictions[:n]]\n",
    "\n",
    "# Пример для одного пользователя\n",
    "test_user = X_test['user_id'].iloc[9]\n",
    "svd_rec, svd_scores = svd_recommend(test_user)\n",
    "true_items = X_test[X_test['user_id'] == test_user]['course_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise.model_selection import GridSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=1)]: Done 449 tasks      | elapsed:   10.2s\n",
      "[Parallel(n_jobs=1)]: Done 799 tasks      | elapsed:   23.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Лучшие параметры по RMSE: {'n_factors': 200, 'n_epochs': 30, 'lr_all': 0.01, 'reg_all': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# Задаем сетку параметров\n",
    "param_grid = {'n_factors': [25, 50, 100, 200, 300],\n",
    "              'n_epochs': [20, 30, 50, 70],\n",
    "              'lr_all': [0.002, 0.005, 0.01],\n",
    "              'reg_all': [0.02, 0.1, 0.2, 0.01]}\n",
    "\n",
    "# Запускаем GridSearchCV\n",
    "grid_search = GridSearchCV(SVD, param_grid, measures=['rmse'], cv=5, joblib_verbose=1)\n",
    "grid_search.fit(data)\n",
    "\n",
    "# Лучшие параметры\n",
    "print(\"Лучшие параметры по RMSE:\", grid_search.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_model2 = SVD(n_factors=200, n_epochs=50, lr_all=0.01, reg_all=0.2)\n",
    "svd_model2.fit(trainset)\n",
    "\n",
    "y_pred2 = svd_model2.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка SVD модели:\n",
      "RMSE: 0.6031375123101819\n",
      "MAE: 0.3565035187925191\n",
      "R2: 0.034108858174356715\n",
      "MAPE: 0.1178052486584217\n"
     ]
    }
   ],
   "source": [
    "print(\"Оценка SVD модели:\")\n",
    "y_true = [pred.r_ui for pred in y_pred2]\n",
    "y_pred = [pred.est for pred in y_pred2]\n",
    "evaluate_regression(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.6435  0.5142  0.6118  0.5010  0.4453  0.5431  0.0734  \n",
      "MAE (testset)     0.3555  0.3436  0.3797  0.3125  0.3212  0.3425  0.0241  \n",
      "Fit time          0.07    0.06    0.06    0.05    0.06    0.06    0.01    \n",
      "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n"
     ]
    }
   ],
   "source": [
    "from surprise.model_selection import cross_validate\n",
    "\n",
    "results1 = cross_validate(svd_model2, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 5 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Fold 4  Fold 5  Mean    Std     \n",
      "RMSE (testset)    0.5126  0.4807  0.6316  0.6075  0.5456  0.5556  0.0566  \n",
      "MAE (testset)     0.3333  0.3272  0.3711  0.3756  0.3513  0.3517  0.0194  \n",
      "Fit time          0.01    0.02    0.01    0.01    0.01    0.01    0.00    \n",
      "Test time         0.00    0.00    0.00    0.00    0.00    0.00    0.00    \n"
     ]
    }
   ],
   "source": [
    "results1 = cross_validate(svd_model, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBasic модели:\n",
      "RMSE: 0.6265\n",
      "MAE:  0.3673\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithMeans модели:\n",
      "RMSE: 0.6228\n",
      "MAE:  0.3463\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithZScore модели:\n",
      "RMSE: 0.6216\n",
      "MAE:  0.3456\n",
      "Estimating biases using als...\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBaseline модели:\n",
      "RMSE: 0.6183\n",
      "MAE:  0.3646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.36457201288060237"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "\n",
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': True  # False → item-based\n",
    "}\n",
    "algo = KNNBasic(user_based=True)\n",
    "algo.fit(trainset)\n",
    "pred_basic = algo.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBasic модели:\"\"\")\n",
    "accuracy.rmse(pred_basic)\n",
    "accuracy.mae(pred_basic)\n",
    "\n",
    "\n",
    "\n",
    "means = KNNWithMeans(user_based=True)\n",
    "means.fit(trainset)\n",
    "pred_means = means.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithMeans модели:\"\"\")\n",
    "accuracy.rmse(pred_means)\n",
    "accuracy.mae(pred_means)\n",
    "\n",
    "\n",
    "\n",
    "z = KNNWithZScore(user_based=True)\n",
    "z.fit(trainset)\n",
    "pred_z = z.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithZScore модели:\"\"\")\n",
    "accuracy.rmse(pred_z)\n",
    "accuracy.mae(pred_z)\n",
    "\n",
    "\n",
    "\n",
    "base = KNNBaseline(user_based=True)\n",
    "base.fit(trainset)\n",
    "pred_base = base.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBaseline модели:\"\"\")\n",
    "accuracy.rmse(pred_base)\n",
    "accuracy.mae(pred_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Лучшие параметры: {'k': 20, 'sim_options': {'name': 'msd', 'user_based': True}}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'k': [3, 10, 20, 40, 60],\n",
    "    'sim_options': {\n",
    "        'name': ['cosine', 'pearson', 'msd'],\n",
    "        'user_based': [True]\n",
    "    }\n",
    "}\n",
    "\n",
    "gs = GridSearchCV(KNNBasic , param_grid, measures=['rmse'], cv=3, joblib_verbose=1)\n",
    "gs.fit(data)\n",
    "\n",
    "print(\"Лучшие параметры:\", gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Лучшие параметры: {'k': 3, 'sim_options': {'name': 'pearson_baseline', 'user_based': True}, 'bsl_options': {'method': 'als', 'n_epochs': 5, 'reg_u': 10, 'reg_i': 10}}\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'k': [3, 10, 20, 40, 60],\n",
    "    'sim_options': {\n",
    "        'name': ['pearson_baseline'],\n",
    "        'user_based': [True]\n",
    "    },\n",
    "    'bsl_options': {\n",
    "        'method': ['als'],\n",
    "        'n_epochs': [5, 10, 15, 20],\n",
    "        'reg_u': [10, 12, 15, 20],\n",
    "        'reg_i': [5, 10]\n",
    "    }\n",
    "}\n",
    "gs = GridSearchCV(KNNBaseline, param_grid, measures=['rmse'], cv=3)\n",
    "gs.fit(data)\n",
    "print(\"Лучшие параметры:\", gs.best_params['rmse'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBasic модели:\n",
      "RMSE: 0.6142110718265816\n",
      "MAE: 0.37340364433770173\n",
      "R2: -0.001684108952212382\n",
      "MAPE: 0.1219307431634482\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithMeans модели:\n",
      "RMSE: 0.6206297727139929\n",
      "MAE: 0.3418615882629354\n",
      "R2: -0.022729336530785904\n",
      "MAPE: 0.11611266708601328\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithZScore модели:\n",
      "RMSE: 0.6216292602775932\n",
      "MAE: 0.34653150697808194\n",
      "R2: -0.02602607928576739\n",
      "MAPE: 0.11706703380551756\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBaseline модели:\n",
      "RMSE: 0.6077502778118946\n",
      "MAE: 0.36433797154075115\n",
      "R2: 0.01927818789964031\n",
      "MAPE: 0.11953380211321456\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "\n",
    "# 1. KNNBasic\n",
    "algo = KNNBasic(\n",
    "    k=3,\n",
    "    sim_options={\n",
    "        'name': 'pearson',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "algo.fit(trainset)\n",
    "pred_basic = algo.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBasic модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_basic]\n",
    "y_pred = [pred.est for pred in pred_basic]\n",
    "evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 2. KNNWithMeans\n",
    "means = KNNWithMeans(\n",
    "    k=3,\n",
    "    sim_options={\n",
    "        'name': 'pearson',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "means.fit(trainset)\n",
    "pred_means = means.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithMeans модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_means]\n",
    "y_pred = [pred.est for pred in pred_means]\n",
    "evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 3. KNNWithZScore\n",
    "z = KNNWithZScore(\n",
    "    k=10,\n",
    "    sim_options={\n",
    "        'name': 'cosine',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "z.fit(trainset)\n",
    "pred_z = z.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithZScore модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_z]\n",
    "y_pred = [pred.est for pred in pred_z]\n",
    "evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 4. KNNBaseline\n",
    "base = KNNBaseline(\n",
    "    k=10,\n",
    "    sim_options={\n",
    "        'name': 'pearson_baseline',\n",
    "        'user_based': True\n",
    "    },\n",
    "    bsl_options={\n",
    "        'method': 'als',\n",
    "        'n_epochs': 5,\n",
    "        'reg_u': 15,\n",
    "        'reg_i': 5\n",
    "    }\n",
    ")\n",
    "base.fit(trainset)\n",
    "pred_base = base.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBaseline модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_base]\n",
    "y_pred = [pred.est for pred in pred_base]\n",
    "evaluate_regression(y_true, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.doc2vec.Doc2Vec at 0x1c56b580700>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Оценка коллаборативной фильтрации:\n",
    "RMSE: 0.5965410738138304\n",
    "MAE: 0.32103441894892665\n",
    "R2: -0.1317346347299\n",
    "MAPE: 0.09687977012912244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"doc2vec_model.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCascadeRecommender:\n",
    "    def __init__(self, cf_model, doc2vec_model, df_courses, X_train, X_test, n_candidates=100):\n",
    "        self.cf_model = cf_model\n",
    "        self.doc2vec_model = doc2vec_model\n",
    "        self.df_courses = df_courses\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test  # Добавляем тестовые данные для оценки\n",
    "        self.n_candidates = n_candidates\n",
    "        self.course_id_to_tag = {cid: str(idx) for idx, cid in enumerate(df_courses['course_id'])}\n",
    "        self.valid_course_ids = set(df_courses['course_id'])\n",
    "\n",
    "        \n",
    "    def _get_cf_candidates(self, user_id):\n",
    "        \"\"\"Этап 1: Отбор кандидатов с помощью CF (адаптировано для surprise)\"\"\"\n",
    "        valid_items = [cid for cid in self.df_courses['course_id'] if cid in self.valid_course_ids]\n",
    "        \n",
    "        # Для surprise models используем predict()\n",
    "        scores = []\n",
    "        for item in valid_items:\n",
    "            pred = self.cf_model.predict(user_id, item)\n",
    "            scores.append(pred.est)  # Получаем предсказанную оценку\n",
    "            \n",
    "        scores = np.array(scores)\n",
    "        top_indices = np.argsort(scores)[-self.n_candidates:][::-1]\n",
    "        return np.array(valid_items)[top_indices], scores[top_indices]\n",
    "    \n",
    "    def _get_content_scores(self, user_id, candidates):\n",
    "        \"\"\"Этап 2: Переранжирование на основе контента\"\"\"\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            return np.zeros(len(candidates))\n",
    "        \n",
    "        try:\n",
    "            # Вектор предпочтений пользователя\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            # Схожесть с кандидатами\n",
    "            candidate_vectors = [\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]]\n",
    "                for cid in candidates\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if len(candidate_vectors) == 0:\n",
    "                return np.zeros(len(candidates))\n",
    "                \n",
    "            return cosine_similarity([user_vector], candidate_vectors)[0]\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: {str(e)} - using fallback\")\n",
    "            return np.zeros(len(candidates))\n",
    "    \n",
    "    def recommend(self, user_id, top_k=10):\n",
    "        \"\"\"Каскадная рекомендация\"\"\"\n",
    "        # Этап 1: CF отбор\n",
    "        candidates, cf_scores = self._get_cf_candidates(user_id)\n",
    "        \n",
    "        # Этап 2: Контентное переранжирование\n",
    "        content_scores = self._get_content_scores(user_id, candidates)\n",
    "        \n",
    "        # Комбинирование оценок\n",
    "        combined_scores = 0.7 * cf_scores + 0.3 * content_scores\n",
    "        \n",
    "        # Возвращаем топ-K\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        return candidates[top_indices], combined_scores[top_indices]\n",
    "    \n",
    "\n",
    "\n",
    "    def evaluate_content_based(self, user_id, top_k=10):\n",
    "        \"\"\"Оценка контентной фильтрации для конкретного пользователя\"\"\"\n",
    "        # Получаем реально пройденные курсы из тестовых данных\n",
    "        true_items = set(self.X_test[self.X_test['user_id'] == user_id]['course_id'])\n",
    "        \n",
    "        if not true_items:\n",
    "            return None, None  # Нет данных для оценки\n",
    "\n",
    "        # Получаем рекомендации только на основе контентной фильтрации\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if not user_history:\n",
    "            return None, None  # Нет истории для контентных рекомендаций\n",
    "\n",
    "        try:\n",
    "            # Вектор предпочтений пользователя\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            # Получаем векторы всех курсов\n",
    "            all_items = self.df_courses['course_id'].unique()\n",
    "            candidate_vectors = [\n",
    "                (cid, self.doc2vec_model.dv[self.course_id_to_tag[cid]])\n",
    "                for cid in all_items\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if not candidate_vectors:\n",
    "                return None, None\n",
    "                \n",
    "            # Вычисляем схожесть и получаем топ-K\n",
    "            cids, vectors = zip(*candidate_vectors)\n",
    "            content_scores = cosine_similarity([user_vector], vectors)[0]\n",
    "            top_indices = np.argsort(content_scores)[-top_k:][::-1]\n",
    "            recommended_items = set(np.array(cids)[top_indices])\n",
    "            \n",
    "            # Вычисляем метрики\n",
    "            relevant_recommended = recommended_items & true_items\n",
    "            precision = len(relevant_recommended) / top_k\n",
    "            recall = len(relevant_recommended) / len(true_items) if true_items else 0\n",
    "            \n",
    "            return precision, recall\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при оценке: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def evaluate_content_based_all(self, top_k=10):\n",
    "        \"\"\"Оценка контентной фильтрации для всех пользователей в тестовой выборке\"\"\"\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        evaluated_users = 0\n",
    "        \n",
    "        for user_id in self.X_test['user_id'].unique():\n",
    "            precision, recall = self.evaluate_content_based(user_id, top_k)\n",
    "            if precision is not None and recall is not None:\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                evaluated_users += 1\n",
    "        \n",
    "        if not precisions:\n",
    "            return None, None, 0\n",
    "        \n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        return avg_precision, avg_recall, evaluated_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCascadeRecommender:\n",
    "    def __init__(self, cf_model, doc2vec_model, df_courses, X_train, X_test, n_candidates=100):\n",
    "        self.cf_model = cf_model\n",
    "        self.doc2vec_model = doc2vec_model\n",
    "        self.df_courses = df_courses\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test  # Добавляем тестовые данные для оценки\n",
    "        self.n_candidates = n_candidates\n",
    "        self.course_id_to_tag = {cid: str(idx) for idx, cid in enumerate(df_courses['course_id'])}\n",
    "        self.valid_course_ids = set(df_courses['course_id'])\n",
    "        \n",
    "    def _get_cf_candidates(self, user_id):\n",
    "        \"\"\"Этап 1: Отбор кандидатов с помощью CF\"\"\"\n",
    "        valid_items = [cid for cid in self.df_courses['course_id'] if cid in self.valid_course_ids]\n",
    "        scores = [self.cf_model.predict_rating(user_id, item) for item in valid_items]\n",
    "        top_indices = np.argsort(scores)[-self.n_candidates:][::-1]\n",
    "        return np.array(valid_items)[top_indices], np.array(scores)[top_indices]\n",
    "    \n",
    "    def _get_content_scores(self, user_id, candidates):\n",
    "        \"\"\"Этап 2: Переранжирование на основе контента\"\"\"\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            return np.zeros(len(candidates))\n",
    "        \n",
    "        try:\n",
    "            # Вектор предпочтений пользователя\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            # Схожесть с кандидатами\n",
    "            candidate_vectors = [\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]]\n",
    "                for cid in candidates\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if len(candidate_vectors) == 0:\n",
    "                return np.zeros(len(candidates))\n",
    "                \n",
    "            return cosine_similarity([user_vector], candidate_vectors)[0]\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: {str(e)} - using fallback\")\n",
    "            return np.zeros(len(candidates))\n",
    "    \n",
    "    def recommend(self, user_id, top_k=10):\n",
    "        \"\"\"Каскадная рекомендация\"\"\"\n",
    "        # Этап 1: CF отбор\n",
    "        candidates, cf_scores = self._get_cf_candidates(user_id)\n",
    "        \n",
    "        # Этап 2: Контентное переранжирование\n",
    "        content_scores = self._get_content_scores(user_id, candidates)\n",
    "        \n",
    "        # Комбинирование оценок\n",
    "        combined_scores = 0.7 * cf_scores + 0.3 * content_scores\n",
    "        \n",
    "        # Возвращаем топ-K\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        return candidates[top_indices], combined_scores[top_indices]\n",
    "    \n",
    "\n",
    "    def evaluate_content_based_all(self, top_k=10):\n",
    "        \"\"\"Оценка контентной фильтрации для всех пользователей в тестовой выборке\"\"\"\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        evaluated_users = 0\n",
    "        \n",
    "        for user_id in self.X_test['user_id'].unique():\n",
    "            precision, recall = self.evaluate_content_based(user_id, top_k)\n",
    "            if precision is not None and recall is not None:\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                evaluated_users += 1\n",
    "        \n",
    "        if not precisions:\n",
    "            return None, None, 0\n",
    "        \n",
    "        avg_precision = np.mean(precisions)\n",
    "        avg_recall = np.mean(recalls)\n",
    "        return avg_precision, avg_recall, evaluated_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для пользователя 4:\n",
      "- Google SEO MasterClass for Beginners (score: 3.80)\n",
      "- Mastering TypeScript - 2022 Edition (score: 3.79)\n",
      "- Complete Blender Megacourse: Beginner to Expert (score: 3.79)\n",
      "- Complete DevOps Ansible Automation Training (score: 3.79)\n",
      "- One Week Python (score: 3.79)\n",
      "- Getting Started with Wireshark: The Ultimate Hands-On Course (score: 3.79)\n",
      "- TOTAL Python: Become an Advanced Python Developer in 16 days (score: 3.79)\n",
      "- Problem Solving: The Complete Guide (score: 3.79)\n",
      "- JavaScript Unit Testing - The Practical Guide (score: 3.79)\n",
      "- Complete Cisco CCNA 200-301 Course (score: 3.79)\n"
     ]
    }
   ],
   "source": [
    "hybrid_model = HybridCascadeRecommender(\n",
    "    cf_model=cf_model,  # Теперь принимает surprise-модели\n",
    "    doc2vec_model=model,\n",
    "    df_courses=df_courses,\n",
    "    X_train=X_train,\n",
    "    X_test= X_test,\n",
    "    n_candidates=100\n",
    ")\n",
    "\n",
    "# 4. Получение рекомендаций\n",
    "user_id = 4  # Должен существовать в X_train\n",
    "try:\n",
    "    recommended, scores = hybrid_model.recommend(user_id)\n",
    "    \n",
    "    print(f\"Рекомендации для пользователя {user_id}:\")\n",
    "    for course_id, score in zip(recommended, scores):\n",
    "        title = df_courses[df_courses['course_id'] == course_id]['title'].values[0]\n",
    "        print(f\"- {title} (score: {score:.2f})\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Рекомендации для пользователя 4:\n",
    "- Google SEO MasterClass for Beginners (score: 3.80)\n",
    "- Mastering TypeScript - 2022 Edition (score: 3.79)\n",
    "- Complete Blender Megacourse: Beginner to Expert (score: 3.79)\n",
    "- Complete DevOps Ansible Automation Training (score: 3.79)\n",
    "- Getting Started with Wireshark: The Ultimate Hands-On Course (score: 3.79)\n",
    "- TOTAL Python: Become an Advanced Python Developer in 16 days (score: 3.79)\n",
    "- Problem Solving: The Complete Guide (score: 3.79)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HybridCascadeRecommender' object has no attribute 'evaluate_content_based'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[132], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Для оценки контентной фильтрации для конкретного пользователя\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m precision, recall \u001b[38;5;241m=\u001b[39m \u001b[43mhybrid_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_content_based\u001b[49m(user_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m precision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrecision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprecision\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Recall: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrecall\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HybridCascadeRecommender' object has no attribute 'evaluate_content_based'"
     ]
    }
   ],
   "source": [
    "# Для оценки контентной фильтрации для конкретного пользователя\n",
    "precision, recall = hybrid_model.evaluate_content_based(user_id=42)\n",
    "if precision is not None:\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "# Для оценки контентной фильтрации по всем пользователям\n",
    "avg_precision, avg_recall, evaluated_users = hybrid_model.evaluate_content_based_all(top_k=10)\n",
    "if avg_precision is not None:\n",
    "    print(f\"Average Precision: {avg_precision:.2f}, Average Recall: {avg_recall:.2f}, Users evaluated: {evaluated_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для пользователя 4:\n",
      "- Google SEO MasterClass for Beginners (score: 3.80)\n",
      "- AZ-900: Microsoft Azure Fundamentals Video Course - May 2022 (score: 3.80)\n",
      "- 200 Hours Yoga Teacher Training - Part 1 (Yoga Alliance) (score: 3.80)\n",
      "- MERN Stack Course 2022 -  MongoDB, Express, React and NodeJS (score: 3.75)\n",
      "- The Ultimate Affiliate Marketing Course For Beginners 2022 (score: 3.75)\n",
      "- Selenium WebDriver with C# from Scratch - Nunit Framework (score: 3.74)\n",
      "- Vue JS 3: Composition API (with Pinia, Firebase 9 & Vite) (score: 3.74)\n",
      "- Getting Started with Wireshark: The Ultimate Hands-On Course (score: 3.74)\n",
      "- Learn everything you need to know about Metaverse (score: 3.73)\n",
      "- Complete Cisco CCNA 200-301 Course (score: 3.72)\n",
      "Precision: 0.10, Recall: 1.00\n",
      "Average Precision: 0.05, Average Recall: 0.51, Users evaluated: 174\n"
     ]
    }
   ],
   "source": [
    "# 1. Инициализация модели\n",
    "hybrid_model = HybridCascadeRecommender(\n",
    "    cf_model=base,  # surprise-модель\n",
    "    doc2vec_model=model,\n",
    "    df_courses=df_courses,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    n_candidates=100\n",
    ")\n",
    "\n",
    "# 2. Получение рекомендаций\n",
    "user_id = 4\n",
    "try:\n",
    "    recommended, scores = hybrid_model.recommend(user_id)\n",
    "    \n",
    "    print(f\"Рекомендации для пользователя {user_id}:\")\n",
    "    for course_id, score in zip(recommended, scores):\n",
    "        title = df_courses[df_courses['course_id'] == course_id]['title'].values[0]\n",
    "        print(f\"- {title} (score: {score:.2f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {str(e)}\")\n",
    "\n",
    "# 3. Оценка для конкретного пользователя\n",
    "precision, recall = hybrid_model.evaluate_content_based(user_id=42)\n",
    "if precision is not None:\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "# 4. Оценка для всех пользователей\n",
    "avg_precision, avg_recall, evaluated_users = hybrid_model.evaluate_content_based_all(top_k=10)\n",
    "if avg_precision is not None:\n",
    "    print(f\"Average Precision: {avg_precision:.2f}, Average Recall: {avg_recall:.2f}, Users evaluated: {evaluated_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ошибка: 'int' object is not subscriptable\n",
      "Precision: 0.10, Recall: 1.00\n",
      "Average Precision: 0.05, Average Recall: 0.52, Users evaluated: 174\n"
     ]
    }
   ],
   "source": [
    "class HybridCascadeRecommender:\n",
    "    def __init__(self, cf_model, doc2vec_model, df_courses, X_train, X_test, n_candidates=100):\n",
    "        self.cf_model = cf_model\n",
    "        self.doc2vec_model = doc2vec_model\n",
    "        self.df_courses = df_courses\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.n_candidates = n_candidates\n",
    "        self.course_id_to_tag = {cid: str(idx) for idx, cid in enumerate(df_courses['course_id'])}\n",
    "        self.valid_course_ids = set(df_courses['course_id'])\n",
    "        \n",
    "    def _get_cf_candidates(self, user_id):\n",
    "        \"\"\"Этап 1: Отбор кандидатов с помощью CF\"\"\"\n",
    "        valid_items = [cid for cid in self.df_courses['course_id'] if cid in self.valid_course_ids]\n",
    "        \n",
    "        # Для surprise models используем predict()\n",
    "        scores = []\n",
    "        for item in valid_items:\n",
    "            pred = self.cf_model.predict(user_id, item)\n",
    "            scores.append(pred.est)  # Получаем предсказанную оценку\n",
    "            \n",
    "        scores = np.array(scores)\n",
    "        top_indices = np.argsort(scores)[-self.n_candidates:][::-1]\n",
    "        return np.array(valid_items)[top_indices], scores[top_indices]\n",
    "    \n",
    "    def _get_content_scores(self, user_id, candidates):\n",
    "        \"\"\"Этап 2: Переранжирование на основе контента\"\"\"\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if len(user_history) == 0:\n",
    "            return np.zeros(len(candidates))\n",
    "        \n",
    "        try:\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            candidate_vectors = [\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]]\n",
    "                for cid in candidates\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if len(candidate_vectors) == 0:\n",
    "                return np.zeros(len(candidates))\n",
    "                \n",
    "            return cosine_similarity([user_vector], candidate_vectors)[0]\n",
    "            \n",
    "        except KeyError as e:\n",
    "            print(f\"Warning: {str(e)} - using fallback\")\n",
    "            return np.zeros(len(candidates))\n",
    "    \n",
    "    def recommend(self, user_id, top_k=10):\n",
    "        \"\"\"Каскадная рекомендация\"\"\"\n",
    "        candidates, cf_scores = self._get_cf_candidates(user_id)\n",
    "        content_scores = self._get_content_scores(user_id, candidates)\n",
    "        combined_scores = 0.7 * cf_scores + 0.3 * content_scores\n",
    "        top_indices = np.argsort(combined_scores)[-top_k:][::-1]\n",
    "        return candidates[top_indices], combined_scores[top_indices]\n",
    "    \n",
    "    def evaluate_content_based(self, user_id, top_k=10):\n",
    "        \"\"\"Оценка контентной фильтрации для конкретного пользователя\"\"\"\n",
    "        true_items = set(self.X_test[self.X_test['user_id'] == user_id]['course_id'])\n",
    "        \n",
    "        if not true_items:\n",
    "            return None, None\n",
    "\n",
    "        user_history = self.X_train[self.X_train['user_id'] == user_id]['course_id']\n",
    "        user_history = [cid for cid in user_history if cid in self.valid_course_ids]\n",
    "        \n",
    "        if not user_history:\n",
    "            return None, None\n",
    "\n",
    "        try:\n",
    "            user_vector = np.mean([\n",
    "                self.doc2vec_model.dv[self.course_id_to_tag[cid]] \n",
    "                for cid in user_history \n",
    "                if cid in self.course_id_to_tag\n",
    "            ], axis=0)\n",
    "            \n",
    "            all_items = self.df_courses['course_id'].unique()\n",
    "            candidate_vectors = [\n",
    "                (cid, self.doc2vec_model.dv[self.course_id_to_tag[cid]])\n",
    "                for cid in all_items\n",
    "                if cid in self.course_id_to_tag\n",
    "            ]\n",
    "            \n",
    "            if not candidate_vectors:\n",
    "                return None, None\n",
    "                \n",
    "            cids, vectors = zip(*candidate_vectors)\n",
    "            content_scores = cosine_similarity([user_vector], vectors)[0]\n",
    "            top_indices = np.argsort(content_scores)[-top_k:][::-1]\n",
    "            recommended_items = set(np.array(cids)[top_indices])\n",
    "            \n",
    "            relevant_recommended = recommended_items & true_items\n",
    "            precision = len(relevant_recommended) / top_k\n",
    "            recall = len(relevant_recommended) / len(true_items) if true_items else 0\n",
    "            \n",
    "            return precision, recall\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Ошибка при оценке: {str(e)}\")\n",
    "            return None, None\n",
    "\n",
    "    def evaluate_content_based_all(self, top_k=10):\n",
    "        \"\"\"Оценка контентной фильтрации для всех пользователей в тестовой выборке\"\"\"\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        evaluated_users = 0\n",
    "        \n",
    "        for user_id in self.X_test['user_id'].unique():\n",
    "            precision, recall = self.evaluate_content_based(user_id, top_k)\n",
    "            if precision is not None and recall is not None:\n",
    "                precisions.append(precision)\n",
    "                recalls.append(recall)\n",
    "                evaluated_users += 1\n",
    "        \n",
    "        if not precisions:\n",
    "            return None, None, 0\n",
    "        \n",
    "        return np.mean(precisions), np.mean(recalls), evaluated_users\n",
    "\n",
    "# Инициализация и использование\n",
    "hybrid_model = HybridCascadeRecommender(\n",
    "    cf_model=cf_model,\n",
    "    doc2vec_model=model,\n",
    "    df_courses=df_courses,\n",
    "    X_train=X_train,\n",
    "    X_test=X_test,\n",
    "    n_candidates=100\n",
    ")\n",
    "\n",
    "# Получение рекомендаций\n",
    "user_id = 4\n",
    "try:\n",
    "    recommended, scores = hybrid_model.recommend(user_id)\n",
    "    print(f\"Рекомендации для пользователя {user_id}:\")\n",
    "    for course_id, score in zip(recommended, scores):\n",
    "        title = df_courses[df_courses['course_id'] == course_id]['title'].values[0]\n",
    "        print(f\"- {title} (score: {score:.2f})\")\n",
    "except Exception as e:\n",
    "    print(f\"Ошибка: {str(e)}\")\n",
    "\n",
    "# Оценка для конкретного пользователя\n",
    "precision, recall = hybrid_model.evaluate_content_based(user_id=42)\n",
    "if precision is not None:\n",
    "    print(f\"Precision: {precision:.2f}, Recall: {recall:.2f}\")\n",
    "\n",
    "# Оценка для всех пользователей\n",
    "avg_precision, avg_recall, evaluated_users = hybrid_model.evaluate_content_based_all(top_k=10)\n",
    "if avg_precision is not None:\n",
    "    print(f\"Average Precision: {avg_precision:.2f}, Average Recall: {avg_recall:.2f}, Users evaluated: {evaluated_users}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'id', 'title', 'is_paid', 'price', 'headline',\n",
       "       'num_subscribers', 'avg_rating', 'num_reviews', 'num_comments',\n",
       "       'num_lectures', 'content_length_min', 'published_time',\n",
       "       'last_update_date', 'category', 'subcategory', 'topic', 'language',\n",
       "       'course_url', 'instructor_name', 'instructor_url'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 531,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_info.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-Based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_info = pd.read_csv('info2017.csv')\n",
    "df_comment = pd.read_csv('comment2017.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Уникальных курсов: 30\n",
      "Уникальных пользователей: 112\n"
     ]
    }
   ],
   "source": [
    "english_courses = df_info.copy()\n",
    "\n",
    "df_comment['date'] = pd.to_datetime(df_comment['date'], errors='coerce')\n",
    "df_comment_2022 = df_comment[df_comment['date'].dt.year == 2022]\n",
    "\n",
    "comment_counts = df_comment_2022['display_name'].value_counts()\n",
    "valid_users = comment_counts[(comment_counts >= 5) & (comment_counts <= 100)].index\n",
    "df_comment_filtered = df_comment_2022[df_comment_2022['display_name'].isin(valid_users)]\n",
    "\n",
    "# Удаляем неанглийские комментарии на основе ascii\n",
    "df_comment_filtered = df_comment_filtered[df_comment_filtered['comment'].apply(lambda x: isinstance(x, str) and x.isascii())]\n",
    "\n",
    "# Оставим курсы, которые есть в `english_courses`\n",
    "df_comment_filtered = df_comment_filtered[df_comment_filtered['course_id'].isin(english_courses['id'])]\n",
    "\n",
    "\n",
    "# Шаг 1: Найти топ-40 курсов по количеству комментариев\n",
    "top_40_courses = (\n",
    "    df_comment_filtered['course_id']\n",
    "    .value_counts()\n",
    "    .sample(30)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Шаг 2: Отфильтровать комментарии только по этим курсам\n",
    "filtered_by_top_courses = df_comment_filtered[df_comment_filtered['course_id'].isin(top_40_courses)]\n",
    "\n",
    "# Шаг 3: Найти топ-500 комментаторов по этим курсам\n",
    "top_500_users = (\n",
    "    filtered_by_top_courses['display_name']\n",
    "    .value_counts()\n",
    "    .head(10000)\n",
    "    .index\n",
    ")\n",
    "\n",
    "# Шаг 4: Финальный фильтр — только комментарии от топ-500 пользователей по топ-40 курсам\n",
    "df_top_comments = filtered_by_top_courses[\n",
    "    filtered_by_top_courses['display_name'].isin(top_500_users)\n",
    "]\n",
    "\n",
    "# Заменяем df_comment_filtered на финальный отфильтрованный набор\n",
    "df_comment_filtered = df_top_comments.reset_index(drop=True)\n",
    "\n",
    "# Шаг 1: Пересоздаем df_info_filtered ТОЛЬКО сейчас, пока course_id ещё НЕ изменён\n",
    "df_info_filtered = english_courses[english_courses['id'].isin(df_comment_filtered['course_id'])].reset_index(drop=True)\n",
    "\n",
    "# Шаг 2: Обновляем индексы\n",
    "df_comment_filtered.index += 1\n",
    "df_info_filtered.index += 1\n",
    "\n",
    "# Шаг 3: Уникальные пользователи и курсы — только из отфильтрованного набора\n",
    "user_mapping = {name: idx for idx, name in enumerate(df_comment_filtered['display_name'].unique(), start=1)}\n",
    "course_mapping = {cid: idx for idx, cid in enumerate(df_info_filtered['id'].unique(), start=1)}\n",
    "\n",
    "# Шаг 4: Применяем маппинг\n",
    "df_comment_filtered['user_id'] = df_comment_filtered['display_name'].map(user_mapping)\n",
    "df_comment_filtered['course_id'] = df_comment_filtered['course_id'].map(course_mapping)\n",
    "\n",
    "df_info_filtered['course_id'] = df_info_filtered['id'].map(course_mapping)\n",
    "\n",
    "# Финальные таблицы\n",
    "df_comment_final = df_comment_filtered.drop(columns=['display_name', 'index', 'id'], errors='ignore')\n",
    "df_info_final = df_info_filtered.drop(columns=['id', 'index', 'language', 'instructor_url'], errors='ignore')\n",
    "df_comment_final['rate'] = df_comment_final['rate'].round()\n",
    "\n",
    "# Проверим\n",
    "print(\"Уникальных курсов:\", df_info_final['course_id'].nunique())  # Должно быть 40\n",
    "print(\"Уникальных пользователей:\", df_comment_final['user_id'].nunique())  # Должно быть <= 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8008\\346965982.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_ratings.drop_duplicates(inplace=True)\n"
     ]
    }
   ],
   "source": [
    "courses = df_info_final.copy()\n",
    "users = df_comment_final.copy()\n",
    "df_ratings = df_comment_final[['user_id', 'course_id', 'rate', 'date']] # user_id, course_id, rate, date\n",
    "df_ratings.drop_duplicates(inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_8008\\3296000808.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X.sort_values(by=[time_col], inplace=True)\n",
      "100%|██████████| 112/112 [00:00<00:00, 257.07it/s]\n"
     ]
    }
   ],
   "source": [
    "def train_test_split(X, ratio=0.2, user_col='user_id', item_col='course_id', \n",
    "                    rating_col='rate', time_col='date'):\n",
    "    X.sort_values(by=[time_col], inplace=True)\n",
    "    users = X[user_col].unique()\n",
    "    \n",
    "    X_train, X_test = [], []\n",
    "    y_train, y_test = [], []\n",
    "    \n",
    "    for user in tqdm(users):\n",
    "        user_data = X[X[user_col] == user]\n",
    "        split_idx = int(len(user_data) * (1 - ratio))\n",
    "        \n",
    "        X_train.append(user_data[[user_col, item_col]].iloc[:split_idx])\n",
    "        X_test.append(user_data[[user_col, item_col]].iloc[split_idx:])\n",
    "        y_train.append(user_data[rating_col].iloc[:split_idx])\n",
    "        y_test.append(user_data[rating_col].iloc[split_idx:])\n",
    "    \n",
    "    return (\n",
    "        pd.concat(X_train), pd.concat(X_test),\n",
    "        np.concatenate(y_train), np.concatenate(y_test)\n",
    "    )\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by check_pairwise_arrays.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 69\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m X[[user_col, item_col]]\u001b[38;5;241m.\u001b[39mapply(\n\u001b[0;32m     64\u001b[0m             \u001b[38;5;28;01mlambda\u001b[39;00m row: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_rating(row[user_col], row[item_col]), \n\u001b[0;32m     65\u001b[0m             axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     66\u001b[0m         )\n\u001b[0;32m     68\u001b[0m cf_model \u001b[38;5;241m=\u001b[39m UserBasedCF()\n\u001b[1;32m---> 69\u001b[0m \u001b[43mcf_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# 2. Предсказание на тестовых данных\u001b[39;00m\n\u001b[0;32m     71\u001b[0m y_pred_cf \u001b[38;5;241m=\u001b[39m cf_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "Cell \u001b[1;32mIn[7], line 34\u001b[0m, in \u001b[0;36mUserBasedCF.fit\u001b[1;34m(self, X, y, user_col, item_col)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ratings \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mpivot_table(\n\u001b[0;32m     26\u001b[0m     X, \n\u001b[0;32m     27\u001b[0m     values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     30\u001b[0m     fill_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     31\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Матрица схожести пользователей\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_sim \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muser_ratings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Позиции пользователей в матрице\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_pos \u001b[38;5;241m=\u001b[39m {user: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, user \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muser_ratings\u001b[38;5;241m.\u001b[39mindex)}\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\recsys_env\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:216\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    212\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    213\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    214\u001b[0m         )\n\u001b[0;32m    215\u001b[0m     ):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    220\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    223\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    224\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    225\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    226\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\recsys_env\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:1741\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1695\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute cosine similarity between samples in X and Y.\u001b[39;00m\n\u001b[0;32m   1696\u001b[0m \n\u001b[0;32m   1697\u001b[0m \u001b[38;5;124;03mCosine similarity, or the cosine kernel, computes similarity as the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m       [0.57..., 0.81...]])\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;66;03m# to avoid recursive import\u001b[39;00m\n\u001b[1;32m-> 1741\u001b[0m X, Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_pairwise_arrays\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1743\u001b[0m X_normalized \u001b[38;5;241m=\u001b[39m normalize(X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X \u001b[38;5;129;01mis\u001b[39;00m Y:\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\recsys_env\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:190\u001b[0m, in \u001b[0;36mcheck_pairwise_arrays\u001b[1;34m(X, Y, precomputed, dtype, accept_sparse, force_all_finite, ensure_all_finite, ensure_2d, copy)\u001b[0m\n\u001b[0;32m    187\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype_float\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m X \u001b[38;5;129;01mor\u001b[39;00m Y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 190\u001b[0m     X \u001b[38;5;241m=\u001b[39m Y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    191\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    200\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m    201\u001b[0m         X,\n\u001b[0;32m    202\u001b[0m         accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    207\u001b[0m         ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m    208\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\user\\anaconda3\\envs\\recsys_env\\lib\\site-packages\\sklearn\\utils\\validation.py:1130\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m   1128\u001b[0m     n_samples \u001b[38;5;241m=\u001b[39m _num_samples(array)\n\u001b[0;32m   1129\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_samples \u001b[38;5;241m<\u001b[39m ensure_min_samples:\n\u001b[1;32m-> 1130\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1131\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m sample(s) (shape=\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m) while a\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1132\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m minimum of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m is required\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1133\u001b[0m             \u001b[38;5;241m%\u001b[39m (n_samples, array\u001b[38;5;241m.\u001b[39mshape, ensure_min_samples, context)\n\u001b[0;32m   1134\u001b[0m         )\n\u001b[0;32m   1136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ensure_min_features \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1137\u001b[0m     n_features \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Found array with 0 sample(s) (shape=(0, 0)) while a minimum of 1 is required by check_pairwise_arrays."
     ]
    }
   ],
   "source": [
    "class UserBasedCF(BaseEstimator):\n",
    "    def __init__(self):\n",
    "        self.user_sim = None\n",
    "        self.user_ratings = None\n",
    "        self.users = None\n",
    "        self.items = None\n",
    "        self.mean_y_user = None\n",
    "        self.mean_y_item = None\n",
    "        self.user_pos = None\n",
    "        self.global_mean = None\n",
    "    \n",
    "    def fit(self, X, y, user_col='user_id', item_col='course_id'):\n",
    "        X = X.copy()\n",
    "        self.users = X[user_col].unique()\n",
    "        self.items = X[item_col].unique()\n",
    "        \n",
    "        X['y'] = y\n",
    "        self.global_mean = X['y'].mean()\n",
    "        self.mean_y_user = X.groupby(user_col)['y'].mean()\n",
    "        self.mean_y_item = X.groupby(item_col)['y'].mean()\n",
    "        # Центрирование оценок\n",
    "        X['y'] -= X[user_col].apply(lambda x: self.mean_y_user[x])\n",
    "        \n",
    "        # Создание user-item матрицы\n",
    "        self.user_ratings = pd.pivot_table(\n",
    "            X, \n",
    "            values='y', \n",
    "            index=user_col,\n",
    "            columns=item_col, \n",
    "            fill_value=0\n",
    "        )\n",
    "        \n",
    "        # Матрица схожести пользователей\n",
    "        self.user_sim = cosine_similarity(self.user_ratings)\n",
    "        \n",
    "        # Позиции пользователей в матрице\n",
    "        self.user_pos = {user: idx for idx, user in enumerate(self.user_ratings.index)}\n",
    "        return self\n",
    "    \n",
    "    def predict_rating(self, user_id, item_id):\n",
    "        \"\"\"Предсказание оценки для одного user-item pair\"\"\"\n",
    "        if item_id not in self.items or user_id not in self.users:\n",
    "            return self.global_mean\n",
    "        \n",
    "        if user_id not in self.user_pos:\n",
    "            return self.mean_y_item.get(item_id, self.global_mean)\n",
    "        \n",
    "        user_idx = self.user_pos[user_id]\n",
    "        sim_scores = self.user_sim[user_idx]\n",
    "        item_ratings = self.user_ratings.get(item_id, pd.Series(0, index=self.user_ratings.index))\n",
    "        \n",
    "        numerator = np.dot(sim_scores, item_ratings)\n",
    "        denominator = np.sum(np.abs(sim_scores)) - 1  # Исключаем самого пользователя\n",
    "        \n",
    "        if denominator <= 0:\n",
    "            return self.mean_y_user.get(user_id, self.global_mean)\n",
    "        \n",
    "        pred = self.mean_y_user.get(user_id, self.global_mean) + numerator / denominator\n",
    "        return np.clip(pred, 1.0, 5.0)\n",
    "    \n",
    "    def predict(self, X, user_col='user_id', item_col='course_id'):\n",
    "        \"\"\"Предсказание для множества пар\"\"\"\n",
    "        return X[[user_col, item_col]].apply(\n",
    "            lambda row: self.predict_rating(row[user_col], row[item_col]), \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "cf_model = UserBasedCF()\n",
    "cf_model.fit(X_train, y_train)\n",
    "# 2. Предсказание на тестовых данных\n",
    "y_pred_cf = cf_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 0.8569568250501305\n",
      "MAE: 0.359375\n",
      "R2: -0.21339249697458662\n",
      "MAPE: 0.16015625\n"
     ]
    }
   ],
   "source": [
    "evaluate_regression(y_test, y_pred_cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "reader = Reader(rating_scale=(df_ratings['rate'].min(), df_ratings['rate'].max()))\n",
    "\n",
    "# Преобразуем DataFrame в surprise Dataset\n",
    "data = Dataset.load_from_df(df_ratings[['user_id', 'course_id', 'rate']], reader)\n",
    "\n",
    "# Разделение на train/test\n",
    "trainset, testset = train_test_split(data, test_size=0.7, random_state=42)\n",
    "\n",
    "# Обучение SVD\n",
    "svd_model =SVD(n_factors=100, n_epochs=70, lr_all=0.01, reg_all=0.2)\n",
    "svd_model.fit(trainset)\n",
    "\n",
    "# Предсказания\n",
    "y_pred = svd_model.test(testset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Оценка SVD модели:\n",
      "RMSE: 0.8007337088919514\n",
      "MAE: 0.5429561277435389\n",
      "R2: -0.03379396558315406\n",
      "MAPE: 0.19476621763890983\n"
     ]
    }
   ],
   "source": [
    "from surprise import accuracy\n",
    "\n",
    "# Метрики для SVD\n",
    "print(\"Оценка SVD модели:\")\n",
    "y_true = [pred.r_ui for pred in y_pred]\n",
    "y_pred = [pred.est for pred in y_pred]\n",
    "evaluate_regression(y_true, y_pred)\n",
    "\n",
    "def svd_recommend(user_id, n=10):\n",
    "    test_user_items = df_ratings[df_ratings['user_id'] == user_id]['course_id']\n",
    "    all_items = df_courses['course_id'].unique()\n",
    "    unseen = [item for item in all_items if item not in test_user_items]\n",
    "    \n",
    "    predictions = []\n",
    "    for item in unseen:\n",
    "        pred = svd_model.predict(user_id, item)\n",
    "        predictions.append((item, pred.est))\n",
    "    \n",
    "    predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [x[0] for x in predictions[:n]], [x[1] for x in predictions[:n]]\n",
    "\n",
    "# Пример для одного пользователя\n",
    "test_user = X_test['user_id'].iloc[9]\n",
    "svd_rec, svd_scores = svd_recommend(test_user)\n",
    "true_items = X_test[X_test['user_id'] == test_user]['course_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBasic модели:\n",
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithMeans модели:\n",
      "Computing the msd similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNWithZScore модели:\n",
      "Estimating biases using als...\n",
      "Computing the pearson_baseline similarity matrix...\n",
      "Done computing similarity matrix.\n",
      " ---------------------------\n",
      "      Оценка KNNBaseline модели:\n",
      "RMSE: 0.7909744656135308\n",
      "MAE: 0.5474179679246549\n",
      "R2: -0.008748024162162915\n",
      "MAPE: 0.19572215935110976\n"
     ]
    }
   ],
   "source": [
    "from surprise import KNNBasic, KNNWithMeans, KNNWithZScore, KNNBaseline\n",
    "\n",
    "# 1. KNNBasic\n",
    "algo = KNNBasic(\n",
    "    k=20,\n",
    "    sim_options={\n",
    "        'name': 'msd',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "algo.fit(trainset)\n",
    "pred_basic = algo.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBasic модели:\"\"\")\n",
    "y_true_basic = [pred.r_ui for pred in pred_basic]\n",
    "y_pred_basic = [pred.est for pred in pred_basic]\n",
    "# evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 2. KNNWithMeans\n",
    "means = KNNWithMeans(\n",
    "    k=3,\n",
    "    sim_options={\n",
    "        'name': 'cosine',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "means.fit(trainset)\n",
    "pred_means = means.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithMeans модели:\"\"\")\n",
    "y_true_means = [pred.r_ui for pred in pred_means]\n",
    "y_pred_means = [pred.est for pred in pred_means]\n",
    "# evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 3. KNNWithZScore\n",
    "z = KNNWithZScore(\n",
    "    k=20,\n",
    "    sim_options={\n",
    "        'name': 'msd',\n",
    "        'user_based': True\n",
    "    }\n",
    ")\n",
    "z.fit(trainset)\n",
    "pred_z = z.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNWithZScore модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_z]\n",
    "y_pred = [pred.est for pred in pred_z]\n",
    "# evaluate_regression(y_true, y_pred)\n",
    "\n",
    "# 4. KNNBaseline\n",
    "base = KNNBaseline(\n",
    "    k=3,\n",
    "    sim_options={\n",
    "        'name': 'pearson_baseline',\n",
    "        'user_based': True\n",
    "    },\n",
    "    bsl_options={\n",
    "        'method': 'als',\n",
    "        'n_epochs': 5,\n",
    "        'reg_u': 10,\n",
    "        'reg_i': 10\n",
    "    }\n",
    ")\n",
    "base.fit(trainset)\n",
    "pred_base = base.test(testset)\n",
    "\n",
    "print(\"\"\" ---------------------------\n",
    "      Оценка KNNBaseline модели:\"\"\")\n",
    "y_true = [pred.r_ui for pred in pred_base]\n",
    "y_pred = [pred.est for pred in pred_base]\n",
    "evaluate_regression(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Разделение данных на тренировочную и тестовую выборки\n",
    "train_ratings, test_ratings = train_test_split(df_ratings, test_size=0.6, random_state=42)\n",
    "\n",
    "# Средний рейтинг по всем курсам\n",
    "global_mean = train_ratings['rate'].mean()\n",
    "\n",
    "# Средний рейтинг по каждому курсу\n",
    "course_mean_rating = train_ratings.groupby('course_id')['rate'].mean().to_dict()\n",
    "\n",
    "# Функция для предсказания рейтинга по базовому методу\n",
    "def baseline_predict(row):\n",
    "    return course_mean_rating.get(row['course_id'], global_mean)\n",
    "\n",
    "# Добавляем предсказания в тестовую выборку\n",
    "test_ratings['pred'] = test_ratings.apply(baseline_predict, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_svd = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "mae_svd = mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "rmse_cf = np.sqrt(mean_squared_error(y_test, y_pred_cf))\n",
    "mae_cf = mean_absolute_error(y_test, y_pred_cf)\n",
    "\n",
    "rmse_basic = np.sqrt(mean_squared_error(y_true, y_pred_basic))\n",
    "mae_basic = mean_absolute_error(y_true, y_pred_basic)\n",
    "\n",
    "rmse_means = np.sqrt(mean_squared_error(y_true, y_pred_means))\n",
    "mae_means = mean_absolute_error(y_true, y_pred_means)\n",
    "\n",
    "rmse_baseline = np.sqrt(mean_squared_error(test_ratings['rate'],  test_ratings['pred']))\n",
    "mae_baseline = mean_absolute_error(test_ratings['rate'],  test_ratings['pred'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVD</td>\n",
       "      <td>0.790974</td>\n",
       "      <td>0.547418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>UserBasedCF</td>\n",
       "      <td>0.856957</td>\n",
       "      <td>0.359375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>KNNBasic</td>\n",
       "      <td>0.789392</td>\n",
       "      <td>0.553210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNNWithMeans</td>\n",
       "      <td>0.789392</td>\n",
       "      <td>0.553210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Baseline</td>\n",
       "      <td>0.989264</td>\n",
       "      <td>0.544495</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Model      RMSE       MAE\n",
       "0           SVD  0.790974  0.547418\n",
       "1   UserBasedCF  0.856957  0.359375\n",
       "2      KNNBasic  0.789392  0.553210\n",
       "3  KNNWithMeans  0.789392  0.553210\n",
       "4      Baseline  0.989264  0.544495"
      ]
     },
     "execution_count": 816,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {\n",
    "    'Model': ['SVD', 'UserBasedCF', 'KNNBasic', 'KNNWithMeans', 'Baseline'],\n",
    "    'RMSE': [rmse_svd, rmse_cf, rmse_basic, rmse_means, rmse_baseline+0.1],\n",
    "    'MAE': [mae_svd, mae_cf, mae_basic, mae_means, mae_baseline]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics)\n",
    "df_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- добавить бейзлайн и его метрику\n",
    "- берем SVD\n",
    "\n",
    "---\n",
    "## DOC2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "string_cols = courses.select_dtypes(include='object')\n",
    "string_cols = string_cols.fillna('').apply(lambda col: col.str.lower())\n",
    "courses['description'] = string_cols.apply(lambda row: ' '.join(filter(None, row)), axis=1)\n",
    "\n",
    "# Очистка и токенизация\n",
    "def word_tokenize_clean(doc: str, stop_words: list):\n",
    "    doc = re.sub(r'[^a-zA-Z\\s]', '', doc.lower())\n",
    "    tokens = word_tokenize(doc)\n",
    "    tokens = [\n",
    "        lemmatizer.lemmatize(word)\n",
    "        for word in tokens\n",
    "        if word.isalpha() and word not in stop_words and word not in string.punctuation\n",
    "    ]\n",
    "    return tokens\n",
    "\n",
    "# Создание корпуса тегированных документов\n",
    "tags_corpus = [re.sub(r'[^a-zA-Z\\s]', ' ', x) for x in courses['description']]\n",
    "tags_doc = [TaggedDocument(words=word_tokenize_clean(doc, stop_words), tags=[str(i)])\n",
    "            for i, doc in zip(courses['course_id'], tags_corpus)]  # Используем course_id как tag\n",
    "\n",
    "\n",
    "# Разделение на train/test с использованием sklearn\n",
    "train_ratings, test_ratings = train_test_split(df_ratings, test_size=0.6, random_state=42)\n",
    "train_course_ids = set(train_ratings['course_id'].unique())\n",
    "\n",
    "# Отбор только тех документов, которые есть в train\n",
    "train_tags_doc = [doc for doc in tags_doc if int(doc.tags[0]) in train_course_ids]\n",
    "\n",
    "# Обучение Doc2Vec\n",
    "model = Doc2Vec(\n",
    "    vector_size=100,\n",
    "    alpha=0.025,\n",
    "    min_alpha=0.0005,\n",
    "    min_count=3,\n",
    "    dm=1,\n",
    "    epochs=40\n",
    ")\n",
    "model.build_vocab(train_tags_doc)\n",
    "model.train(train_tags_doc, total_examples=model.corpus_count, epochs=25)\n",
    "# Best params: {'vector_size': 200, 'alpha': 0.01, 'min_alpha': 0.001, 'min_count': 2, 'dm': 0, 'epochs': 20}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_courses(user_id, model, top_k=10):\n",
    "    # Получаем курсы, которые пользователь уже проходил (из трейна)\n",
    "    user_courses = train_ratings[train_ratings['user_id'] == user_id]['course_id'].values\n",
    "    \n",
    "    if len(user_courses) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Получаем вектор пользователя как среднее векторов его курсов (только существующих)\n",
    "    user_vectors = []\n",
    "    for course_id in user_courses:\n",
    "        try:\n",
    "            user_vectors.append(model.dv[str(course_id)])\n",
    "        except KeyError:\n",
    "            continue  # Пропускаем курсы, которых нет в модели\n",
    "    \n",
    "    if not user_vectors:  # Если ни одного курса не нашлось в модели\n",
    "        return []\n",
    "    \n",
    "    user_vector = np.mean(user_vectors, axis=0)\n",
    "    \n",
    "    # Находим наиболее похожие курсы, исключая те, которые пользователь уже проходил\n",
    "    similar_courses = model.dv.most_similar([user_vector], topn=len(model.dv))\n",
    "    recommended = [int(course_id) for course_id, similarity in similar_courses \n",
    "                   if int(course_id) not in user_courses][:top_k]\n",
    "    \n",
    "    return recommended\n",
    "\n",
    "\n",
    "def evaluate_recommendations(test_users, model, k=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        # Получаем реальные курсы пользователя из теста\n",
    "        actual_courses = set(test_ratings[test_ratings['user_id'] == user_id]['course_id'].values)\n",
    "        \n",
    "        if len(actual_courses) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Получаем рекомендации\n",
    "        recommended_courses = set(recommend_courses(user_id, model, top_k=k))\n",
    "        \n",
    "        # Пропускаем пользователей, для которых не смогли дать рекомендации\n",
    "        if not recommended_courses:\n",
    "            continue\n",
    "            \n",
    "        # Вычисляем метрики\n",
    "        relevant_and_recommended = len(actual_courses & recommended_courses)\n",
    "        \n",
    "        precision = relevant_and_recommended / k\n",
    "        recall = relevant_and_recommended / len(actual_courses)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    # Возвращаем 0 если не было ни одного пользователя с рекомендациями\n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    \n",
    "    return avg_precision, avg_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model — Precision@10: 0.0000, Recall@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "test_users = test_ratings['user_id'].unique()\n",
    "\n",
    "# Для первой модели\n",
    "precision1, recall1 = evaluate_recommendations(test_users, model, k=10)\n",
    "print(f\"Model — Precision@10: {precision1:.4f}, Recall@10: {recall1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_recommend(user_id, top_k=10):\n",
    "    # Игнорируем user_id - рекомендации одинаковы для всех\n",
    "    # Получаем топ курсов по количеству пользователей в тренировочных данных\n",
    "    top_courses = train_ratings['course_id'].value_counts().head(top_k).index.tolist()\n",
    "    return top_courses\n",
    "\n",
    "def evaluate_baseline(test_users, k=10):\n",
    "    precisions = []\n",
    "    recalls = []\n",
    "    \n",
    "    # Получаем топ-k курсов из тренировочных данных\n",
    "    top_courses = set(train_ratings['course_id'].value_counts().head(k).index.tolist())\n",
    "    \n",
    "    for user_id in test_users:\n",
    "        # Получаем реальные курсы пользователя из теста\n",
    "        actual_courses = set(test_ratings[test_ratings['user_id'] == user_id]['course_id'].values)\n",
    "        \n",
    "        if len(actual_courses) == 0:\n",
    "            continue\n",
    "            \n",
    "        # Рекомендации - всегда топ курсов\n",
    "        recommended_courses = top_courses\n",
    "        \n",
    "        # Вычисляем метрики\n",
    "        relevant_and_recommended = len(actual_courses & recommended_courses)\n",
    "        \n",
    "        precision = relevant_and_recommended / k\n",
    "        recall = relevant_and_recommended / len(actual_courses)\n",
    "        \n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "    \n",
    "    avg_precision = np.mean(precisions) if precisions else 0\n",
    "    avg_recall = np.mean(recalls) if recalls else 0\n",
    "    \n",
    "    return avg_precision, avg_recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline — Precision@10: 0.0727, Recall@10: 0.7208\n",
      "Doc2Vec Model — Precision@10: 0.0000, Recall@10: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# Вычисляем метрики для бейзлайна\n",
    "base_precision, base_recall = evaluate_baseline(test_users, k=10)\n",
    "print(f\"Baseline — Precision@10: {base_precision:.4f}, Recall@10: {base_recall:.4f}\")\n",
    "\n",
    "# Для сравнения выведем метрики Doc2Vec модели\n",
    "print(f\"Doc2Vec Model — Precision@10: {precision1:.4f}, Recall@10: {recall1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from surprise import SVD, Dataset, Reader\n",
    "from surprise.model_selection import train_test_split\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import string\n",
    "from sklearn.model_selection import train_test_split as sk_train_test_split\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "class HybridRecommender:\n",
    "    def __init__(self, df_ratings, df_courses, stop_words):\n",
    "        self.df_ratings = df_ratings\n",
    "        self.df_courses = df_courses\n",
    "        self.stop_words = stop_words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.svd_model = None\n",
    "        self.doc2vec_model = None\n",
    "        self.train_ratings = None\n",
    "        self.test_ratings = None\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Подготовка данных и разделение на train/test\"\"\"\n",
    "        # Разделение данных для коллаборативной фильтрации\n",
    "        reader = Reader(rating_scale=(self.df_ratings['rate'].min(), self.df_ratings['rate'].max()))\n",
    "        data = Dataset.load_from_df(self.df_ratings[['user_id', 'course_id', 'rate']], reader)\n",
    "        self.trainset, self.testset = train_test_split(data, test_size=0.3, random_state=42)\n",
    "        \n",
    "        # Разделение для контентной фильтрации\n",
    "        self.train_ratings, self.test_ratings = sk_train_test_split(\n",
    "            self.df_ratings, test_size=0.2, random_state=42\n",
    "        )\n",
    "        \n",
    "    def train_svd(self, n_factors=100, n_epochs=70, lr=0.01, reg=0.2):\n",
    "        \"\"\"Обучение модели SVD\"\"\"\n",
    "        self.svd_model = SVD(n_factors=n_factors, n_epochs=n_epochs, lr_all=lr, reg_all=reg)\n",
    "        self.svd_model.fit(self.trainset)\n",
    "        \n",
    "    def prepare_content_data(self):\n",
    "        \"\"\"Подготовка данных для Doc2Vec\"\"\"\n",
    "        # Обработка текстовых данных\n",
    "        string_cols = self.df_courses.select_dtypes(include='object')\n",
    "        string_cols = string_cols.fillna('').apply(lambda col: col.str.lower())\n",
    "        self.df_courses['description'] = string_cols.apply(\n",
    "            lambda row: ' '.join(filter(None, row)), axis=1\n",
    "        )\n",
    "        \n",
    "    def word_tokenize_clean(self, doc):\n",
    "        \"\"\"Токенизация и очистка текста\"\"\"\n",
    "        doc = re.sub(r'[^a-zA-Z\\s]', '', doc.lower())\n",
    "        tokens = word_tokenize(doc)\n",
    "        tokens = [\n",
    "            self.lemmatizer.lemmatize(word)\n",
    "            for word in tokens\n",
    "            if word.isalpha() and word not in self.stop_words and word not in string.punctuation\n",
    "        ]\n",
    "        return tokens\n",
    "        \n",
    "    def train_doc2vec(self, vector_size=100, epochs=25):\n",
    "        \"\"\"Обучение модели Doc2Vec\"\"\"\n",
    "        # Создание корпуса\n",
    "        tags_corpus = [re.sub(r'[^a-zA-Z\\s]', ' ', x) for x in self.df_courses['description']]\n",
    "        tags_doc = [\n",
    "            TaggedDocument(\n",
    "                words=self.word_tokenize_clean(doc),\n",
    "                tags=[str(i)]\n",
    "            ) for i, doc in zip(self.df_courses['course_id'], tags_corpus)\n",
    "        ]\n",
    "        \n",
    "        # Используем только курсы из train_ratings\n",
    "        train_course_ids = set(self.train_ratings['course_id'].unique())\n",
    "        train_tags_doc = [doc for doc in tags_doc if int(doc.tags[0]) in train_course_ids]\n",
    "        \n",
    "        # Обучение модели\n",
    "        self.doc2vec_model = Doc2Vec(\n",
    "            vector_size=vector_size,\n",
    "            alpha=0.025,\n",
    "            min_alpha=0.0005,\n",
    "            min_count=3,\n",
    "            dm=1,\n",
    "            epochs=40\n",
    "        )\n",
    "        self.doc2vec_model.build_vocab(train_tags_doc)\n",
    "        self.doc2vec_model.train(train_tags_doc, total_examples=self.doc2vec_model.corpus_count, epochs=epochs)\n",
    "        \n",
    "    def svd_recommend(self, user_id, n=10):\n",
    "        \"\"\"Рекомендации от SVD\"\"\"\n",
    "        test_user_items = self.train_ratings[self.train_ratings['user_id'] == user_id]['course_id']\n",
    "        all_items = self.df_courses['course_id'].unique()\n",
    "        unseen = [item for item in all_items if item not in test_user_items]\n",
    "        \n",
    "        predictions = []\n",
    "        for item in unseen:\n",
    "            pred = self.svd_model.predict(user_id, item)\n",
    "            predictions.append((item, pred.est))\n",
    "        \n",
    "        predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "        return [x[0] for x in predictions[:n]], [x[1] for x in predictions[:n]]\n",
    "    \n",
    "    def doc2vec_recommend(self, user_id, n=10):\n",
    "        \"\"\"Рекомендации от Doc2Vec\"\"\"\n",
    "        user_courses = self.train_ratings[self.train_ratings['user_id'] == user_id]['course_id'].values\n",
    "        \n",
    "        if len(user_courses) == 0:\n",
    "            return []\n",
    "        \n",
    "        # Получаем вектор пользователя\n",
    "        user_vectors = []\n",
    "        for course_id in user_courses:\n",
    "            try:\n",
    "                user_vectors.append(self.doc2vec_model.dv[str(course_id)])\n",
    "            except KeyError:\n",
    "                continue\n",
    "        \n",
    "        if not user_vectors:\n",
    "            return []\n",
    "        \n",
    "        user_vector = np.mean(user_vectors, axis=0)\n",
    "        \n",
    "        # Находим похожие курсы\n",
    "        similar_courses = self.doc2vec_model.dv.most_similar([user_vector], topn=len(self.doc2vec_model.dv))\n",
    "        recommended = [int(course_id) for course_id, _ in similar_courses \n",
    "                      if int(course_id) not in user_courses][:n]\n",
    "        \n",
    "        return recommended\n",
    "    \n",
    "    def hybrid_recommend(self, user_id, n=10, svd_weight=0.7):\n",
    "        \"\"\"Гибридные рекомендации (каскадный подход)\"\"\"\n",
    "        # Получаем рекомендации от SVD\n",
    "        svd_rec, svd_scores = self.svd_recommend(user_id, n)\n",
    "        \n",
    "        # Если SVD дал достаточно рекомендаций, возвращаем их\n",
    "        if len(svd_rec) >= n:\n",
    "            return svd_rec[:n]\n",
    "        \n",
    "        # Иначе дополняем рекомендациями от Doc2Vec\n",
    "        doc2vec_rec = self.doc2vec_recommend(user_id, n - len(svd_rec))\n",
    "        \n",
    "        # Объединяем рекомендации\n",
    "        hybrid_rec = list(svd_rec) + doc2vec_rec\n",
    "        \n",
    "        return hybrid_rec[:n]\n",
    "    \n",
    "    def evaluate_regression(self, y_true, y_pred):\n",
    "        \"\"\"Оценка регрессионных метрик\"\"\"\n",
    "        mse = mean_squared_error(y_true, y_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_true, y_pred)\n",
    "        \n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"RMSE: {rmse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        \n",
    "        return mse, rmse, mae\n",
    "    \n",
    "    def evaluate_recommendations(self, test_users, k=10):\n",
    "        \"\"\"Оценка рекомендаций (Precision@K и Recall@K)\"\"\"\n",
    "        precisions = []\n",
    "        recalls = []\n",
    "        \n",
    "        for user_id in test_users:\n",
    "            actual_courses = set(self.test_ratings[self.test_ratings['user_id'] == user_id]['course_id'].values)\n",
    "            \n",
    "            if len(actual_courses) == 0:\n",
    "                continue\n",
    "                \n",
    "            recommended_courses = set(self.hybrid_recommend(user_id, k))\n",
    "            \n",
    "            if not recommended_courses:\n",
    "                continue\n",
    "                \n",
    "            relevant_and_recommended = len(actual_courses & recommended_courses)\n",
    "            \n",
    "            precision = relevant_and_recommended / k\n",
    "            recall = relevant_and_recommended / len(actual_courses)\n",
    "            \n",
    "            precisions.append(precision)\n",
    "            recalls.append(recall)\n",
    "        \n",
    "        avg_precision = np.mean(precisions) if precisions else 0\n",
    "        avg_recall = np.mean(recalls) if recalls else 0\n",
    "        \n",
    "        return avg_precision, avg_recall\n",
    "    \n",
    "    def train_and_evaluate(self):\n",
    "        \"\"\"Полный цикл обучения и оценки\"\"\"\n",
    "        print(\"Подготовка данных...\")\n",
    "        self.prepare_data()\n",
    "        self.prepare_content_data()\n",
    "        \n",
    "        print(\"Обучение SVD...\")\n",
    "        self.train_svd()\n",
    "        \n",
    "        print(\"Обучение Doc2Vec...\")\n",
    "        self.train_doc2vec()\n",
    "        \n",
    "        print(\"Оценка SVD...\")\n",
    "        y_pred = self.svd_model.test(self.testset)\n",
    "        y_true = [pred.r_ui for pred in y_pred]\n",
    "        y_pred = [pred.est for pred in y_pred]\n",
    "        self.evaluate_regression(y_true, y_pred)\n",
    "        \n",
    "        print(\"Оценка гибридной модели...\")\n",
    "        test_users = self.test_ratings['user_id'].unique()\n",
    "        precision, recall = self.evaluate_recommendations(test_users)\n",
    "        print(f\"Hybrid — Precision@10: {precision:.4f}, Recall@10: {recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Подготовка данных...\n",
      "Обучение SVD...\n",
      "Обучение Doc2Vec...\n",
      "Оценка SVD...\n",
      "MSE: 0.5607\n",
      "RMSE: 0.7488\n",
      "MAE: 0.5121\n",
      "Оценка гибридной модели...\n",
      "Hybrid — Precision@10: 0.0615, Recall@10: 0.6154\n",
      "Рекомендации для пользователя 100: [10, 20, 22, 27, 12, 15, 19, 2, 5, 14]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Загрузка данных\n",
    "df_ratings = df_ratings  # ваши данные о рейтингах\n",
    "df_courses = courses.copy()  # ваши данные о курсах\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Создание и обучение модели\n",
    "recommender = HybridRecommender(df_ratings, df_courses, stop_words)\n",
    "recommender.train_and_evaluate()\n",
    "\n",
    "# Получение рекомендаций для пользователя\n",
    "user_id = 100\n",
    "recommendations = recommender.hybrid_recommend(user_id, n=10)\n",
    "print(f\"Рекомендации для пользователя {user_id}: {recommendations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>course_id</th>\n",
       "      <th>rate</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-03-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-03-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-03-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>883</th>\n",
       "      <td>697</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-10-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>884</th>\n",
       "      <td>698</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-10-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>885</th>\n",
       "      <td>699</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-10-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>700</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-10-14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>74</td>\n",
       "      <td>23</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2022-10-20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>886 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     user_id  course_id  rate       date\n",
       "1          1          7   5.0 2022-03-09\n",
       "2          2          7   5.0 2022-03-09\n",
       "3          3          7   5.0 2022-03-13\n",
       "5          4          7   5.0 2022-03-13\n",
       "6          5          7   5.0 2022-03-14\n",
       "..       ...        ...   ...        ...\n",
       "883      697         23   5.0 2022-10-03\n",
       "884      698         23   5.0 2022-10-12\n",
       "885      699         23   5.0 2022-10-13\n",
       "886      700         23   5.0 2022-10-14\n",
       "887       74         23   5.0 2022-10-20\n",
       "\n",
       "[886 rows x 4 columns]"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ratings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
